[["index.html", "Research Process for Music Psychologists R version Chapter 1 Preface", " Research Process for Music Psychologists R version Last updated 2025-04-16 Chapter 1 Preface This is an R adaptation of the statistics content from a subject called Research Process for Music Psychologists (MUSI90252). The subject is an introduction to research methods for new Masters and PhD students in music psychology and music science at the University of Melbourne. The general idea The subject’s statistics material was originally written for Jamovi, given its ease of use and impressive functionality. However, we have also encouraged any quantitative-oriented students to consider learning R in the long term. With that in mind, by and large the content is a very faithful reproduction of the Jamovi-focused content in the Canvas shell, except: Some commentary has either been added for R-specific material (e.g. information on functions). Some embedded content is not available here (mainly Jamovi-specific content, and Readings Online content). Some content has been reorganised because of how R outputs things compared to Jamovi. The first chapter is a very (very) brief overview of how to use core R and tidyverse functions. The R version of the RPMP content is therefore a little different to the regular Jamovi version, by virtue of the fact that even though Jamovi is built on R it isn’t necessarily built with R users in mind. Rather, Jamovi is built for users of SPSS and other platforms who may be used to a point-and-click approach. To clarify, there’s nothing wrong with this at all - I actually think this is a great thing, and I believe one of Jamovi’s greatest strengths is how easy it is to use. At the same time, for the budding R-using music psychologist - at least for RPMP - it means that the same procedures in R work a little differently to how they function in Jamovi. For this reason that the R adaptation of the RPMP content has been written with the key principle of parity with Jamovi in mind. That is, the focus of writing this version of the RPMP material has been to align the procedures, outputs etc. with what Jamovi provides. R can and will do so much more than what is presented in this book, and we encourage interested students to seek out this content separately. "],["intro-to-R.html", "Chapter 2 A very brief introduction to R", " Chapter 2 A very brief introduction to R This chapter is not meant to be an exhaustive be-all end-all to how to use R. It will only go through enough to complete RPMP’s content. It will, however, introduce a couple of things that I think make for good starting habits using R - which is never a bad thing! Naturally, if you are reading this version of the RPMP material and are interested in using R, you will need to download R and RStudio. In general, I am a huge proponent of the ‘tidy’ workflow of data analysis. While it was originally designed for data science in mind, I think it’s a valuable model for psychological science as well: The original subject was written with this workflow broadly in mind, although it wasn’t intended to follow this closely (only to bring awareness to the general idea of modelling). For a far more comprehensive and rigorous overview of how to program in R, there is no better guide than R 4 Data Science (R4DS) by Hadley Wickham: https://r4ds.had.co.nz/. "],["R-syntax.html", "2.1 Basic syntax", " 2.1 Basic syntax 2.1.1 Operators We can do basic maths in R using the following symbols: Addition: + Subtraction: - Multiplication: * Division: / Exponentiation: ^ # Addition and subtraction 5 + 2 ## [1] 7 6 - 3 ## [1] 3 # Multiplication and division 2 * 5 ## [1] 10 24/6 ## [1] 4 R will recognise brackets and perform calculations appropriately, following BEDMAS (or PEMDAS). As you normally would with a written equation, R will perform calculations left to right. (6 + 2) * 3/4 ## [1] 6 2.1.2 Assignment and naming When you type something in R, you almost always have the option of assigning that value to an object/variable to give it a name. In R, assigning values/strings etc. to objects is slightly different to other programming languages. Instead of using equals signs, we use a left arrow &lt;- for assignment. For example: # Use this x &lt;- 5 # This works but isn&#39;t preferred x = 5 When it comes to naming variables and the like, the easiest/most readable way for most people is to separate words using an underscore. e.g. # this is preferred variable_name # this is also very common variable.name # sentence case is also sometimes used VariableName # no spaces are allowed variable name # This will give you an error, as R will think this is two separate variables # some heathens use camel case variableName To view what has been assigned to an object, you can simply write the variable name: # This is the variable we named earlier x ## [1] 5 In general, it is good practice to use variable names that are clear but concise. In other words, avoid naming your variables something like x1, x2 etc. Instead, use the name of the measure/thing the data represents directly, e.g. scale_total. 2.1.3 Variable types Like many programming languages, R works by manipulating different types of variables. Knowing how to work with these different variables is fairly essential to using R, so here is a brief overview. First, we have our most basic classes of variables. The first is numeric, which is as it says on the tin (i.e. it stores numbers): var_a &lt;- 5.25 var_a ## [1] 5.25 A special form of a numeric variable is an integer variable, which is used for whole numbers. var_b &lt;- 6 var_b ## [1] 6 The second is character, which is used for text (named strings in programming language). Strings in character variables must be enclosed with speech marks: var_c &lt;- &quot;This is a string&quot; var_c ## [1] &quot;This is a string&quot; # &quot;This&quot; and &quot;is&quot; would be treated by R as two separate strings Finally, we have logical variables, which can take on the form of TRUE or FALSE. TRUE and FALSE (or alternatively T or F) are special values in R that, as their names suggest, are used to indicate when a certain value is true or false. var_d &lt;- TRUE var_d ## [1] TRUE "],["data-structures.html", "2.2 Data structures", " 2.2 Data structures Of course, in R we don’t usually work with single values. We instead work with larger data structures. While there are a number of data structures in R, by and large the main one we will work with are data frames. 2.2.1 Vectors Vectors are extremely important in R: so much so that many functions are what we call vectorised, meaning that they operate over vectors. Vectors are a data structure that provide an ordered list of values of the same type. Vectors can contain multiple numbers, strings or logical values, as an example. To create a vector, the c() function is used. Below is a vector containing 5 numbers, thereby making it a vector of numerics: vector_a &lt;- c(4, 1, 6, 2, 3) vector_a ## [1] 4 1 6 2 3 Each value in the vector has an index, which denotes its position in the vector starting from 1. We can pull values from vectors by using square brackets, []. We give R the name of the vector, followed by the index of the value we want. Let us pull the number 1, for instance, which has an index of 2 (as it is 2nd in the vector): vector_a[2] ## [1] 1 To subset multiple values, we can simply give a vector of indices within the square brackets. For example, let’s say we want to pull values from indices 2-4. This means that our output should be 1, 6 and 2. We can create a vector corresponding to the indices that we want (c(2, 3, 4)), and give this to the square brackets for subsetting. vector_a[c(2, 3, 4)] ## [1] 1 6 2 But there’s a neater trick here that R allows you to do. When placing a semicolon, :, between two numbers, R will create a vector of numbers between the two numbers you give. The below command, for example, will create a vector of integers between the numbers 2 and 7: 2:7 ## [1] 2 3 4 5 6 7 We can use this to great effect by subsetting multiple values from a vector at once: vector_a[2:4] ## [1] 1 6 2 2.2.2 Data frames Data frames are flexible, row-column structures that contain data. Data frames can essentially be thought of as several vectors joined together as columns. R works best when data frames are in a tidy format. In a tidy format: Each variable is its own column Each observation (participant, object) is its own row Each value is in its own cell. Figure 2.1: Adapted from R4DS. Here is an example of a data frame in tidy format. Note how each column corresponds to a different variable, or piece of data that we’re interested in. Each column is also clearly labelled, so it is clear what it represents. Each row corresponds to an observation (a single penguin, in this case). So, the first row represnts an Adelie penguin on Torgersen Island, with a bill length of 39.1mm etc etc. library(palmerpenguins) penguins For the purposes of RPMP you won’t need to create any data frames, but you will need to know how to read files in and work with them. With data frames, there are a number of functions in base R that allow us to do certain operations. These will be super useful in a range of scenarios. First, it helps to understand that data frames are (conceptually) like matrices, in that they have rows and columns that are indexed. We can therefore pull out bits of information by the row or column index (i.e. number) using R. To do so, we can use the format name[row, column]. name in this instance is the name of our data frame, while row and column are the row and column numbers we want respectively. For example, let us take the cell corresponding to the first row and the first column: penguins[1,1] Or, the cell in row 3, column 4: penguins[3,4] We can pull out whole rows or columns by simply leaving the number blank. If we want all values in a row, we do not specify a column and vice versa. For example, let us take all of row 1 from the penguins dataset: penguins[1,] Or let’s take all of the species column only, which is column 1: penguins[,1] The above operation actually can be done another way in R, and perhaps a way that is more intuitive. With data frames, we can grab individual columns using the $ operator, followed by the column’s name. This lets us grab columns by their name. penguins$species ## [1] Adelie Adelie Adelie Adelie Adelie Adelie Adelie ## [8] Adelie Adelie Adelie Adelie Adelie Adelie Adelie ## [15] Adelie Adelie Adelie Adelie Adelie Adelie Adelie ## [22] Adelie Adelie Adelie Adelie Adelie Adelie Adelie ## [29] Adelie Adelie Adelie Adelie Adelie Adelie Adelie ## [36] Adelie Adelie Adelie Adelie Adelie Adelie Adelie ## [43] Adelie Adelie Adelie Adelie Adelie Adelie Adelie ## [50] Adelie Adelie Adelie Adelie Adelie Adelie Adelie ## [57] Adelie Adelie Adelie Adelie Adelie Adelie Adelie ## [64] Adelie Adelie Adelie Adelie Adelie Adelie Adelie ## [71] Adelie Adelie Adelie Adelie Adelie Adelie Adelie ## [78] Adelie Adelie Adelie Adelie Adelie Adelie Adelie ## [85] Adelie Adelie Adelie Adelie Adelie Adelie Adelie ## [92] Adelie Adelie Adelie Adelie Adelie Adelie Adelie ## [99] Adelie Adelie Adelie Adelie Adelie Adelie Adelie ## [106] Adelie Adelie Adelie Adelie Adelie Adelie Adelie ## [113] Adelie Adelie Adelie Adelie Adelie Adelie Adelie ## [120] Adelie Adelie Adelie Adelie Adelie Adelie Adelie ## [127] Adelie Adelie Adelie Adelie Adelie Adelie Adelie ## [134] Adelie Adelie Adelie Adelie Adelie Adelie Adelie ## [141] Adelie Adelie Adelie Adelie Adelie Adelie Adelie ## [148] Adelie Adelie Adelie Adelie Adelie Gentoo Gentoo ## [155] Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo ## [162] Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo ## [169] Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo ## [176] Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo ## [183] Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo ## [190] Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo ## [197] Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo ## [204] Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo ## [211] Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo ## [218] Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo ## [225] Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo ## [232] Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo ## [239] Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo ## [246] Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo ## [253] Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo ## [260] Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo ## [267] Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo Gentoo ## [274] Gentoo Gentoo Gentoo Chinstrap Chinstrap Chinstrap Chinstrap ## [281] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap ## [288] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap ## [295] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap ## [302] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap ## [309] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap ## [316] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap ## [323] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap ## [330] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap ## [337] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap ## [344] Chinstrap ## Levels: Adelie Chinstrap Gentoo Note though that the output here is different; it simply returns a vector, while the [row,column] notation returns a data frame. Given that many functions in R rely on vectors, this notation is often useful. Finally, if we want to select multiple rows or columns then we need to give vectors to the row and/or column arguments within the square brackets. This means that our semicolon notation will work here as well. For instance, let’s say we want to get the first 6 rows of the dataset: penguins[1:6,] Or, columns 2 to 4: penguins[,2:4] "],["packages-and-functions.html", "2.3 Packages and functions", " 2.3 Packages and functions 2.3.1 Functions R works primarily with functions, which take one or more inputs and return an output. Functions in R are always defined in name() format, i.e. the name of the function followed by brackets. Every function generally serves a very specific purpose, such as performing a specific calculation, manipulation of data or otherwise. Therefore, working with R requires us to use lots and lots of functions. Most functions will have at least one, if not multiple arguments. Arguments define the options for a given function. For example, the class() function, which comes in base R, tells us the type of a variable. class() has one main argument, x - which is simply the name of the variable we want to know about. As an example, let us say we wanted to know what type of variable var_a was. We could write the following: class(x = var_a) ## [1] &quot;numeric&quot; More simply, because class() only has one argument we can optionally write: class(var_a) ## [1] &quot;numeric&quot; Functions may either have mandatory or optional arguments. Mandatory arguments are ones that you need to provide in order for the function to run. Optional arguments are often defaults that can be changed if needed. Many functions in R will have multiple arguments that are typically a mix of both. A key point arises here though: functions in R define arguments in specific orders. In other words, R expects you to input arguments in specific orders unless you explicitly define each argument’s value, as we did for the first instance of class(). The easiest way to find out information about what a function does and the arguments it requires is to type a question mark, ?, with the name of the function immediately afterwards. e.g.  ?class A basic function is the round() function, which - as the name suggests - rounds a value you give it. round() has two arguments: x, which is the number or the name of an object we want to round, and digits, which specifies the number of digits we want to round to. x is a mandatory argument, but digits is optional and has a preset value of 0. Therefore, if we type in the following you can see what we get: round(4.326) ## [1] 4 If we want to round to 2dp, for instance, we would need to explicitly define the digits argument and set it to a different value. round(4.326, digits = 2) ## [1] 4.33 2.3.2 Packages By default, R comes with lots of functions, many of which will be used throughout this book. The beauty of R though is that its functionality is essentially limitless; its open-source nature and strong community mean that new functions and capabilities are regularly made for R. These new functions augment/extend what R is capable of doing, and are generally available in the form of packages. To provide a very simple explanation of what packages are, packages are a collection of code that provide extra functions in R. Some packages occassionally come with data too, such as the palmerpenguins package. When you start a new R session, the first thing that you’ll want to do is load the packages that you need to use. For now, we’ll load two packages for functions: tidyverse and rstatix. tidyverse is a huge package that contains a group of other packages designed for data manipulation, visualisation and cleaning. rstatix allows for simple statistical tests to be performed in an easy way. palmerpenguins comes with a dataset for practicing on. palmerpenguins loads a dataset named penguins that contains basic info on 3 species of penguins across 3 different islands. To load a package, call the library() function and enter the name of the package in brackets: library(tidyverse) ## ── Attaching core tidyverse packages ─────── ## ✔ dplyr 1.1.4 ✔ readr 2.1.5 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ ggplot2 3.5.1 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.4 ✔ tidyr 1.3.1 ## ✔ purrr 1.0.2 ## ── Conflicts ────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors library(rstatix) ## ## Attaching package: &#39;rstatix&#39; ## ## The following object is masked from &#39;package:stats&#39;: ## ## filter library(palmerpenguins) 2.3.3 Tidyverse The tidyverse is a mega-collection of phenomenal packages that fundamentally change how to interface with R. The tidyverse provides packages for things like: Wrangling data Reading and writing data Making graphs Tidying and reshaping data Scraping the web The tidyverse is probably the single most popular suite of packages for R because of the functionality it provides. All of the tidyverse packages are written in consistent syntax, generally use very easy language that has an emphasis on verbs (i.e. you’re telling R to do something) and integrate seamlessly with each other and R. The tidyverse is a philosophy of R just as much as it is a suite of functions, and is part of what makes R so powerful today. Many aspects of the tidyverse are reliant on the pipe operator, %&gt;%. This basically tells R to take a dataframe or output and pass it onto a function that comes directly afterwards. Any function that takes a data frame as its first argument can (theoretically) be piped, meaning that we can chain strings of functions together in one run in a readable way. See the example below: data %&gt;% function_1() %&gt;% function_2(do = &quot;this&quot;) %&gt;% function_3(avoid = &quot;that&quot;) in the hypothetical example above, we first take data, and pass it to function 1. We then take the output of function 1 and pass that to function 2, which has the argument do = \"this\". Afterwards, we take the output of function 2 and pass it to function 3. There are a number of clear benefits to the tidyverse way of doing things. These include: Functions are generally stated as verbs, which means that you’re always doing something with a function (and it’s clear what that something is) Piping avoids the cyclical hell of creating intermediate variables. Consider a non-tidyverse version of the code example above, written down below. This code is not only a bit of a pain to read, but is also clunky in that it generates several intermediate variables that aren’t all that useful (a lot of the time). output_1 &lt;- function_1(data) output_2 &lt;- function_2(output_1, do = &quot;this&quot;) output_3 &lt;- function_3(output_2, avoid = &quot;that&quot;) Piped code is generally quite easy to read. tidyverse also provides a consistent syntax for other packages! It provides a quasi-philosophy and style guide for developers to write their own packages to write ‘tidy’ packages. rstatix is a great example of this. Throughout this book you will see a lot of tidyverse! 2.3.4 The here package Another staple bit of code you will see throughout RPMP is the here package. The official vignette for here summarises what this package does: The here package enables easy file referencing by using the top-level directory of a file project to easily build file paths. This is in contrast to using setwd(), which is fragile and dependent on the way you order your files on your computer. Alternatively, read the below quote from R goddess Jenny Bryan: If the first line of your #rstats script is setwd(\"C:\\Users\\jenny\\path\\that\\only\\I\\have\"), I will come into your lab and SET YOUR COMPUTER ON FIRE. In short, here() allows us to locate files in a relative manner as opposed to an absolute one. This is super super useful for sharing your code and data with other people, and ensuring that your scripts will run no matter where they are. We won’t get too into project-oriented workflows for RPMP. However, imagine you have a folder structure like this: |-code |-----rpmp_week1.Rmd |-data |-----w1_dataset.csv |-output |-RPMP.rproj Normally, to locate a file on a disk you would generally have to give the entire pathway to that file. That could be something like \"C:\\Users\\Dan\\Documents\\Subjects\\RPMP\\data\\w1_dataset.csv\" - which is immensely unwieldy if we want to read in data - and won’t work the moment I give my script to someone else, as their folder structure could be completely different! The alternative with here() could be as simple as: here(&quot;data&quot;, &quot;w1_dataset.csv&quot;) This tells R that I’m looking for something here in the data folder, specifically a file named w1_dataset.csv. As long as the relative positions are correct - i.e. the .csv file is in the data folder - R will know where to locate the file. You will see a lot of here() in this version of the RPMP guide because as you may appreciate, there are a lot of data files stored in all manner of folders. We will talk specifically about using this function to read in data on the next page! "],["loading-data-into-r.html", "2.4 Loading data into R", " 2.4 Loading data into R Naturally, we cannot do any statistical analyses without reading in data. R can flexibly read in a number of data formats. 2.4.1 Setting up R projects Before we continue, it’s worth taking a moment here to pick up on the project workflow we mentioned on the previous page. By default, R will operate out of your Documents folder on both Windows and macOS. If you make an R project, you can essentially create an instance of R that starts from the folder you place your R project in. This can be really useful for keeping all of your files related to one project in one place! There are several guides online on how to initialise R projects, so we will not try to reinvent the wheel here. However, the basic idea is something like this:L Create a new R project. While creating the new project, either create a new folder for it or assign it to the existing folder. This will create an .rproj file in that folder. When working on a project, open the .rproj file. This will open RStudio in that project, which you can think of as an instance of R Studio. Work away! An example basic project structure might look like this: |-Documents |---RPMP (this is the folder for the project) |-----RPMP.rproj The .rproj file will sit within the folder you specify, meaning that all your files and filepaths will be indexed relative to that folder. For here, this means that you will now start from the RPMP folder, and not Documents. As we will touch on below, this is really useful for easily finding files! At a basic level, we recommend a simple file structure like this: |-Documents |---RPMP (this is the folder for the project) |-----RPMP.rproj |-----code |-----data |-----output The code folder in your project is your place for storing code, the data folder is for storing data and output is useful for saving any outputs you generate. 2.4.2 csv files By and large, the most common file format for R is the .csv file format. .csv stands for comma separated values, and is basically a file format that stores your data in text form, separated by commas. The commas indicate where your data’s columns are, and thus The basic structure of a .csv file is identical to that of a regular dataframe in R. The first column should typically indicate the column name/heading, and each row should contain values. .csvs can be easily created using Excel. If you have a dataset in Excel format, you can export an Excel spreadsheet as a .csv file by going File -&gt; Export. However, as .csv files are stored as plain text, they will not retain any special formatting that you might be accustomed to in an Excel spreadsheet. They will only store data in plain text. To read a .csv file in R, the read_csv() function from tidyverse or read.csv() from base R will work equally as well. Both functions require you to specify where your .csv file can be found. dataset &lt;- read_csv(&quot;Insert your file path here.csv&quot;) 2.4.3 Text files R can also read in plain text files in .txt format. This is very similar to the .csv format, in that your text file will typically have column headers in the first row, each row with one participant/observation’s values, and each column separated by spaces. tidyverse provides a function called read_tsv() to read in these files. dataset &lt;- read_tsv(&quot;Insert your file path here.txt&quot;) If, however, you are working with a file that has a different type of character between each column - for example, a slash (/) - then you can use the function read_delim(), which is a more generic form of the two above. With this function, you must specify the delimiter - i.e. what separates the columns in the files. This can be done using the delim = argument. dataset &lt;- read_delim(&quot;Insert your file path here.txt&quot;, delim = &quot;/&quot;) 2.4.4 SPSS files SPSS is still a popular program of choice in psychological sciences, and so a lot of the datasets you may come across may be in SPSS format. SPSS data files are in .sav format. Base R cannot natively read these files. However, the haven package provides a function called read_spss() that will read these files in for you. library(haven) dataset &lt;- read_spss(&quot;Insert your file path here.sav&quot;) 2.4.5 Using here to read data On the previous page, we talked about the here package, and how it enables easy pathing. here becomes really useful for reading in data, and it becomes even easier if you have an R project set up. If you have a project structure like the example below: |-RPMP |---code |-----rpmp_week1.Rmd |---data |-----w1_dataset.csv |---output |---RPMP.rproj Then you can use here() in conjunction with any of the data-reading functions above. As the data-reading functions primarily only need a filepath, you can use here() to create that filepath and point to the right place. Here is an example: dataset &lt;- read_csv(here(&quot;data&quot;, &quot;w1_dataset.csv&quot;)) The here() call tells R to look in the data folder, and then look for w1_dataset.csv. Remember, in an R project, every file is indexed relative to the project folder. This means that we don’t need to faff around with finding out where our RPMP folder is on our computer, because essentially that’s where we start! The filepath to this file will then be used as the argument for read_csv(). Even if you are not using a project structure (although we highly recommend you do), you can still use here() - so long as the file can be located relative to your current working directory, which can be obtained using setwd(). However, it’s usually worth saving yourself the hassle and creating a new R project for substantive bits of work, and saving files as folders within that project’s folder. "],["wrangling-data-with-dplyr-and-others.html", "2.5 Wrangling data with dplyr and others", " 2.5 Wrangling data with dplyr and others dplyr is a package within the tidyverse for manipulating and wrangling data. dplyr is one of the most popular packages on R because it provides a suite of functions that are fairly essential to manipulating and working with data. Below is a brief overview of some of these functions, applied to the penguins dataset. 2.5.1 Selecting columns with select() select() lets you select the columns you want from a dataset. Simply specify the columns that you want by name. Below we take the species and island columns from the penguins dataset: penguins %&gt;% select(species, island) You can select columns via a number of ways: Simply by name, e.g. select(species, island) By their index or column number, e.g. select(1, 2) select(1:4) will select columns 1 to 4 select(-1) will select the last column By certain operator functions, such as starts_with() and ends_with(), e.g. ends_with(\"mm\") will select all columns that end with “mm” Combinations of the above also work. Removing columns is simply done by adding a minus sign - in front of the arguments for select, and are compatible with all of the options above. 2.5.2 Filtering rows with filter() filter() selects the rows that you want based on a certain condition. Here, we specify the column that want to filter by and state the condition (== means equals to). We can filter on multiple conditions; for example, filtering Adelie penguins by the year 2007: penguins %&gt;% filter(year == 2007, species == &quot;Adelie&quot;) You can also choose to filter out rows based on a condition by adding an exclamation mark in front of the column name. penguins %&gt;% filter(!year == 2007) drop_na() is another useful starting function that simply removes all rows with NA/empty cells. If you enter it as is then it will clean the entire dataset; if you specify a column then it will remove all rows with NAs in that column. Below is an example of a pipe using these functions - going from selecting columns to filtering rows and finally cleaning up the empty cells. penguins %&gt;% select(species, body_mass_g, year) %&gt;% filter(year == 2009) %&gt;% drop_na() 2.5.3 Creating new columns with mutate() mutate() is a function that lets you create new columns. This can be extremely useful for operations like recoding variables and transforming them. The nice thing about mutate() is that you can do all manner of operations without touching your original data. The basic workflow of mutate() looks like this: data %&gt;% mutate( new_column_1 = a_function(...), new_column_2 = another_function(...) ) This example code would create two new columns, named new_column_1 and new_column_2, with their respective values being whatever the functions were. For example, in the penguins dataset we have a variable called body_mass_g, which is the body mass of each penguin in grams. If we wanted to convert this to kilograms, we would need to divide each penguin’s value on this variable by 1000. mutate() makes this a piece of cake. Let’s also chain a select() command to only show the following variables: species, island, body_mass_g and sex. penguins %&gt;% select(species, island, body_mass_g, sex) %&gt;% mutate(body_mass_kg = body_mass_g/1000) You can see now that we have a new column called body_mass_kg that has our new transformed variable. mutate() can also take functions (and likely will make up the majority of your use of it). For example, let’s say that we want to make a variable that takes the natural logarithm of bill length (for whatever reason). We could do this as follows using the log() function within our mutate() call: penguins %&gt;% select(species, island, bill_length_mm, bill_depth_mm) %&gt;% mutate(bill_length_log = log(bill_length_mm)) 2.5.4 Summarising data with summarise() and group_by() Finally, sometimes we will want to summarise data - for example, to calculate basic features such as descriptives or for plotting. To do that, we can use the function summarise() (or summarize() for American users). summarise() works very similarly to mutate(). data %&gt;% summarise( summary_1 = a_function(...), summary_2 = another_function(...) ) The difference is that while mutate() retains the features of your data, summarise() will instead collapse it. To illustrate, let’s say we want to calculate a) how many penguins there are (with the function n()) and b) the mean body mass (with the mean() function). penguins %&gt;% summarise( n_penguins = n(), mean_mass = mean(body_mass_g, na.rm = TRUE) ) This is… good and all, but consider what we’ve just done. We’ve just calculated the number of penguins and the mean body mass across the entire dataset. However, that may not necessarily be meaningful, particularly in this instance where we have meaningful groups within the data. For example, the above mean collapses across years, which may not be appropriate. Enter in another function called group_by(). As the name implies, group_by() will perform operations per a grouping variable that you specify. group_by() works especially well with summarise, because the idea is something like this: data %&gt;% group_by(variable) %&gt;% # Tell R to group the subsequent output by this variable summarise( summary_1 = a_function(...), summary_2 = another_function(...) ) %&gt;% ungroup() # Tell R grouping is no longer needed Let’s put this into practice by calculating the n and mean per year. Notice how the output now calculates n and the mean body mass per year, which is much more informative! penguins %&gt;% group_by(year) %&gt;% summarise( n_penguins = n(), mean_mass = mean(body_mass_g, na.rm = TRUE) ) %&gt;% ungroup() Naturally, group_by() can group using multiple variables. This is easy to do so as well penguins %&gt;% group_by(year, island) %&gt;% summarise( n_penguins = n(), mean_mass = mean(body_mass_g, na.rm = TRUE) ) %&gt;% ungroup() ## `summarise()` has grouped output by &#39;year&#39;. ## You can override using the `.groups` ## argument. Suddenly this is much more informative - we can now do calculations/operations per year and island, which provides a lot more nuance. 2.5.5 Some other handy tidyverse functions As stated at the start of this chapter, this book will only cover enough R functions to provide you with an understanding of what goes on in this book and how. Nonetheless, there are so many tidyverse functions out there that are worth exploring and knowing about. Below is a brief list of some other functions from dplyr you may wish to keep in mind. To find out what a given function does in more detail, you just need to type ?function into the R console to search for its documentation (or ??x to do a broad search). Note that all of these are dplyr functions, so need to be piped from a dataset like usual. arrange() will sort your rows by a variable you specify. For example, if you wanted to sort the penguins dataset by island, you could use arrange(island) (or arrange(desc(island)) for descending order). distinct() will give you all unique values in a given column. distinct(island), for instance, will give you each unique island name. rename() will let you rename columns. relocate() will let you rearrange the column order. The slice() set of functions will subset rows, but mainly based on positions (e.g. first, last) rather than conditions. "],["ggplot.html", "2.6 Making graphs with ggplot2", " 2.6 Making graphs with ggplot2 2.6.1 Building the plot ggplot2 (henceforth referred to as ggplot) is the tidyverse package for plotting and visualising data. It is an immensely powerful and flexible way of graphing data in R, and is basically the de facto means of creating visualisations for R users.1 The ‘gg’ in the name ggplot stands for grammar of graphics. The idea is that a graph is built by layering different components of a graph (the graphics) in a structured way (the grammar). The idea was first put forward by Leland Wilkinson, and looks something like this: A cheatsheet for ggplot can be found here. To start a plot, we first call the ggplot() function. Here, we specify three of the main features of our plot - the dataset that we want to use, the x axis and the y axis. the x and y axes are wrapped within the aes() function, which defines our aesthetics. Here, we simply say which columns of our dataset should go on the x and y axes. Using the penguin dataset, we can start to visualise a scatter plot between bill length and bill depth like this: ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm)) However, notice that the plot is empty. This is because we’ve only specified the first two layers of our graph: the data and the aesthetics. We haven’t defined any geoms, or actual plotting methods, in our plot. 2.6.2 Geoms Geoms (short for geometries) define how data is plotted. If ggplot() provides the basic canvas for the graph (i.e. what to plot), geoms are what create the graph (i.e. how it is plotted). Different graph types are defined using geoms. As per the diagram of the grammar of graphics above, we add (literally, using +) additional layers to our base ggplot() call in order to build our graph. Refer to the cheatsheet for all of the possible geoms. For now, we will stick to some basics. A scatter plot can be specified by adding geom_point(). This requires that your x and y variables are continuous: ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point() geom_smooth() will add a line of best fit to a scatterplot. You can add this as another geom layer by adding geom_smooth(method = \"lm\"). Specifying the method is important because by default, geom_smooth() will probably fit LOESS curves (local polynomial regressions). Note that this function should be added after using geom_point(). By default, the line will also have standard error bands around it. You can turn this off by also specifying se = FALSE in the geom_smooth() call. ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; geom_boxplot() will create boxplots. For this geom to work, a categorical variable must be on the x axis and a continuous variable must be on the y axis. An example is below, with species on the x axis and body mass on the y: ggplot(data = penguins, aes(x = species, y = body_mass_g)) + geom_boxplot() geom_violin() plots violin plots, which is a variant of the boxplot that also plots the distribution. ggplot(data = penguins, aes(x = species, y = body_mass_g)) + geom_violin() geom_bar() and geom_col() will both create bar plots, but their usage is slightly different. geom_bar() is best used if you want to plot the number of items in each category. geom_col() is best used to plot means or similar statistics. penguins %&gt;% group_by(species) %&gt;% summarise( mean_mass = mean(body_mass_g, na.rm = TRUE) ) %&gt;% ggplot(aes(x = species, y = mean_mass)) + geom_col() geom_histogram() will plot a histogram, which is useful for visualising distributions. For this, you only need to provide one continuous variable on the x-axis. geom_density() will plot the same information but using a smoothed line instead of bins. ggplot(data = penguins, aes(x = body_mass_g)) + geom_histogram() ggplot(data = penguins, aes(x = body_mass_g)) + geom_density() 2.6.3 Making the graph look better This is all well and good, and we already have workable basic graphs using ggplot. From here, we can change many things by adding or modifying our existing layers to our ggplot. There are so many options available here, but here we’ll only focus on some basic considerations. An important aspect of data visualisation is a good use of colour. Colours should be used in both an informative and an aesthetically appealing way. A particularly common but effective use of colour is to use different colours to denote different groups in a plot. ggplot provides two mechanisms within aesthetics for controlling colour. Both must typically be included in the aes() part of ggplot(). colour controls the colouring of points and lines. For graphs with shapes, such as boxplots and violin plots, this argument will control the colour of the border. fill controls the colouring of anything over an area, such as bar plots, histograms and density/violin plots. We can tell gggplot to colour in aspects of our graph based on another variable in the dataset we are using. For example, if we want to colour a scatterplot between two vraiables by sex (which is a column in the penguins data), we can do so by specifiyng colour = sex in our aes() function. ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, colour = sex)) + geom_point() Many graphs allow for control of both colour and fill. In these instances, as noted above, fill controls the colour within the shape while colour controls the colour of the shape’s edges. For example, here is a density plot that specifies both colour and fill to be controlled by the variable island. This has the effect of making both the borders and the area within the shape the same colour. ggplot(data = penguins, aes(x = body_mass_g, fill = island, colour = island)) + geom_density() Now, this is all well and good but in this particular instance the overlapping of curves means that we can’t actually see what is going on very well. The data points from Torgersen island, for instance, almost completely overlap with the data points from Dream Island. One way to get past this is to specify another aesthetic called alpha, which sets the transparency of the fill. As we typically only want to change the transparency of one type of geom in a plot, the best place to include an alpha argument is within the geom call itself (in this case, within geom_density()). alpha can range from 0 - 1, where 1 means an object is 100% opaque (i.e. 0% transparent) and 0 means 0% opaqueness (100% transparency). Here, we use alpha = 0.5 to set the fill to be 50% transparent: ggplot(data = penguins, aes(x = body_mass_g, fill = island, colour = island)) + geom_density(alpha = 0.5) Now we can see the outlines of each density curve more clearly! This is great. ggplot also supports the manual specification of colours. R by default comes with a bunch of strings that are recognised as colours during graphing, such as \"black\", \"blue\" and \"red\". These can be used to manually set either the colour or the fill of a geom. To use these, you can instead use colour/fill within the geom like so: ggplot(data = penguins, aes(x = species, y = body_mass_g)) + geom_boxplot(colour = &quot;blue&quot;) You can also provide hexadecimal strings (e.g. “white” corresponds to \"#FFFFFF\"). See the Appendix for a full list of the basic palettes in R. Every graph needs good axis titles. By default, ggplot will use variable names as axis labels, which often aren’t very informative by default. We can change this by adding labs(), which is a simple way of specifying x and y labels: ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point() + labs(x = &quot;Bill length (mm)&quot;, y = &quot;Bill depth (mm)&quot;) Finally, if you want to split plots by a certain variable, add facet_wrap() to your call. You need to specify what variable/column you want to split by, with a tilde in front. For example, if we wanted to split the scatter plot by year: ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, colour = sex)) + geom_point() + labs(x = &quot;Bill length (mm)&quot;, y = &quot;Bill depth (mm)&quot;) + facet_wrap(~year) You can see that the graph now creates one graph per year, which can be immensely useful for visualising data points between groups. From here, there are so many things you can do with ggplot - and it helps to get creative! Almost all of the graphs in RPMP (in fact, I think it actually is 100%) were made using ggplot2.↩︎ "],["descriptives.html", "Chapter 3 Descriptive statistics (Module 5)", " Chapter 3 Descriptive statistics (Module 5) Descriptive statistics, as the name implies, are used to describe data. A key part of the quantitative research process is understanding the various ins and outs of your data. You’ll probably have a sense of why this is important if you have the qualitative presentation still fresh in your mind - namely, knowing your data is also really important for knowing what to do with it. In this first module, we will start with the first steps of understanding quantitative data. This involves visualising our data to see what it looks like, and describing key features of the data. If you’re familiar with statistics then some of this may seem a bit trivial, but it’s important that we get the basics right before we go on to doing fancy statistical tests. Also, I know that the idea of doing statistics and maths freaks a lot of us out - and that’s totally normal. Yes, there will be some number-crunching and maths in the next series of modules, but the focus of these modules is not to force you to calculate things by hand. You will encounter a whole bunch of mathematical formulae, but the point of doing so is to illustrate the concepts that underpin them. These concepts are crucial to understanding the ‘magic’ that happens with quantitative analysis, and a really solid foundation in statistical concepts will go a long way. That being said, throughout these statistics modules there will be a number of activities that ask you to actively work with sample datasets and analyse them. We promise that you will get so much more out of these modules if you complete these activities, because readings and webpages aren’t the best substitute for actually doing it and getting your hands dirty with data. By the end of this module you should be able to: Create both appropriate and meaningful graphs from data Calculate various forms of descriptive statistics Interpret both graphs and descriptive statistics, and explain what they tell you Figure 3.1: xkcd: Statistics "],["visualising-data.html", "3.1 Visualising data", " 3.1 Visualising data When you have data in your hand, it is often tempting to dive straight into plugging it into an analysis and seeing what the results are. However, in general this is unwise. The first step in working with quantitative data is to see what your data looks like. Looking at the data can give us a first glance into many aspects of the results, which can be informative for analyses. 3.1.1 Why visualise data? “The greatest value of a picture is when it forces us to notice what we never expected to see.” -John W. Tukey One of the first steps in working with quantitative data is data visualisation, which is the process of graphing it and looking at it. If you work with quantitative data then it should become standard practice for you to graph your data: specifically, after you’ve defined your research questions and methods of analysis, but before you actually analyse it. It’s common for many students learning research methods and statistics to simply take a ‘cookie cutter’ approach - that is, collect data, run basic tests on it and call it a day. Sadly this is common even at our level, and you will almost certainly see this happen at music psychology conferences that you go to in future. People will present complex analyses that sound impressive - until you pick up on small cues that suggest they don’t really understand their data at all. Below is an example of why data visualisation should be a crucial part of the quantitative research process: This series of datasets is called the Datasaurus Dozen, which are 12 datasets that look entirely different (including the dinosaur!) but share almost identical summary statistics, as shown by the bold numbers on the right. Data visualisation is important because: It lets you observe patterns in your data. It can reveal unexpected structures in your data that would normally be missed otherwise. It is an effective way of communicating information. The best graphs tell a reader everything they need to know in one image. In the Canvas version of this subject, there are some general guidelines as to how to make good figures right around this point. For this version of the book, the section on ggplot is going to be infinitely better. Regardless of which graph you use, every good graph should have the basic following features: Content made with H5P. "],["basic-desc.html", "3.2 Counts and central tendencies", " 3.2 Counts and central tendencies Once we understand what our data looks like, we can then move to describing the general properties of the data. Such general properties are called descriptive statistics. Reporting descriptive statistics is crucial for many aspects of quantitative research. 3.2.1 Basic features There are a couple of basic features of any dataset that should be looked at and noted: Name (APA Symbol) Definition When to report? Count (n) The number of data points. The number of participants should always be reported - not just for the sample as a whole, but for each analysis done. Range In the context of writing up statistics, this is usually the minimum and maximum values. Reporting these values is often useful as a range when writing up demographic variables, e.g. age or years of training. Percentages Use primarily for categorical data, e.g. sex or groups. We can use R to find some of these values, either using straight base R or tidyverse functions. For this page/module only, I will use the variable_a mock variable from Section 2.2.2 with a minor amendment: vector_a &lt;- c(4, 1, 6, 2, 3, 4) vector_a ## [1] 4 1 6 2 3 4 For tidyverse usage, I’ll refer to df_a, which is just the same but as a one-column dataframe: df_a To find the count, or the number of items in a vector, we can use the length() function. length(vector_a) ## [1] 6 To find the minimum and maximum, we can use the min() and max() functions respectively. Specifying na.rm = TRUE will remove any missing data before calculation. min(vector_a, na.rm = TRUE) ## [1] 1 max(vector_a, na.rm = TRUE) ## [1] 6 In tidyverse fashion, we can wrap this all in summarise() as follows: df_a %&gt;% summarise( n = n(), min = min(column_a, na.rm = TRUE), max = max(column_a, na.rm = TRUE) ) Note here that rather than using length(), we use a function called n(). This function only works within summarise() and mutate(), but is essentially shorthand for length(). 3.2.2 Central tendencies While the range can be informative in some sitautions, it usually isn’t enough to draw deeper interpretations from raw data. One key way of describing data is in terms of central tendency - or where the ‘average’ value approximately is. There are three main types of central tendency, summarized in the table below. Name (APA Symbol) Definition When to use? Things to note Mean (M) The sum of all values, divided by the number of data points. Use if data is normally distributed. Can be influenced by outliers, so generally unsuitable when data is skewed. Median (Mdn) The ‘middle’ data point, when sorted in order. Use for skewed data, or for ordinal data. Generally is less preferable to the mean, except for use in skewed/ordinal data. Mode The most frequent value. Use for nominal data. Unsuitable for most other types of data. To calculate a mean and median, use the mean() and median() functions respectively. Both functions also take the na.rm argument. mean(vector_a, na.rm = TRUE) ## [1] 3.333333 median(vector_a, na.rm = TRUE) ## [1] 3.5 # Using summarise() and piping df_a %&gt;% summarise( mean = mean(column_a, na.rm = TRUE), median = median(column_a, na.rm = TRUE) ) Interestingly, R doesn’t offer a base function to calculate a mode - if you need this information, you either need to manually work this out or turn to a package that offers it. One such example is the fantastic DescTools package, which provides a function called Mode(): DescTools::Mode(vector_a) ## [1] 4 ## attr(,&quot;freq&quot;) ## [1] 2 The first number is the value of the mode, while the second number is the number of times the mode occurs (twice, in this case). "],["variability.html", "3.3 Variability", " 3.3 Variability The other important part of describing data is in how spread out it is. Is our data tightly bunched together, or is it very spread out? This helps us understand where most of our data falls, as well as how it looks. 3.3.1 The variability of data The other key way of describing data is in its spread, or distribution. The way data is distributed can give key insights into how that data should be treated. Consider the following graphs below. You can see that all three graphs peak at around the same point, but look very different outside of that. The orange line is narrow, while the red line is considerably more spread out. All of these graphs peak at the same point but still look very different. Therefore, they have very different spreads, or distributions. We saw on the last page that we can quantify how far values are spread apart by finding the range. However, this isn’t always a good idea - two datasets with the exact same range can look wildly different. Therefore, we need ways of quantifying how data is spread out as well. 3.3.2 Standard deviation Standard deviation (\\(\\sigma\\), or SD) describes how spread out our data is within our sample, in standard (i.e. comparable) units. Data that is spread out widely (like the red curve above) will have a large standard deviation; likewise, data that has a narrow spread will have a small standard deviation. We’ll touch on this a bit more in the following pages, but for now just remember what a standard deviation is for. To calculate standard deviation, we first calculate variance, which is another measure of spread: \\[ Variance = \\frac{\\Sigma (x_i - \\bar{x} )^2}{n - 1} \\] Or, in human terms: Take each data point (\\(x_i\\)) Subtract the mean from each data point (x with the bar) and square that difference Add them all up together Divide by \\(n - 1\\) And then to calculate standard deviation, we simply take the square root of the variance. \\[ SD = \\sqrt{Variance} \\] Or, in full formula form: \\[ SD = \\sqrt{\\frac{\\Sigma (x_i - \\bar{x} )^2}{n - 1}} \\] Standard deviations (SD) should reported alongside means when results are written up (consult an APA guide). To calculate standard deviations in R, use the sd() function. Once again, this has an na.rm argument you can specify. sd(vector_a, na.rm = TRUE) ## [1] 1.75119 # Tidyverse form df_a %&gt;% summarise( sd = sd(column_a, na.rm = TRUE) ) 3.3.3 Standard error, and the SDoTM Imagine that I have a population of 100 regular people (shown on the left). I take a sample of 10 people, measure their heights and then calculate the mean height of that one sample. I then repeat this process over and over again, and plot where each sample’s mean falls. Of course, because every sample is slightly different the mean of each sample will be slightly different too due to sampling error. Some sample means will be lower than the true population mean, while some will be higher. Eventually, we might end up with something like this: The spread of these sample means is called the sampling distribution of the mean (SDoTM), shown on the right. This gives us a sense of where the population mean (the parameter that we are interested in) might lie. With enough samples, the peak of this sampling distribution of the mean will converge around the population mean. As you can see in our hypothetical example, the peak of the sampling distribution of the mean sits pretty close to the original population mean, meaning our estimate is pretty good. The standard error of the mean (standard error; SE) is another measure of variability - this time, it is the spread of sample means across the sampling distribution of the mean. This represents how close our sample mean is to the likely population mean. If our sampling distribution is wide, our standard error will be large - and that means that we won’t have a very precise estimate of the population mean. However, if we have a small standard error that will mean that our sample mean is likely to be close to the population mean. Standard error is calculated using the below formula: \\[ SE = \\frac{SD}{\\sqrt n} \\] Where SD = standard deviation, and n = sample size. Practice: You have a dataset of 400 people. You know that the mean of the DV is 760, with a standard deviation of 40. Calculate the standard error for this sample. "],["distributions.html", "3.4 Distributions", " 3.4 Distributions The ‘shape’ of our data is equally important. What does our data actually look like? Does it even matter what it looks like? The topic of distributions in statistics and probability can make up its own subject (in fact it does), but here we discuss the basics below. 3.4.1 The normal distribution Earlier, we saw a series of graphs overlaid on top of each other. These graphs, while having different variability, were essentially all the same shape - they were symmetrical bell curves. These were all examples of the normal distribution (also called the Gaussian distribution). The classic normal distribution takes on a neat bell-shaped curve: In the normal distribution, the majority of data points cluster in the middle, while all other values are symmetrically distributed from either side from the middle. This is what gives the normal distribution its recognisable bell shape. The normal distribution is defined by two parameters: the mean and the standard deviation of the data. These two parameters define the overall shape of the bell curve - the mean defines where the peak is, while the standard deviation defines how spread out the tails are. An important feature of the normal distribution is where all of the data is spread, regardless of its shape: 95% of the data within the curve falls within 1.96 standard deviations, either side of the mean. This applies to any normal distribution no matter what the scale of the data is. 99.7% of data falls within just below 3 standard deviations. 3.4.2 Graphing distributions The slides below demonstrate a couple of ways in which you can graph distributions. 3.4.3 Skew Skewness, as the name implies, describes whether or not a distribution is symmetrical or skewed. If a distribution is skewed, we would expect numbers to be bunched up at one end of the distribution. Have a look at the three graphs below: The purple graph in the middle is symmetrically distributed, so we say that it has no skew. The red graph has values that are weighted towards the right-hand side of the x-axis, and so we say that it is either skewed left or negatively skewed. The blue graph, on the other hand is skewed right or positively skewed. The left-right refers to which end the tail of the distribution is on. Skewness can also be quantified numerically: A skewness of 0 means that a distribution is normal A positive skew value means that the data is skewed right A negative skew value means that the data is skewed left As a general rule, if a distribution has a skew greater than +1 or lower than -1, it is skewed. If your data is skewed then this is not the end of the world; it depends on the analysis you are performing, or what you are trying to do with the data. We will touch on this a bit more in coming weeks. 3.4.4 Kurtosis Kurtosis refers to the shape of the tails specifically. Are all of the data bunched very tightly around one value, or are the data evenly spread out? The three graphs you saw up above all have different kurtoses. The orange graph has most values very close to the peak at 50; therefore, the tails themselves are very small. The red line, on the other hand, is spread out and flatter so the tails are larger. The blue curve again approximates a normal distribution. We can quantify kurtosis through the idea of excess kurtosis - in other words, how far does it deviate from what we see in a normal distribution. This is shown below: The different types of excess kurtoses are: Leptokurtic (heavy-tailed) - tails are smaller. Kurtosis &gt; 1 Mesokurtic - normally distributed. Kurtosis is close to 0 Platykurtic (short-tailed) - tails are larger, and the peak is flatter. Kurtosis &lt; -1 Therefore, in the example above the orange curve would be considered leptokurtic, while the red one would be platykurtic. Below are a series of skewness and kurtosis values from three different data sets. For each: Determine if the data is skewed or not, and if so then what type of skew Determine the type of kurtosis Sketch a rough version of what this skew and kurtosis might look like (doesn’t have to be perfect!) Dataset A Dataset B Dataset C Skewness 0.3209 5.2934 -3.1945 Kurtosis -0.1023 10.9238 -2.7263 "],["clt.html", "3.5 Central Limit Theorem", " 3.5 Central Limit Theorem In Module 6, we will cover the foundations of statistical tests. However, in order to understand what those tests tell us and how useful they are, it is important to basically look at what allows them to work in the first place. In comes the Central Limit Theorem, one of the most important concepts in all of statistics. You’ll want to keep the concept of the sampling distribution of the mean fresh in mind for this page, as it all relates to that! 3.5.1 What is the Central Limit Theorem? The Central Limit Theorem (CLT) is a fundamental theorem of probability theory. It states that under the right conditions, the sampling distribution of the mean will converge to a normal distribution. This occurs even when the original data are not normally distributed. Why is this important? After all, it’s not like this is something we immediately see in action most of the time. Put simply, without the CLT we would not be able to do any of the statistics we do. The CLT allows us to make statistical inferences even when we don’t know the true nature (i.e. distribution) of our data by using the normal distribution to test hypotheses. The fact that it applies even when we have skewed or non-normal data means that we can still make valuable inferences in these scenarios as well. Therefore, it is critical ‘under the hood’ to all of the statistical tests we run. 3.5.2 Simulation To test this for yourself, try the below sample simulator. You can set what distribution you want to draw from, and choose how many samples and simulations you want to run. The Population Distribution tab will show you what you are sampling from; the Samples tab is each individual sample and the Sampling Distribution tab shows the distribution of sample means. Try and change the sample size and see how that impacts on the Sampling Distribution. (You may need to scroll within the app to see the full output.) 3.5.3 Some important points A general rule of thumb for sample sizes is that n &gt; 30 is sufficient even when the population is skewed. In other words, even if a population is heavily skewed on a variable, taking several samples of n &gt; 30 will still show a normally distributed set of sample means. You can see this for yourself in the simulator above - try set sample size to 5, 10 and then 30, and see what happens in the Sampling Distribution tab. In addition, remember that with bigger samples, the variability in sample means (i.e. standard error) decreases - and therefore, the sample mean gets closer to the population mean. This means that with large samples, we should ideally be getting a really good estimate of the population of interest! Conversely, smaller sample sizes (as is common in music research) are unlikely to be good estimates of populations. We will touch on the issue of sample size more in Module 6 but this should already also give you an indication of one of the most important things when it comes to statistical tests: sample size matters. "],["zscores.html", "3.6 z-scores", " 3.6 z-scores The last major component of this week is about a really useful but important property of the normal distribution (which, as you may have guessed, is fairly important in statistics. The process of standardising data and calculating z-scores is one that we actually use a lot in statistics. Let’s briefly recap where we’re at so far: We’ve covered basic descriptive statistics, such as means, standard deviations etc etc. We’ve talked a bit about the normal distribution and its properties - specifically, that 95% of your data lies within 1.96 SD either way of the mean If you’ve got those concepts down, the rest of this page will be fairly straightforward. 3.6.1 z-scores z-scores (z), sometimes called standard scores, are a measure that describe how many standard deviations a single data point is from the mean. If you recall the figure of the normal distribution from the previous page, notice how we quantify how much data is captured in terms of the number of standard deviations. z-scores are essentially this number - in other words, 95% of your data lies between z = -1.96 and z = 1.96. The process of calculating z-scores is called standardisation. The primary utility of converting data into z-scores is that it becomes possible to compare data on different scales. Many statistical analyses employ some form of standardisation for a variety of reasons - some of which we’ll see in this subject. 3.6.2 Calculating z-scores The formula for converting a raw data point into a z-score is: \\[ z = \\frac{x - \\mu}{\\sigma} \\] Where x = an individual data point, \\(\\mu\\) = mean and \\(\\sigma\\) = SD. For example - in their paper on the Goldsmiths Musical Sophistication Index, Mullensiefen et al. (2014) show that their general sophistication measure has a mean of 81.58, with an SD of 20.62. If a participant scores 100, we can calculate a z-score to see how many standard deviations they are away from the mean: \\[ z = \\frac{100 - 81.58}{20.62} \\] \\[ z = 0.8933 \\] A participant with a general sophistication score of 100 would be roughly 0.89 standard deviations away from the mean. To z-score a vector in R, we use the scale() function. The scale() function takes two arguments: center, which determines whether the data is centered (i.e. subtracts the mean from each value), and scale, which essentially scales the data so the SD is 1. By default, both of these arguments are true. scale(vector_a) ## [,1] ## [1,] 0.3806935 ## [2,] -1.3324272 ## [3,] 1.5227740 ## [4,] -0.7613870 ## [5,] -0.1903467 ## [6,] 0.3806935 ## attr(,&quot;scaled:center&quot;) ## [1] 3.333333 ## attr(,&quot;scaled:scale&quot;) ## [1] 1.75119 3.6.3 Comparing across scales As mentioned above, we can use z-scores to compare across measures on different scales. This becomes really useful when we want to compare two participants, for instance, or two different measures. This is simply done by calculating a z-score for each formula - as long as you know the mean and standard deviation of each scale as well. As a simplistic example, let’s say we have two scales: Measure A sits on a scale of 0 - 100, with a mean of 50 and a standard deviation of 5 Measure B sits on a scale of 0 - 80, with a mean of 45 and a standard deviation of 4 If a participant scores 40 on both scales, clearly we can’t compare them directly - a 40/100 is vastly different to a 40/80! But we could convert these into z-scores to see where the participant sits on each scale: \\(z_a = \\frac{40 - 50}{5}\\), and \\(z_b = \\frac{40 - 50}{5}\\) \\(z_a = \\frac{-10}{5}\\), and \\(z_b = \\frac{-5}{4}\\) \\(z_a = -2, z_b = -1.25\\) In other words, the participant’s z-score on Measure A is -2, and -1.25 on Measure B. Based on this, we can say that the participant scored slightly higher (relatively) on Measure B compared to Measure A. A z-score lets us see how many standard deviations away from the mean a participant is. However, a more intuitive way of thinking about this is what percentile they sit in. To do this, we use something called a z-table. This z-table, in short, allows us to work out this percentage. Most versions of the z-tables will present two separate z-tables: one for negative z-values, and one for positive z-values (credit: https://zoebeesley.com/2018/11/13/z-table/) Here are the steps to read this table: Choose which table to read first. If you have a negative z-score, read the left one; if you have a positive z-score, read the right one. The rows and columns are basically arranged by decimal place. The rows index z-scores to 1dp, while the columns add the second decimal place. So, find the row first that corresponds to your z-score. In our example from above, our z-score was 0.89, so we want to find the row corresponding to 0.8. Next, find the column that corresponds to the second decimal place. We want to go all the way to the right-hand column labelled .09, to find the right column for our z-score of 0.89. Find the cell that corresponds to the row and column from above - that is the probability of getting a value below our z-score. In this instance, our z-score of 0.89 has an associated probability of .8133, meaning that 81.3% of scores are below this z-score. R, however, has a way of finding probabilities (percentiles) for z-scores for you. The pnorm() function calculates the probability of a specified value on a normal distribution. Given that z-scores follow the normal distribution, we can use pnorm() to calculate a given z-score’s associated probability. The function is simple: it requires you to give the z-score (as argument q), the mean and standard deviation of the normal distribution you are interested in. By default, the mean is set to 0 and the SD set to 1, which is what we want for a z-score. pnorm(0.89) ## [1] 0.8132671 "],["inferential-statistics.html", "Chapter 4 Inferential statistics", " Chapter 4 Inferential statistics From a conceptual point of view, this week’s module might just be the most important that we cover in this subject. I don’t say that to freak you out, but in many ways it just is the genuine truth. The reason that this might be the most important one is because the topics that we cover in this module are ones that many people frequently misunderstand or misappropriate - to the point of scientific fraud. Inferential statistics are the subject of very heated debate in statistics, primarily about whether p-values and the like are really the right way to do science and statistics. We won’t really go there because that’s neither here nor there in terms of what the aim of this module really is: to not only show you one way of hypothesis testing, but to also equip you with the knowledge needed to understand how other people test hypotheses - or, sometimes, fail to do so. Before you go into this module - make sure that you have 5.4 Variability, 5.5 Distributions and 5.6 Central Limit Theorem firmly under your belt as they will be relevant here. By the end of this module you should be able to: Describe the steps taken to test a statistical null and alternative hypothesis Correctly define a p-value Explain the difference between Type I and Type II error, and how these relate to statistical power Construct a basic confidence interval for a point estimate, and interpret it Figure 4.1: xkcd: Null Hypothesis "],["the-logic-of-hypothesis-testing.html", "4.1 The logic of hypothesis testing", " 4.1 The logic of hypothesis testing Central to all statistical testing is the underlying logic of hypothesis testing. All of the statistical tests that we cover in this module are built on this logic - and so it marks a great place for us to start our venture into inferential statistics. 4.1.1 Revisiting the hypotheses In Module 4 on Canvas, you will have had the chance to write your own hypothesis. This hypothesis guides the overall research and methodological design of our studies. However, you may have noticed that the hypotheses discussed in the video are not quite in the same format. This is because ‘hypothesis’ in this context refers to statistical hypotheses. Statistical hypotheses are formal statements that we use when testing for an effect. As mentioned in the video above, we propose two contrasting hypotheses when we want to do statistical testing: The null hypothesis (\\(H_0\\)) - that there is no effect or difference The alternative hypothesis (\\(H_1\\)) - that there is an effect or difference We can never know for sure whether one hypothesis is correct over the other. Instead, we choose to either reject or not reject the null hypothesis. 4.1.2 The issue of tails There are two main types of alternative hypotheses: Two-tailed: we hypothesise that something is happening regardless of whether it’s greater, smaller, increasing, decreasing etc… One-tailed: we hypothesise that the effect has a direction In a two-tailed hypothesis, we predict that an effect is occuring but we don’t predict anything beyond that. For example, if we are comparing whether two groups are different our alternative hypothesis would be that they are different - but we don’t predict whether group A will be bigger than group B or vice versa. These are sometimes called non-directional hypotheses. The blue areas on the curve on the right represent a 0.05 level of signifiance for a two-tailed hypothesis test. This is essentially how likely it is that we would observe our effect/difference if nothing was happening. If a difference is large enough that it falls within these blue tails, it is statistically significant. This is because if nothing really was happening, it would be unlikely that we see an effect/difference this big. Therefore, we would choose to reject the null hypothesis at this would fall below our chosen significance level. In a one-tailed hypothesis, we predict that the effect exists in a specific direction (hence, they are directional hypotheses). For example, we might predict that Group A is bigger than Group B. Or, we might predict that as X increases, Y increases too. The two curves below show the significance levels for one-tailed hypotheses. If we predicted that Group A was smaller than Group B, we would only reject the null hypothesis if Group A really was smaller than Group B, and this difference was large enough to be statistically significant (i.e. within one of the green tails). "],["p-values.html", "4.2 p-values", " 4.2 p-values At one point, the video on the previous page talks about ‘levels of significance’ - what does this actually mean? Here, we’ll talk about the p-value - one of the most commonly used, and possibly abused, concepts in research and statistics. We’ll talk about what the p-value actually means, and tie it to two related concepts later in this module: error and statistical power. 4.2.1 The definition of the p-value The definition of the p-value is the following (APA Dictionary, 2018): “in statistical significance testing, the likelihood that the observed result would have been obtained if the null hypothesis of no real effect were true.” What does this actually mean? We actually already touched on this in the previous page, but let’s dig a bit deeper. 4.2.2 A brief probability primer, and how this relates to p-values To start with, a p-value is a probability. A probability, of course, ranges from 0 to 1; 0 (0%) representing an impossible result, and 1 (100%) representing a certain result. Probabilities can be conditional, meaning that they are dependent on a certain condition being true. A p-value is a conditional probability. Specifically, a p-value refers to the probability of getting a particular result, assuming the null hypothesis. To illustrate what we mean, consider the two graphs below, showing some example data as points. On the right is an example of what one possible alternative hypothesis might look like - that this data is meaningfully represented by two underlying distributions or groups, and that there is a difference between these two groups (shown as the blue and red curves). Contrast that with the left graph, where we hypothesise that there is no difference - in other words, that the data can be captured by the one distribution. This graph on the left is a representation of our null hypothesis, and as per the definition of the p-value, this is the distribution we focus on. The area shaded blue represents the probability of getting that specific result. In the left example, the area shaded represents the probability of getting a value lower than x = 1. You can see that the blue area is quite big, so the probability of getting a value lower than 1 is quite high. On the right hand side is another example, which represents the probability of getting a value of x higher than 2. Here, the shaded area is small, so the associated probability is low. Now, recall the following figure from last week: We established last week that on a normal distribution, 95% of the data lies within a certain range of values. Obtaining values beyond these thresholds (1.96 standard deviations either side of the mean) account for a relatively small percentage of possible values. In other words, the probability of getting a value beyond these bounds (in the figure above, where the dark green areas are) is low. This is basically the thinking we use when we calculate a p-value. Let’s break down the steps: We first assume the null hypothesis is true, and so we use the distribution of the null hypothesis to calculate the probability of getting our result or greater. To figure out where our result(s) sits on this null distribution, we calculate a test statistic. This test statistic is essentially a value that represents where our data sits on the test (null) distribution. Each test statistic is calculated in a different way because every distribution looks different, so we’ll come back to this over the next couple of weeks. Afterwards, we calculate the probability of getting our test statistic (or greater) using a similar logic to the example above. This probability is our p-value. This marks the probability of observing this result or greater. Once we have a p-value we then compare this against alpha, which is our chosen significance level (usually p = .05). If the probability of getting our result is smaller than this (pre-defined) cutoff, it means that it is unlikely assuming the null hypothesis is true, and therefore our result is statistically significant and we reject the null hypothesis. So in essence, if a p-value is p = .05 we’re saying that assuming the null is true, there is a 5% chance of observing this result or greater. Likewise, an extremely small p-value (e.g. p = .0000000001) means that assuming the null is true, the probability of getting the data we have is extremely small. The logic, therefore, is that there must be an alternative explanation. To clarify, the example above is just on a normal distribution - each statistical test we perform has its own (null) distribution, which we will talk about more in future modules. However, the rationale across tests is essentially the same. 4.2.3 The debate around p-values Throughout history, p-values have been so misunderstood and misappropriated that some journals, such as Basic and Applied Social Psychology, actually either discourage or outright ban the reporting of p-values. This ties into a wider debate about the usefulness or meaningfulness of the hypothesis testing approach outlined on the previous page, with a number of academics and scientists arguing that it is time to do away with the system as a whole. The debate is something that goes beyond the scope of the subject, so we won’t be taking a strong stand on it either way. What we do think though is that it’s really important that you understand how hypothesis testing works and what p-values can or can’t tell you. "],["types-of-error.html", "4.3 Types of error", " 4.3 Types of error Nothing in life is perfect, and that applies to the inferential statistics that we do. Every sample will differ slightly from one to another due to inherent sampling error; in a similar way, whenever we do an inferential test about a population from a sample, we always run the risk of making an error in the decisions that we make. 4.3.1 Statistical error Imagine an old man is seeing his local GP complaining of a headache. Upon examination, the doctor concludes that the old man is pregnant. A pregnant woman in her last trimester then comes in to the same GP. Despite her numerous pregnancy-related complaints, the doctor concludes that she is not pregnant. Both of these scenarios (while hopefully very unlikely!) are obviously forms of errors on the doctor’s part. In the first scenario, the doctor has accepted a diagnosis that is very clearly wrong. In the second scenario, the doctor has rejected the correct diagnosis. The same kind of logic applies directly to quantitative research. We want to be sure that when we observe a result, that result is actually likely. We therefore want to minimise the possibility of errors like above. We can draw a table to illustrate the possible outcomes when we perform a given hypothesis test: Accept the null Reject the null The null is true Correctly accept the null Type I error The alternative is true Type II error Correctly reject the null 4.3.2 Type I error and alpha A moment ago we talked about alpha (\\(\\alpha\\)), or the significance level, from the previous sections about hypothesis testing and the p-value. Alpha is the same here as it is there - it is the probability of making a Type I error - that we incorrectly reject the null hypothesis when the null hypothesis is actually true. Essentially, alpha is the rate of Type I error we’re willing to accept whenever we do a hypothesis test. We generally set alpha as p &lt; .05 out of convention - i.e. most of the time, we’re willing to accept a 5% chance of a Type I error rate. However, we can set alpha to anything we want. Sometimes, we may set it lower (more on this in a few weeks’ time). "],["statistical-power.html", "4.4 Statistical power", " 4.4 Statistical power Another related but crucial consideration for inferential statistics is the concept of statistical power. Without spoiling too much about what it means here, below is an overview of what this concept is and why it is important. 4.4.1 Statistical power Power in a statistical context essentially describes how likely we are to actually detect an effect given our sample size. Mathematically, power is defined as \\(1 - \\beta\\), which in English terms means that it is the probability of not making a Type II error. Power is expressed as a percentage. For example, if your study has 50% power, it means it has an 50% chance of actually detecting an effect. The most common guideline is to aim for a study with 80% power. 4.4.2 Factors that affect power The primary factor (within your control) that affects how much statistical power you have to detect an effect is sample size. Think back to the formula for standard error, as a proxy explanation as to why this is the case. Larger samples tighten the sampling distribution of the mean, and so two distributions will overlap less and less the greater the sample sizes are. Therefore, if there is less overlap there is greater space to detect an effect. Some other factors that can affect power are: The effect size - how large is the difference between your groups, etc? If you’re trying to detect very small effects, you need much more power to detect it compared to larger ones. Performing a one-tailed test - because effects are only being tested in one direction, this alters the p-value (it actually halves it; a two-tailed p = .10 is a one-tailed p = .05). Don’t do this though, because there are very few instances in which you can justify using a one-tailed test without reviewers and other clued-in readers suspecting that you’re intentionally fudging your power. Increase alpha - for a semi-detailed explanation of why, see here. In addition, there is a nifty tool here that lets you see what happens to error rates when you change specific parameters: Link to the tool The consequence of being underpowered means that you can miss effects that exist. A good proportion of studies in psychology are underpowered, meaning that effects are being missed where they exist. Power is therefore an integral consideration of good study design, particularly for experimental contexts. 4.4.3 Power analyses Identifying an appropriate level of statistical power is an important part of planning quantitative research. Before conducting a study, it is wise to run a power analysis. Doing a power analysis allows you to identify how many participants you may need in order to reliably detect an effect of a given size. Most modern statistics software will allow you to conduct power analyses: SPSS (from version 27) Jamovi (with the jpower module) R (with the pwr package, among many others) We won’t get too into the maths here of how this is done (it heavily depends on your research design, e.g. how many groups you have, what test you plan on doing…). These programs will let you select the appropriate design you want to test, and choose the size of the effect you want and your alpha level. The power analysis will then give you a minimum sample size per group for you to achieve that level of statistical power. 4.4.4 Post-hoc power analyses You might see authors in some papers where you present a power analysis after collecting your sample and doing your analyses. Supposedly, this is to show that your sample had enough power to detect an effect. However, this is conceptually flawed. The primary flaw is that post-hocs are essentially just restatements of your p-values and so do little to show the true power of a design/test. "],["confidence-intervals.html", "4.5 Confidence intervals", " 4.5 Confidence intervals As we saw on the page about p-values, some people are vocal about their distaste for relying solely on p-values for decision making. One way of augmenting our estimates is to calculate a confidence interval for each estimate we make. Confidence intervals provide an estimate of the precision of our estimate, and so are a crucial concept to know about. 4.5.1 A reminder from the previous module It’s time to force you to remember what the graph below means once again (I promise this is the last time you will see this figure! I think). By now you should be very familiar with what this graph shows; namely, that 95% of the data in a normal distribution lies within 1.96 standard deviations either side of the mean, yada yada. This sounds all well and good, but this provides us with some useful information. If all that business about values within 1.96 standard deviations is still fresh in your mind, this next statement should be a no-brainer: if 95% of our data on a normal distribution lies within 1.96 SD either side of the mean, that means that there is a certain range of values that 95% of our data falls in. For example, say we have a sample of scores on a test, with a mean of 70 and a standard deviation of 4. If we want to know where 95% of the scores lie in this sample, we would do the following calculation: \\[ 95\\% \\ range = M \\pm (1.96 * SD) \\] Using this formula, we can calculate the values where 95% of the data lie: \\[ 95\\% \\ range = M \\pm (1.96 * SD) \\] \\[ 70 \\pm 7.84 \\] \\[ = 62.16, 77.84 \\] In other words, 95% of our data in this sample lie between 62.16 and 77.84. The remaining 5% of the sample lie above or below these values. 4.5.2 Confidence intervals We can use the same principle to make inferences about the true population parameter. When we take a sample, each one will have its own standard error (remember this reflects an estimate of the distance between the sample mean and the true population mean). If we were to repeatedly take samples, in the long run we would expect the true population mean to fall within 95% of all sample means. And, just like the normal distribution, when we look at the sampling distribution of the means below, 95% of all sample means will fall within 1.96 standard errors (thanks to the Central Limit Theorem). With this important property in mind, we can calculate a 95% confidence interval. This is an estimate of the range of values for our estimate of the parameter. In other words, it is a measure of precision. The formula for a 95% confidence interval, as it turns out, is exactly the same as above with one change: \\[ 95\\% CI = M \\pm (1.96 \\times SE) \\] Therefore, if we have an estimate of a population parameter (e.g. what the population mean is), we can use a 95% CI around that estimate to quantify the precision/uncertainty of that estimate. If a confidence interval is narrow, it suggests our estimate is quite precise. This can be extremely informative: not only can we use CIs to infer whether an effect is significant or not, but now they quantify how precise our estimates are. For example, pretend that we have a null hypothesis that a parameter is equal to 0 (as is usually the case). If we calculate a confidence interval and that happens to include the value of 0 (e.g. 95% CI: [-0.5, 1.5]), we can immediately infer that 0 is a likely value for this parameter - and thus, the null hypothesis is plausible. On the other hand, if the CI did not include 0 then we could infer that there likely is a significant effect. Likewise, a CI of something like [0.5, 0.8] compared to a CI of [0.2, 20.5] tells us that the former is a much more precise of a parameter estimate than the latter. Not all confidence intervals though will contain the true parameter purely because of how samples work (i.e. the inherent error between a sample and a population). In addition, it does not mean that there is a 95% chance a single interval will contain the true parameter. So what does the 95% part refer to? 4.5.3 Confidence The confidence level is a long-run probability that a group of confidence intervals will contain the true population parameter. A 95% confidence level means that if we were to take samples repeatedly and calculate a CI for each one, 95% of those CIs will contain the true population parameter in the long-run. Or, say if you were to take 100 samples and calculate a CI for each one, 95 of them would include the true population parameter: The 95% long-run probability does not change with sample size. What does change, however, is the precision of the estimates. This makes sense if you remember the formula for standard error, which divides by the square root of n. A larger n will lead to lower SE, and thus narrower confidence intervals. Below is an example with a much larger sample size. Notice how the confidence intervals are now much smaller: The choice of 95% is conventional, like alpha (our significance criterion); we can (but often don’t) change our level of confidence. This changes the relevant formula for calculating the interval, as in the examples below: \\[ 90\\% CI = M \\pm (1.645 \\times SE) \\] \\[ 99\\% CI = M \\pm (2.576 \\times SE) \\] Notice that the value we multiply the SE by has changed. If you have been especially observant so far, you may have figured out what these values are: they’re z-scores! 4.5.4 Confidence intervals in literature On the page about p-values, I left a brief note around some of the current discourse around the usefulness (or uselessness) of p-values. Proponents of getting rid of p-values/moving away from them advocate strongly for two alternatives: a) effect sizes (self-explanatory; we will come to this) and b) confidence intervals, to show the range of long-run plausible values for the estimate. Again, we won’t be taking an especially strong stance either way. That being said, it is now fairly common practice to report 95% confidence intervals alongside the results of significance tests for transparency. Programs like Jamovi will usually calculate these intervals automatically for you. "],["chi-squares.html", "Chapter 5 Chi-squares", " Chapter 5 Chi-squares .math { font-size: x-large; } While many of the things we are interested in when it comes to psychological research are continuously distributed (height, weight, reaction time, personality), there are many instances where we will need to work with data that is categorical. This can involve categorical independent variables (e.g. assigning participants to one or two groups) or categorical outcomes (e.g. responding yes or no). We’ll start with looking at relationships between these categorical variables, and testing for significant associations. This module will see you diving deep into Jamovi again - so be prepared to get hands on with a bunch of data! Between the seminar, the worked examples and exercises there are at least 6 different datasets to play around with for this week! By the end of this module you should be able to: Describe how a chi-square statistic is calculated Conduct two forms of chi-square tests: goodness-of-fit and tests of independence Calculate and interpret an appropriate effect size for tests of independence Figure 5.1: xkcd: Question "],["calculating-a-chi-square.html", "5.1 Calculating a chi-square", " 5.1 Calculating a chi-square We’ll start this module off by briefly going through the basics of what research designs suit chi-square tests, as well as the basic maths underlying the first part of the statistical test. 5.1.1 Categorical data As mentioned at the start of this module, chi-square tests are used when we work with categorical data - i.e., when we are dealing with counts of items or people, rather than continuous variables. Research questions focused on relationships or associations among categorical variables are suited to chi-square tests. Every categorical variable will have levels (categories) within them. These are the different values that categorical variable can be. For example, biological sex is often coded with two levels: male and female. Or perhaps you might categorise socioeconomic status into three levels: low, medium and high bands. This is something that can form a core part of your research design. The most basic example is asking participants what their biological sex is - participants will respond with one of the two categories. Alternatively, you can create categories from existing data. For example, many scales designed to assess psychological disorders such as depression and anxiety often have ‘cutoff’ points, where a certain score on the scale is indicative of a possible disorder. If you have everyone’s raw scores, you can convert these scores into categories depending on whether they are above or below these cutoff points (though this needs to be strongly justified). All of this kind of data are amenable to chi-square tests, if you are interested in relationships between categorical variables. The family of chi-square tests basically work by comparing the observed values to the expected values. As the names imply, observed value simply means the number of observations we have in each category or level (i.e. what our data actually is). Expected values, on the other hand, are the number of things we would expect to see under the null hypothesis. We can visualise categorical variables in two basic ways: a) a contingency table or b) a bar graph of counts. Below is the same set of data, shown in both forms: 5.1.2 The chi-square formula To test whether a result is significant, remember that we need to calculate a test statistic, and see where that fits on its underlying distribution. Here, our test statistic is handily named the chi-square statistic. To calculate the chi-squared test statistic we use the following formula: \\[ \\chi^2 = \\Sigma \\frac{(O-E)^2}{E} \\] Where: O = observed value E = expected value The English translation of that above formula can be described in four steps: Calculate observed count - expected count Square that difference Divide it by the expected count Do this for each cell, and add them all up We’ll look at this in more detail when we look at the actual tests. For the time being, here’s the key takeaway: have a look at the two graphs below, representing the observed and expected values of two different datasets. Have a think about the following: What is each graph telling you? The left graph appears to show noticeable differences between the observed and expected values. Based on the mathematical formula above, what will happen to the chi-square value? Each graph represents one set of data. Based on your answers to a) and b) above, which one of the two would you expect to demonstrate a significant effect? "],["the-chi-square-distribution.html", "5.2 The chi-square distribution", " 5.2 The chi-square distribution On the previous page we introduced the formula for a chi-square test statistic. But what do we do with it? We’ll go through this below in detail, given that this is the first time we’re coming across an actual test. While a computer will do all of this stuff automatically, it’s useful to know the actual mechanisms underlying the test. 5.2.1 The chi-square distribution Recall from Week 6 about how hypothesis tests work - we calculate a test statistic that conforms to a particular distribution, and we assess how likely our observed test statistic is (or greater) on this distribution, assuming the null hypothesis is true. This gives us the p-value for that test. With that in mind, the chi-square distribution that underlies the chi-square test looks something like this: The shape of the chi-square distribution is only dependent on the degrees of freedom (df). We’ll look at how to calculate degrees of freedom for various tests, including the family of chi-square tests, as we move along the subject. Here is the same set of lines above, but shown in their own plot this time: As you can hopefully see, the chi-square distribution is very heavily skewed at lower degrees of freedom (and therefore in smaller sample sizes). As degrees of freedom increase though, it approximates a normal distribution. For instance, here’s what the chi-square distribution looks like when df = 100: 5.2.2 Hypothesis testing in chi-squares At this point, also recall that p-values are the probability that we would get our observed result (or greater), assuming the null hypothesis is true. Here is our first application of this concept. When we use a chi-square test, we are performing the following basic steps: Establish null and alternative hypotheses Determine alpha level (in this case, \\(\\alpha\\) = .05 as always) Calculate our test statistic - here, this is the chi-square statistic Compare our chi-square test statistic against the chi-square distribution Calculate how likely we would have seen our chi-square value or greater on this distribution a. Or, alternatively, establish a critical chi-square value - the value that must be crossed for a result to be significant We’ll expand on this more in the next couple of pages. 5.2.3 Calculating significance Let’s say that we have a df of 5. How do we know where the critical chi-square value is? For that, we consult a chi-square table. This table gives the critical chi-square value at a set degrees of freedom and alpha level. These are freely available online, but here’s a short excerpt. To read this table: The far-left column lists different degrees of freedom. We need to find the row that corresponds to df = 5. Each column provides critical chi-squares at different alpha levels. We want to find the column that says .05. The number at both of these points tells us the critical chi-square value. So, for a df = 5 and an \\(\\alpha\\) = .05, the critical chi-square value is 11.07. If our own chi-square value is greater than this, the probability of that value or greater occurring (assuming the null) will be less than 5%; i.e. the boundary for statistical significance. In picture form: Hopefully that makes sense - this kind of logic is pretty much identical to the other tests that we will cover in the coming weeks! "],["goodness-of-fit.html", "5.3 Goodness of fit", " 5.3 Goodness of fit Now that we’ve covered the conceptual groundwork for chi-square tests in general, we can now start looking at actual tests that can help us answer research questions. The most basic chi-square test is the goodness of fit test. 5.3.1 Goodness of fit tests Goodness of fit tests are used when we want to compare a set of categorical data against a hypothetical distribution. Goodness of fit tests require one categorical variable - as the name implies, a goodness of fit test looks at whether the proportions of categories/levels in this variable fits an expected distribution. In other words, do our counts for each category match what we would expect under the null? “Distribution” in this context means probability distributions, and can apply to a wide range of scenarios. For the purposes of what we’re learning here, we’ll stick to a question along the lines of: “do the categories of variable X align with their expected probabilities”? 5.3.2 Example An example question that we look at in the seminar is Do Skittle bags have an even number of each colour? In the seminar, we go through whether or not a random bag has an even split of colours. Here on Canvas, we’ll now tackle their equally delicious rivals, M&amp;Ms. The data and analysis come courtesy of Rick Wicklin, a data analyst at SAS (who also make statistics software). You can read his full blog here: The distribution of colors for plain M&amp;M candies. We’ll be recreating Rick’s first analysis here. We’re sticking with the candy theme because a) they’re delicious and b) the M&amp;Ms are a great way to introduce what to do when expected proportions are not equal. M&amp;Ms come in six colours: red, blue, green, brown, orange and yellow. Unlike Skittles, these colours are not distributed equally within each bag of M&amp;Ms. In 2008, Mars (the parent company) published the following percentage breakdown of colours: 13% red 20% orange 14% yellow 16% green 24% blue 13% brown Rick collected his data in 2017, and so was interested in seeing if the proportions observed in his 2017 sample of M&amp;Ms aligned with the distribution of colours listed in 2008. A goodness of fit is the perfect test for this scenario because: We are making a claim about a distribution Our variable (colour) is categorical A chi-square goodness of fit will allow us to test whether the distribution of colours in a sample of M&amp;Ms aligns with the published proportions. Here’s our dataset: mnm_data &lt;- read_csv(here(&quot;data&quot;, &quot;week_7&quot;, &quot;W7_M&amp;M.csv&quot;)) head(mnm_data) 5.3.3 Calculating \\(\\\\\\chi^2\\) In goodness of fit tests, we first calculated the expected frequencies. In this example though, the expected proportions aren’t equal - and so we have to be mindful of this when calculating expected values. See the expected count cell for Red M&amp;Ms for how expected value is worked out in this instance. We can draw this up in table form alongside our own data: Colour Observed Expected proportion Expected count Blue 133 0.24 170.88 Brown 96 0.13 92.56 Green 139 0.16 113.92 Orange 133 0.20 142.4 Red 108 0.13 \\(712 \\times 0.13 = 92.56\\) Yellow 103 0.14 99.68 Then we would use the formula we saw on the previous page to calculate a chi-square statistic. However, since we’re doing this in R we’ll skip the manual maths. \\[ \\chi^2 = \\Sigma \\frac{(O-E)^2}{E} \\] If we have a look at the observed vs expected values, we might have a good idea of what’s going on already: 5.3.4 Using R The relevant function for doing a chi-square test in R is the chisq.test() function.2 By default, the chisq.test() function in R will assume that your categories have an equal chance of happening. However, in this instance we know that the colours are not evenly distributed. To ensure the proper probabilities are set beforehand, this needs to be specified by giving the p argument within chisq.test(). Note that the order of the expected probabilities needs to match the order they appear in the dataset (R will generally order these alphabetically unless told otherwise):3 # This pulls the relevant variable directly w7_mnm_table &lt;- table(mnm_data$colour) w7_mnm_chisq &lt;- chisq.test(w7_mnm_table, p = c(0.24, 0.13, 0.16, 0.20, 0.13, 0.14)) 5.3.5 Output Here’s what our output looks like. First is our table of proportions. This can be really useful in laying out the data and seeing where the differences between observed and expected proportions might lie. w7_mnm_chisq$observed ## ## Blue Brown Green Orange Red Yellow ## 133 96 139 133 108 103 w7_mnm_chisq$expected ## Blue Brown Green Orange Red Yellow ## 170.88 92.56 113.92 142.40 92.56 99.68 Next, here is our test output. The result is significant (p = .004), suggesting that the 2017 bag of M&amp;Ms does not follow the same distribution of colours as the 2008 values. (If you read the rest of Rick’s blog post, it turns out that somewhere between 2008 and 2017 they changed where M&amp;Ms are made, and actually split production across two factories that produce different distributions of colours. Rick eventually found out that his bag most likely came from one of the plants, although Mars has not made these proportions public like they used to.) w7_mnm_chisq ## ## Chi-squared test for given probabilities ## ## data: w7_mnm_table ## X-squared = 17.353, df = 5, p-value = 0.003877 Here is an example write-up of these results: A chi-square test of goodness of fit was conducted to see whether the number of M&amp;Ms aligned with the expected proportions published in 2008. There was a significant difference between the observed and expected proportions (\\(chi^2\\)(5, N = 712) = 17.35, p = .004). There were more green M&amp;Ms and less blue M&amp;Ms than expected. There is an analogous function called chisq_test() in the rstatix package, but the base R chisq.test() is easy enough to learn.↩︎ If you need to order your variable a certain way, you should use the factor() function. You will need to specify a levels argument, which describes the ordering of the categories/groups, and optionally a labels argument which gives each one a label. Both arguments take vectors, i.e. labels = c(\"1\", \"2\", \"3\").↩︎ "],["tests-of-independence.html", "5.4 Tests of independence", " 5.4 Tests of independence The next chi-square test we will cover is probably the most common - the chi-square test of independence. Here, we move from one categorical variable to two. Chi square tests of independence are used when we want to test whether two categorical variables are associated with each other (i.e. show a relationship). Some examples of this question might take on the following: Is smoking history (yes/no) associated with lung cancer diagnosis? (yes/no) Is there an association between gender and employment status? 5.4.1 Example scenario We’ll start off with a very basic example. In the below dataset, children from several schools were surveyed regarding what instrument they played. This dataset focuses on two instruments that have historically been seen as gendered (e.g. see Abeles 2009) - clarinet and drums. The sex of the child playing the instrument was also recorded. Our research question is: is there an association between sex and instrument choice? Dataset: instrument_data &lt;- read_csv(here(&quot;data&quot;, &quot;week_7&quot;, &quot;W7_instruments.csv&quot;)) ## Rows: 122 Columns: 3 ## ── Column specification ──────────────────── ## Delimiter: &quot;,&quot; ## chr (2): instrument, sex ## dbl (1): id ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(instrument_data) 5.4.2 Contingency tables The primary way of ‘drawing up’ categorical data, particularly when two variables are involved, is to draw a contingency table. A contingency table is a two-way table that shows how many participants/items/objects fall under each combination of our two variables. Here is a contingency table of our data below: w7_instrument_table &lt;- table(instrument_data$instrument, instrument_data$sex) w7_instrument_table %&gt;% addmargins() ## ## F M Sum ## clarinet 51 48 99 ## drums 6 17 23 ## Sum 57 65 122 5.4.3 Expected frequencies To calculate expected frequencies in a two-way contingency table (i.e. a test of independence), we use the following formula: \\[ E = \\frac{R \\times C}{N} \\] Where R = row total and column = column total. Let’s put this into practice with girls who play the clarinet (highlighted above). The row total for this cell is 99 (i.e. total number of clarinet players). The column total is 57 (total number of girls). To calculate an expected value for this cell, we would therefore calculate the following: \\(E = \\frac{99 \\times 57}{122}\\) This works out to be roughly 46.25 - which means that we would expect roughly 46 female clarinet players. We then go through and calculate this for each cell, so that we have all of our expected values. Once we’ve done that, we can then calculate our chi-square test statistic using the same formula as always: \\[ \\chi^2 = \\Sigma \\frac{(O-E)^2}{E} \\] 5.4.4 Output Here’s our output! Firstly, our contingency table: w7_inst_chisq &lt;- chisq.test(w7_instrument_table, correct = FALSE) w7_inst_chisq$observed ## ## F M ## clarinet 51 48 ## drums 6 17 w7_inst_chisq$expected ## ## F M ## clarinet 46.2541 52.7459 ## drums 10.7459 12.2541 Next is our chi-square test output. As you can see, our test of independence suggests a significant result (p = .03). In other words, we reject the null hypothesis that there is no association between instrument and sex. w7_inst_chisq ## ## Pearson&#39;s Chi-squared test ## ## data: w7_instrument_table ## X-squared = 4.848, df = 1, p-value = 0.02768 Like the previous page, here’s an example write-up of our results: A chi-square test of association was conducted to examine whether there was an association between sex and instrument choice. A significant association was observed (\\(\\chi^2\\)(1, N = 122) = 4.85, p = .028; Cramer’s V = .199). There were more male drummers and less female drummers than expected. Note that as part of the write-up above, you would also include a brief interpretation of the effect size - but we will discuss this on the next page. "],["effect-sizes-for-chi-squares.html", "5.5 Effect sizes for chi-squares", " 5.5 Effect sizes for chi-squares This is the first time we’re coming across effect sizes for any test - thankfully, we start with relatively easy ones to wrap your head around. We will cover two effect sizes: phi (\\(\\phi\\)) and Cramer’s V, both of which apply when conducting a test of independence. 5.5.1 Phi Phi (\\(\\phi\\)) is an effect size for chi-squares that applies only to 2x2 designs. The formula for phi is: \\[ \\phi = \\sqrt{\\frac{\\chi^2}{n}} \\] Essentially, it is the chi-square test statistic divided by the sample size, which is then square rooted. Again, it only works for 2x2 designs - i.e. each categorical variable can only have two categories within it. 5.5.2 Cramer’s V Cramer’s V is another effect size for chi-squares, but one that can be used for anything beyond a 2x2 design as well. The formula for Cramer’s V is similar: \\[ V = \\sqrt{\\frac{\\chi^2}{n(k-1)}} \\] Here, k refers to the number of groups in the variable with the lowest number of groups. So for example, in a 2x3 design, one variable has 2 levels and the other has 3; k = 2 in this instance. Phi and Cramer’s V can both be calculated in R with the following functions from the effectsize package, a handy package that will calculate many common effect size measures. Like chisq.test(), both functions will work if you give them a contingency table. Helpfully, both functions calculate 95% CIs. effectsize::phi(w7_instrument_table, adjust = FALSE, alternative = &quot;two.sided&quot;) effectsize::cramers_v(w7_instrument_table, adjust = FALSE, alternative = &quot;two.sided&quot;) 5.5.3 Interpretation Phi and Cramer’s V are essentially both correlation coefficients (more on this in Week 10). Both phi and Cramer’s V can only be between 0 and 1. For this subject, the size of Cramer’s V and phi can be interpreted as follows: If the effect size = .10, the effect is small If effect size = .30, the effect is medium If effect size = .50 or above, the effect is large 5.5.4 Practice Given the following results from a 2x2 chi-square test of independence: \\(\\chi^2\\) = 5.45 N = 46 Calculate both phi and Cramer’s V. (You should get the same answer, but have a go at trying it both ways!) "],["bonus-mcnemars-test.html", "5.6 Bonus: McNemar’s Test", " 5.6 Bonus: McNemar’s Test The chi-square test of independence, as the name implies, relies on the assumption of independence - that variable A is statistically independent of B. But what happens if we have a relationship that doesn’t meet this assumption? The most common kind is when data is paired or repeated, i.e. participants are measured on the same variable twice. McNemar’s test allows us to apply chi-square techniques to this kind of data. 5.6.1 McNemar’s test McNemar’s test is used for when you have repeated-measures categorical data. Specifically, it applies to a 2x2 repeated measures contingency table. This will apply when you have one sample tested twice on a binary outcome. The most common example of this is a yes/no outcome, before and after something (e.g. an intervention. McNemar’s test will apply to situations where you have one sample tested twice like this: Timepoint 2 - A Timepoint 2 - B Total Timepoint 1 - A Timepoint 1 - B Total 5.6.2 Mathematical basis Consider the table above. We can formulate the cells between each combination of predictors, and their totals, as follows: Timepoint 2 - A Timepoint 2 - B Total Timepoint 1 - A a b a + b Timepoint 1 - B c d c + d Total a + c b + d N Essentially, in this instance cell a represents the number of cases/participants in T1-A and T2-A, b is T1-A and T2-B etc etc. Each cell has two marginal probabilities, which are the row and column totals corresponding to that cell. Cell a, for example, has marginal probabilities of a + b (the row total) and a + c (the column total). Cell d has marginal probabilities of c + d and b + d. The null hypothesis in this scenario is that the two marginal properties for each outcome are the same. This is a principle known as marginal homogeneity. In essence, the McNemar test tests the hypothesis that the proportion of participants responding A beforehand is the same at that responding A afterwards. The marginal probability in this instance is a + b (proportion of response A at timepoint 1) and a + c (proportion of response A at timepoint 2). The same hypothesis applies to the proportion of participant saying B before and after, corresponding to cell d. In this case, the marginal probabilities are c + d (time 1) and b + d (time 2). We can express this null hypothesis like this: \\[ p_a + p_b = p_a + p_c \\] \\[ p_b + p_d = p_c + p_d \\] We can simplify both equations by removing identical terms from both sides of the equation. You might see then that both equations cancel out to simply be: \\[ p_b = p_c \\] That, in effect, is our null hypothesis - that the probability of cell b is identical to cell c! So now we can express our null and alternative hypotheses: \\(H_0: p_b = p_c\\) \\(H_1: p_b \\neq p_c\\) Our chi-square test statistic is calculated as follows: \\[ \\chi^2 = \\frac{(b-c)^2}{b+c} \\] And then from here on, the process of deriving a p-value is identical to a regular chi-square test, with the exception that the df is always df = 1 (remember we have a 2 x 2 table, and the formula for a df in a two-way chi-square test is to subtract 1 from each and multiply them together). 5.6.3 Example Below is a fictional dataset from 70 registered voters in a fictional country. The 70 voters were asked whether they intended to vote for the current government twice: before event X happened in the government and after event X. Their responses were recorded as simple Yes-No answers. w7_voting &lt;- read_csv(here(&quot;data&quot;, &quot;week_7&quot;, &quot;W7_voting.csv&quot;)) ## Rows: 71 Columns: 3 ## ── Column specification ──────────────────── ## Delimiter: &quot;,&quot; ## chr (2): before, after ## dbl (1): id ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Here is a contingency table of our data: w7_voting_table &lt;- table(w7_voting$before, w7_voting$after) w7_voting_table %&gt;% addmargins() ## ## No Yes Sum ## No 18 28 46 ## Yes 11 13 24 ## Sum 29 41 70 We can identify our b = 28 and our c = 11. We can calculate a relevant chi-squared test statistic using the formula above: \\[ \\chi^2 = \\frac{(b-c)^2}{b+c} \\] \\[ \\chi^2 = \\frac{(28-11)^2}{28+11} \\] \\[ \\chi^2 = \\frac{(17)^2}{39} \\] \\[ \\chi^2 = 7.41 \\] If we were doing this fully by hand, we could consult a chi-squared table and see what the relevant p-value would be with this chi-squared value and a df = 1. However, we’ll skip straight to Jamovi. A McNemar test can be run in R using the mcnemar.test() function in base R: w7_voting_mcn &lt;- mcnemar.test(w7_voting_table, correct = FALSE) w7_voting_mcn ## ## McNemar&#39;s Chi-squared test ## ## data: w7_voting_table ## McNemar&#39;s chi-squared = 7.4103, df = 1, p-value = 0.006485 The output looks like this, and confirms that there is a significant change in proportions before and after event (\\(\\chi^2\\)(1, N = 70) = 7.41, p = .006). Based on the original values of b and c, we can infer that this might be because more people changed their vote from No -&gt; Yes than the other way round. "],["t-tests.html", "Chapter 6 t-tests", " Chapter 6 t-tests t-tests are usually one of the first families of statistical tests that students learn when they take a research methods subject. I (Dan) pretty much learnt only t-tests until my third year of my psychology major (I learnt chi-squares and other tests through taking separate statistics subjects through my uni’s Department of Statistics before I learnt them in psychology). It’s not hard to see why this is the case - t-tests are really intuitive and simple to conduct, and so are an accessible way into learning statistical tests (even though chi-squares are even easier). The family of t-tests come into play when we have one categorical IV with two levels, and one continuous DV. As you can imagine, there are many instances where this kind of design comes into play, and you will see as much in the datasets and examples this week. There are nine datasets for you to play around this week (3 for each kind of test) - so hopefully that will give you plenty of practice! By the end of this module you should be able to: Describe how a t-test works in principle Conduct three forms of chi-square tests: one-sample, independent-samples and paired-samples Calculate and interpret an appropriate effect size for the above tests Figure 6.1: xkcd: Control Group "],["t-tests-and-the-t-distribution.html", "6.1 t-tests and the t-distribution", " 6.1 t-tests and the t-distribution We begin this week’s module in much the same way we went through last week’s. We’ll look at the shape of the underlying distribution, and what determines the shape of that distribution. On this page we’ll also go through what the basic premise of the t-test is. 6.1.1 What is a t-test? The family of t-tests, broadly speaking, are used when we want to compare one mean against another. This can take on three major forms, which we will go into later in the module: Is this sample mean different from the population mean? Are these two group means different? Is the mean at point 1 different to the mean at point 2? All of these instances require a comparison between two means, which the t-tests will allow you to test for. So, in a general sense our hypotheses would be something like: \\(H_0\\): The means between the two groups are not significantly different. (i.e. \\(\\mu_1 = \\mu_2\\)) \\(H_1\\): The means between the two groups are significantly different. (i.e. \\(\\mu_1 \\neq \\mu_2\\)) The question of when t-tests should be used is hopefully somewhat obvious - in general, we use them we want to compare two means with each other. The important part is what kind of means they are: If we compare one sample mean to a hypothesised population mean, this is a one-sample t-test If we compare two group means, this is an independent samples t-test If we measure one group twice and compare the two means, this is a paired-samples t-test. In this module, we’ll go through all three (but will emphasise the latter two especially as they see the most use). 6.1.2 The t-statistic Last week we introduced the chi-square statistic, which is the value that we use when we want to assess whether a result is significant. When we want to assess whether a difference between two means is significant we calculate a different test statistic, which (as the name implies) is the t-statistic. While you won’t be required to calculate this by hand this week, it would be good to wrap your head around the below formula so you understand how it works in principle: \\[ t = \\frac{M_1 - M_2}{SE} \\] This is how t is calculated conceptually - it is the difference in the two means over the standard error of the mean. Note that the world conceptually is stressed here because the actual formula is slightly different for each t-test, but all work on this above principle. Again, we won’t go through the maths of this in detail - this is beyond the scope of the subject! 6.1.3 The t-distribution By now, hopefully you’re comfortable with the idea that we use our test statistic and find its position on its underlying probability distribution in order to calculate the p-value. The underlying distribution of this test is the t-distribution, which is depicted below. Note that like the chi-square distribution, degrees of freedom is the only parameter that determines its shape: ## Warning: Using `size` aesthetic for lines was ## deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 ## hours. ## Call `lifecycle::last_lifecycle_warnings()` ## to see where this warning was generated. The one key difference between the t-distribution and the chi-square distribution from a mathematical point of view is that the t-distribution is symmetrical, much like the normal distribution (although they are not the same). Therefore, it is possible to get a negative t-test statistic; however, this simply reflects the order in which the groups are being compared. E.g. Say that Group 1 - Group 2 gives a test statistic of t = 1.5. If you were to enter the groups as Group 2 - Group 1 instead, the t would be -1.5. This simply reflects the ordering of the groups. 6.1.4 The t-table Once again, like the chi-square we have a beautiful little table for calculating a critical t-value. We won’t go into too much depth over how this works because it works exactly like how it does for chi-squares - find the row corresponding to your degrees of freedom, then find the column corresponding to your alpha level. Yes, there are a lot of cross-references to what we covered last week with chi-squares - and that’s a good thing! The point here is that conceptually, the process of testing hypotheses using t-tests is exactly the same as what we did with chi-squares, but the specific design and maths are different. "],["cohens-d.html", "6.2 Cohen’s d", " 6.2 Cohen’s d This might be starting to sound a little familiar by now, but here are some effect sizes for t-tests. Note that they’re different to the Cramer’s V we saw in the chi-square test of independence last week - this is because it is a) conceptually different and b) interpreted differently too. 6.2.1 What is Cohen’s d? Cohen’s d is a measure of effect size that is used when comparing between two means (i.e. in a t-test). It essentially is a measure of the distance between the two means. See below for three pairs of means: If two groups aren’t all that different (e.g. panel A), then any effect of group will be small or negligible. If the two groups are further apart, however (like panel C), there is a more obvious effect of group - and so the size of the effect itself will be larger. Cohen’s d essentially is a measure of this ‘distance’. The basic formula for calculating Cohen’s d is: \\[ d = \\frac{M_1 - M_2}{\\sigma_{pooled}} \\] In other words, Cohen’s d is calculated by taking the difference between the two group means and dividing that by the pooled standard deviation across both groups. Pooled SD is essentially an aggregate SD across both groups in the sample, and not something we’ll concern ourselves with this week (because like the maths for the t-statistic, the calculation of the pooled SD depends on the test and is hard). 6.2.2 Interpreting Cohen’s d Cohen provided some now-famous guidelines for interpreting the size of Cohen’s d values: Effect size Interpretation d = .20 Small d = .50 Medium d = .80 Large "],["one-sample-t-test.html", "6.3 One-sample t-test", " 6.3 One-sample t-test The first test that we’ll look at is the one-sample t-test, which is the most simple of the three that we will look at this week. 6.3.1 One-sample t-test A one-sample t-test is used when we want to compare a sample against a hypothesised population value. It is useful when we already know the expected value of the parameter we’re interested in, such as a population mean or a target value. The basic hypotheses for a three-way interaction are: \\(H_0\\): The sample mean is not significantly different from the hypothesised mean. (i.e. \\(M = \\mu\\)) \\(H_1\\): The sample mean is significantly different from the hypothesised mean. (i.e. \\(M \\neq \\mu\\)) It’s worth noting that one-sample t-tests aren’t that commonly used because they require you to know the population value (or, if you hypothesise a value, you need to justify why). However, they’re included here because they’re still a part of the t-test family, and they serve as a nice introduction to how t-tests work. 6.3.2 Example data Historically, scores in a fictional research methods class average at 72. This year, you are the subject coordinator for the first time, and you notice that last year’s cohort appear to have really struggled. You want to see if there is a meaningful difference between the cohort’s average grade and what the target grade should be. Here’s the dataset below: w8_grades &lt;- read_csv(here(&quot;data&quot;, &quot;week_8&quot;, &quot;W8_grades.csv&quot;)) ## Rows: 128 Columns: 2 ## ── Column specification ──────────────────── ## Delimiter: &quot;,&quot; ## dbl (2): id, grade ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(w8_grades) 6.3.3 Assumption checks There is only one relevant assumption that we need to check for the one-sample t-test: whether our data is distributed normally or not. We can do this in two ways. The first and quickest is through a test called the Shapiro-Wilks test (often abbreviated as the SW test). The SW test is a significant test of departures from normality. The statistic in question, W, is an index of normality. If W is close to 1, then data is normally distributed; the smaller W becomes, the more non-normal the data is. A significant p-value on the SW test suggests that the data is non-normal. Thankfully, that isn’t an issue here. shapiro.test(w8_grades$grade) ## ## Shapiro-Wilk normality test ## ## data: w8_grades$grade ## W = 0.98652, p-value = 0.2395 The second way is through a QQ plot (Quantile-Quantile) plot. Essentially, these plot where data should be (if the data are normally distributed) against where the data actually is. If the normality assumption is intact, most of the data should lie on or close to the straight line, like the left plot. Data that looks like the right, where the data curves away from the central line, is more likely to be non-normally distributed. 6.3.4 Output Here are our descriptive statistics. They alone might already tell us something is going on: w8_grades %&gt;% summarise( mean = mean(grade, na.rm = TRUE), sd = sd(grade, na.rm = TRUE), median = median(grade, na.rm = TRUE) ) %&gt;% knitr::kable(digits = 2) mean sd median 67.09 5.38 67.5 To run a one-sample t-test, the basic function is the t.test() function. For a one-sample t-test, you must provide the argument x (the data) and mu, which is the hypothesised population mean. Below is our output from the one-sample t-test. Our result tells us that there is a significant difference between the mean of the sample (M = 67.09) and the hypothesised mean of 72 (p &lt; .001). The mean difference here is calculated as Sample - Hypothesis; therefore, a difference of -4.91 means that the sample mean is 4.91 units lower than the population mean (which hopefully would have been evident from the descriptives anyway). This means that for some reason, last year’s cohort are performing worse than the expected average. R Note: Jamovi will give you a 95% confidence interval around the mean difference as 95% CI = [-5.85, -3.97]. R however will give you a CI around the actual mean. In this case, the 95% confidence interval for the group mean is [66.15, 68.03]. Really though, this is giving us the same information; 72 - 5.85 = 66.15, and 72 - 3.97 = 68.03. So the only difference is in what value is presented for the 95% CI, but the actual inference itself doesn’t change. t.test(w8_grades$grade, mu = 72) ## ## One Sample t-test ## ## data: w8_grades$grade ## t = -10.342, df = 127, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 72 ## 95 percent confidence interval: ## 66.14570 68.02617 ## sample estimates: ## mean of x ## 67.08594 Anoter way of writing a one-sample t-test is to use variable ~ 1 formula notation. The ~ 1 is used to indicate that we are running a one-sample t-test. We still need to define mu = 72 to set our hypothesised population mean. t.test(grade ~ 1, data = w8_grades, mu = 72) ## ## One Sample t-test ## ## data: grade ## t = -10.342, df = 127, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 72 ## 95 percent confidence interval: ## 66.14570 68.02617 ## sample estimates: ## mean of x ## 67.08594 Alternatively, you can use the t_test() function in rstatix. This function also requires formula notation. detailed = TRUE will print a detailed output that will give the 95% CI for the mean as well: w8_grades %&gt;% t_test(grade ~ 1, mu = 72, detailed = TRUE) As we have previously done for chi-square tests, we will want to calculate an effect size for our t-test. The cohens_d() function from effectsize can handle the calculation of effect sizes for all three variants of t-tests. One thing that’s quite nice about this function is that the required syntax for cohens_d() is nearly identical to that for t.test() - meaning that the correct type of Cohen’s d (one-sample, independent samples, paired) will be calculated based on what you put in. For a one-sample t-test, for instance, you can write the syntax exactly as you would for t.test(): # One-sample # t.test(w8_grades$grade, mu = 72) effectsize::cohens_d(w8_grades$grade, mu = 72) The alternate way of specifying a one-sample t-test, the var ~ 1 format, also works. cohens_d() will recognise both. # One-sample - alternate # t.test(grade ~ 1, data = w8_grades, mu = 72) effectsize::cohens_d(grade ~ 1, data = w8_grades, mu = 72) Alternatively, you can use the rstatix package. This will give the same values, but the confidence intervals generated by effectsize are traditional confidence intervals, whereas rstaix uses a different method (which we need not concern ourselves with). In any case, the actual width of the intervals should be similar. Just note that for paired samples Cohen’s d, long data is required (as is the case with t-tests in rstatix). For a one-sample t-test, the formula once again needs to be in var ~ 1 format and mu must be specified: w8_grades %&gt;% rstatix::cohens_d(grade ~ 1, mu = 72, ci = TRUE, ci.type = &quot;norm&quot;) Here is an example write-up of the above: A one-sample t-test was conducted to examine whether grades in the research methods class differed from the historical average of 72. The sample’s grades (M = 67.09, SD = 5.38) were significantly lower than the historical average (t(127) = -10.3, p &lt; .001), with a mean difference of 4.91 (95% CI = [-5.85, -3.97]). This effect was large in size (d = -0.91). "],["independent-samples-t-test.html", "6.4 Independent samples t-test", " 6.4 Independent samples t-test The independent-samples t-test is one of the most common tests that you will see in literature - it is one of the bread-and-butter tests of many music psychologists (for better or worse). 6.4.1 Independent samples Independent samples t-tests are used when we want to compare two separate groups on one continuous outcome. They’re therefore well-suited for data with one categorical IV with two levels, against one continuous outcome. 6.4.2 Example data For this example we’ll use a contrived but really simple example. A group of self-reported professional and amateur musicians were asked how many years of training they had on their primary instrument. w8_training &lt;- read_csv(here(&quot;data&quot;, &quot;week_8&quot;, &quot;W8_training.csv&quot;)) ## Rows: 60 Columns: 3 ## ── Column specification ──────────────────── ## Delimiter: &quot;,&quot; ## chr (1): Group ## dbl (2): Participant, Years_training ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(w8_training) Let us start with a nice plot: w8_training %&gt;% group_by(Group) %&gt;% summarise( mean = mean(Years_training, na.rm = TRUE), sd = sd(Years_training, na.rm = TRUE), se = sd/n() ) %&gt;% ggplot( aes(x = Group, y = mean) ) + geom_point(size = 3) + geom_errorbar(aes( ymin = mean - 1.96*se, ymax = mean + 1.96*se ), width = 0.2) + theme_pubr() 6.4.3 Assumption checks There are three main assumptions for a basic independent samples t-test: Data must be independent of each other - in other words, one person’s response should not be influenced by another. This should come as a feature of good experimental design. The equality of variance (homoscedasticity) assumption. The classical t-test assumes that each group has the same variance (homoscedasticity). We can test this using a significant test called Levene’s test. If the test is significant (p &lt; .05), the assumption is violated. In our data, this assumption seems to be intact (F(1, 58) = .114, p = .737). w8_training %&gt;% levene_test(Years_training ~ Group, center = &quot;mean&quot;) ## Warning in leveneTest.default(y = y, group = group, ...): group coerced to ## factor. The residuals should be normally distributed. This essentially has implications for how well the data behaves. We can test this in two ways. The first is using a normality test, like the Shapiro-Wilks (SW) test, which is usually done using shapiro.test(). Like Levene’s test, if the result of this test is significant it suggests that the normality assumption is violated. The result that Jamovi gives is not super clear in terms of what values exactly have been used to run the SW-test, so we will turn to another method. For an independent t-test specifically, another way of testing this assumption is to simply see whether the dependent variable is normally distributed in both groups separately. In other words, we perform a Shapiro-Wilks test on years of training in both amateurs and professionals. rstatix provides its own version of the SW-test (shapiro_test) that is compatible with group_by() and general tidyverse notation. Thus, we can first group using group_by(), and then run a SW-test on each group: w8_training %&gt;% group_by(Group) %&gt;% shapiro_test(Years_training) We can therefore see that for professionals our data is normally distribution (W = .94, p = .090), but not for amateurs (W = .90, p = .008). 6.4.4 Output In reality, it’s rare that any of these assumptions are fully met even when tests say they are (the tests we just mentioned can be biased). This is especially true for classical t-tests, which are very sensitive to violations. A consistently better alternative is to use the Welch t-test, which assumes the equality of variance assumption is not met. Welch t-tests are also fairly robust against the normality assumption, and so are more flexible without sacrificing accuracy. Here’s our output from R. Note that this is from a Welch t-test - R will do this by default. t.test(Years_training ~ Group, data = w8_training) ## ## Welch Two Sample t-test ## ## data: Years_training by Group ## t = -5.8586, df = 57.989, p-value = 2.33e-07 ## alternative hypothesis: true difference in means between group Amateur and group Professional is not equal to 0 ## 95 percent confidence interval: ## -3.922048 -1.924448 ## sample estimates: ## mean in group Amateur mean in group Professional ## 4.387097 7.310345 w8_training %&gt;% t_test(Years_training ~ Group, detailed = TRUE) Now for the effect size. For an independent samples t-test, you can give it almost the same as the regular t.test() function. The only change is that by default, cohens_d() will calculate Cohen’s d assuming equal variance. In order for our value of d to match a Welch-samples t-test, we need to set pooled_sd = FALSE (which makes the syntax slightly different to t.test()). # Independent samples # t.test(Years_training ~ Group, data = w8_training) effectsize::cohens_d(Years_training ~ Group, data = w8_training, pooled_sd = FALSE) Or again, we can alternatively use the cohen_d() function from rstatix: w8_training %&gt;% rstatix::cohens_d(Years_training ~ Group, ci = TRUE, ci.type = &quot;norm&quot;) From this, we can see that the two groups do significantly differ on years of training (t(57.99) = 5.86, p &lt; .001). We can use the mean difference value to see, well… the difference in means between the two groups. In this case, professionals have 2.92 more years of training (on average; 95% CI = [1.92, 3.92]) compared to amateurs. So, we could write this up as something like: An independent samples t-test was conducted to examine whether professionals and amateurs differed on years of musical instrument training. Professionals had on average 2.92 years more training (95% CI [1.92, 3.92]) compared to amateurs (t(57.99) = 5.86, p &lt; .001), corresponding to a large significant effect (d = 1.51). (n.b. the signs for t and the mean difference don’t overly matter so long as they are interpreted in the right way. The output above calculates amateurs - professionals, which is why the values for both are negative; but if you were to force the test to run the other way round, the values would be the same with the signs flipped. Hence why descriptives and graphs are super important too! If for some reason you do want R to run a Student’s t-test, you need to specify var.equal = TRUE. This tells R that we assume equality of variances. t.test(Years_training ~ Group, data = w8_training, var.equal = TRUE) ## ## Two Sample t-test ## ## data: Years_training by Group ## t = -5.8424, df = 58, p-value = 2.476e-07 ## alternative hypothesis: true difference in means between group Amateur and group Professional is not equal to 0 ## 95 percent confidence interval: ## -3.924805 -1.921691 ## sample estimates: ## mean in group Amateur mean in group Professional ## 4.387097 7.310345 w8_training %&gt;% t_test(Years_training ~ Group, detailed = TRUE, var.equal = TRUE) "],["paired-samples-t-test.html", "6.5 Paired-samples t-test", " 6.5 Paired-samples t-test Here’s our last test for the module, and it is again another bread-and-butter statistical test in literature: the paired-samples t-test. 6.5.1 Paired-samples Paired samples t-tests, as the name sort of implies, are used when we have a sample and we take measurements twice. Often, paired-samples t-tests are interested in testing the effect of time on an outcome; for example, a before-after design lends itself quite nicely to paired-samples and other repeated-measures tests. The core hypotheses are very much the same here, aside from the caveat that the means are between conditions and not groups. Mathematically, the paired-samples t-test is actually just a variant of the one-sample t-test. If we did a one-sample t-test on the differences between the two timepoints/conditions, we would get the same results. You can see a demonstration of this in the dropdown at the end of this section. 6.5.2 Example data For this example, we’ll take a look at a simple interventions study. Participants were asked to answer a short list of questions relating to how they were feeling, once before an intervention and once afterwards. Higher scores represent better emotional states. The intervention was a series of self-regulation classes and exercises that the participants took twice a week. We’re interested in seeing whether the intervention was effective. w8_symptoms &lt;- read_csv(here(&quot;data&quot;, &quot;week_8&quot;, &quot;W8_symptoms.csv&quot;)) ## Rows: 32 Columns: 2 ## ── Column specification ──────────────────── ## Delimiter: &quot;,&quot; ## dbl (2): before, after ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(w8_symptoms) R-note: Data for paired-samples t-tests can either be in wide form or long form, and this depends on the specific function used. Our dataset is already in wide form as we have one column for before and one for after, so the following code will create a long-form version: # Pivot to long format w8_symptoms_long &lt;- w8_symptoms %&gt;% pivot_longer( cols = everything(), names_to = &quot;time&quot;, values_to = &quot;symptom_score&quot; ) # Display start of new data head(w8_symptoms_long) 6.5.3 Assumption checks Similar to other tests, we need to check normality. Here, the assumption is whether the differences between time 1 and 2 are normally distributed (not necessarily time 1 and 2 themselves). Hence, when we run a Shapiro-Wilks test we’re running this on the values we get from Time 1 - Time 2. In our data, this assumption seems to be intact (W .985, p = .918). shapiro.test(w8_symptoms$before - w8_symptoms$after) ## ## Shapiro-Wilk normality test ## ## data: w8_symptoms$before - w8_symptoms$after ## W = 0.98465, p-value = 0.9177 6.5.4 Output In R, you can do pairwise t-tests using the default t.test() (or the rstatix equivalent t_test()) function. In both methods, you must set paired = TRUE in order to run a paired t-test. However, there is one key difference: the base t.test() requires data in wide format, whereas rstatix::t_test() requires data in long format. Here are both methods. For the base t.test() function, your data needs to be in wide format. From there it is as simple as giving the two columns as the arguments to the function, and setting paired = TRUE: t.test(x = w8_symptoms$before, y = w8_symptoms$after, paired = TRUE) ## ## Paired t-test ## ## data: w8_symptoms$before and w8_symptoms$after ## t = -2.9501, df = 31, p-value = 0.005999 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## -4.651185 -0.848815 ## sample estimates: ## mean difference ## -2.75 Alternatively, you can use the below notation. Pair(before, after) indicates that we want R to treat the before and after variables as paired data. The ~ 1, as we saw before, indicates that this is a one-sample t-test. This is because a paired-samples t-test is functionally equivalent to a one-sample t-test on the differences between conditions (see the expandable dropdown below for more details). t.test(Pair(before, after) ~ 1, data = w8_symptoms) ## ## Paired t-test ## ## data: Pair(before, after) ## t = -2.9501, df = 31, p-value = 0.005999 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## -4.651185 -0.848815 ## sample estimates: ## mean difference ## -2.75 Note that before R 4.4.0 (2024), the notation for paired-samples t-tests were different. If you prefer rstatix, on the other hand, your data needs to be in long format. After that, you can pass the test as a formula much like how you would do so in the independent samples case: w8_symptoms_long %&gt;% t_test(symptom_score ~ time, paired = TRUE, detailed = TRUE) Once again, we’ll also want to calculate Cohen’s d for our paired-samples t-test. For a paired-samples t-test the developers of effectsize recommend using the rm_d() function, which stands for repeated-measures Cohen’s d. This function takes the same basic syntax as t.test() for paired-samples t-tests, including the use of wide data, but a couple of extra arguments are required. method = \"z\" defines how the pooled SD term is calculated. There are six possible options, but this option gives the standard calculation. adjust = FALSE: calculates Hedges’ G, which is an alternative effect size that corrects for small sample bias. We won’t worry about this here, so we will set this to FALSE. Note that like the other functions, you can either provide the arguments in Pairs() ~ 1 or by subsetting the columns (x = x$1, y = x$2). # Paired sample - works with wide data # t.test(Pair(before, after) ~ 1, data = w8_symptoms) effectsize::rm_d(Pair(before, after) ~ 1, data = w8_symptoms, method = &quot;z&quot;, adjust = FALSE) # t.test(x = w8_symptoms$before, y = w8_symptoms$after, paired = TRUE) effectsize::rm_d(x = w8_symptoms$before, y = w8_symptoms$after, method = &quot;z&quot;, adjust = FALSE) If using cohen_d() from rstatix, paired = TRUE must be selected: w8_symptoms_long %&gt;% rstatix::cohens_d(symptom_score ~ time, paired = TRUE, ci = TRUE, ci.type = &quot;norm&quot;) The Canvas version asks you to interpret each of these effect sizes, but… rstatix will automatically label this for you! The mean symptom scores of the two timepoints are significantly different (t(31) = 2.95, p = .006). Based on the means and mean difference (2.75; 95% CI = [0.85, 4.65]), participants reported having significantly better emotional states after the intervention compared to beforehand. We can write that up as below, and in this specific example we will condense the text down: A paired samples t-test found that symptoms significantly decreased after the intervention (t(31) = 2.95, p = .006), with an average decrease of 2.75 points (95% CI [0.85, 4.65]). This decrease was medium in size (d = 0.52). Paired samples using the one-sample t-test Mathematically, all a paired-samples t-test is doing is running a one-sample t-test on the differences between the two timepoints/groups, Below is a demonstration of how paired-samples tests can be run using the one-sample t-test. Let’s start by returning to the wide version of our dataset. We first need to calculate the difference between the before and after columns, which we can easily do with mutate(). w8_symptoms &lt;- w8_symptoms %&gt;% mutate( diff = before - after ) w8_symptoms We can run a one-sample t-test on the differences now, with the null hypothesis value being 0 - i.e. we are testing the null hypothesis that the mean differences are not significantly different from 0. t.test(w8_symptoms$diff ~ 1, mu = 0) ## ## One Sample t-test ## ## data: w8_symptoms$diff ## t = -2.9501, df = 31, p-value = 0.005999 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## -4.651185 -0.848815 ## sample estimates: ## mean of x ## -2.75 As we can see, the results are equivalent to the output of the paired-samples t-test above. Note too that our Shapiro-Wilks test will also give equivalent results if it is run on the differences directly. "],["anovas.html", "Chapter 7 ANOVAs", " Chapter 7 ANOVAs t-tests, as we saw last week, were a simple way of comparing a continuous outcome between two groups or conditions (i.e. one categorical variable with two levels). However, it’s also common to compare across three or more groups when doing real research. To test this kind of hypothesis where we have three or more groups, we need to turn to a different method - the ANOVA. ANOVAs are useful when you have one categorical IV with 3+ levels, and one continuous DV. The ANOVA is perhaps one of the most common statistical tests you will see in music psychology literature, in part because they generalise to a lot of research designs. In many respects, this week is fairly important in terms of knowing how to conduct an ANOVA and when. We’ll only be stepping through the basics this week, but there are some really important fundamentals to cover here. It’s also common to analyse two independent variables against one dependent variable. These too can be done using ANOVAs. While we won’t go into this in this module, there will be an Extension Module that deals with this separately - check it out sometime around Week 10/11 if you’re interested. By the end of this module you should be able to: Understand and describe the conceptual basis of an analysis of variance Describe how an ANOVA is calculated Conduct both one-way ANOVAs and one-way repeated ANOVAs, including their assumption tests Interpret the output of the ANOVAs and report their findings Figure 7.1: xkcd: Third Way "],["anovas-and-the-f-distribution.html", "7.1 Anovas and the F-distribution", " 7.1 Anovas and the F-distribution Just as we’ve done so for chi-squares and t-tests, let’s begin with an overview of the mathematical and conceptual underpinnings of the ANOVA. 7.1.1 The basic logic of ANOVAs As we said in the introductory page, ANOVA stands for ANalysis Of VAriance. This essentially sums up how an ANOVA works in principle. ANOVAs can be used when analysing a categorical IV with two or more groups (typically 3+) and a continuous DV. As the setup suggests, ANOVAs are a good way of comparing different groups on a singular outcome. The most basic hypotheses for an ANOVA center around whether or not the means between groups are significantly different, i.e.: \\(H_0\\): The means between groups are not significantly different. (i.e. \\(\\mu_1 = \\mu_2 = ... \\mu_k\\)) \\(H_1\\): The means between groups are significantly different. (i.e.\\(\\mu_1 \\neq \\mu_2 \\neq ... \\mu_k\\)) How do we test for this? The basic logic of the ANOVA is this: Whenever we have data that we categorise into different groups, we end up with two key sources of variability, or variance: variance that exists between groups, and variance that exists within groups: Pretend that the three curves above represent data covering three different groups, and that we’ve hypothesised that there are three different means. Collectively, this data has a certain amount of variance. The first fundamental to recognise is that this total variance can be broken down into variance between groups and variance within groups: \\[ Variance_{total} = Variance_{between} + Variance_{within} \\] The variability between the groups would simply be the variance between the blue curve and the orange curve - in other words, how far apart the two sets of data are (in a simplistic sense). The within-group variance, on the other hand, is how much variance there is within each curve. If there is a lot of within-group variance within each curve, that would mean that (thinking back to Module 5) each group’s curve would be spread widely. If you’re following the logic of this so far, the consequences of high within-group variance might be obvious - the two curves would significantly overlap. In contrast, if the between-group variance is far higher than the within-group variance, the curves may not overlap much at all - suggesting that the means of the two groups really are different. This forms the basis of the ANOVA’s F-test, which is a ratio of variance: \\[ F = \\frac{Variance_{between}}{Variance_{within}} \\] We will see this more in practice on the next page, but the F-statistic, which we use as part of significance testing in ANOVAs, is calculated by simply dividing the between-group variance by the within-group variance. 7.1.2 The F-distribution Like the other tests we’ve encountered so far, ANOVAs have their own underlying distribution. In this instance, this is the F distribution, which describes how the F-statistic behaves. We won’t go far into the maths around this, but the key here is that the F distribution is characterised by two sets of degrees of freedom: one that relates to the between-groups variance/effect, and one that relates to the within-groups variance. One thing to note here is the terminology in the headers of each graph: the first number in the brackets refers to the between-groups variance, while the second number refers to the within-groups variance. 7.1.3 F-statistic tables And, once again, we have a special F-table to determine critical F values, given two degrees of freedom values. However, given that we have two degrees of freedom the process is a little bit more complicated. Most F-stat tables actually provide multiple tables, corresponding to different alpha levels. For now, we will always use the table corresponding to alpha = 0.05, a snippet of which is below: knitr::include_graphics(here(&quot;img/w9_f-table.png&quot;)) To read this table, you need to know both degrees of freedom (more on how to find that on the next page). The columns, marked df1, refer to the first df value (between-groups), while the rows correspond to the second df value (within-groups). So, for example, if we have an F-test with degrees of freedom of (4, 10), we would first need to find the column that corresponds to 4 (our df1), and then the row that corresponds to df2 = 10. Reading the cell at the intersection would give us a critical F-statistic of 3.48; this is the value our F-statistic would need to be greater than to be significant at the p &lt; .05 level. "],["the-anova-table.html", "7.2 The ANOVA table", " 7.2 The ANOVA table Every time we do an ANOVA, there are actually a raft of calculations that have to occur in order to get our test statistic and p-value. Statistical software will display all of this in the form of an ANOVA table, which we cover below. Don’t stress too much about the maths here - while the quiz does include one question about this, the question itself isn’t difficult (and you won’t be expected to do any of this by hand otherwise). The reason why we go through this, however, is to demonstrate the conceptual logic from the previous page. 7.2.1 The ANOVA table On the previous page, we talked in depth about how the F statistic is calculated, and what shapes the overall distribution. But how do we actually calculate all of that stuff to begin with from raw data? Enter the ANOVA table, a beautiful (remember, beauty is subjective) way of calculating this information and laying it all out to see. The basic ANOVA table looks like this, which you will see on any statistical package you use. Sums of squares (SS) Degrees of freedom (df) Mean square (MS) F p Group (our effect) Error/Residual Total A couple of terminology-related things here. ‘Group’ in this context is our independent variable (i.e. the effect of group)- this is our between-subject variance. ‘Error’ or ‘Residual’ is the within-group variance. Let’s use the below data to calculate this by hand. Group 1 Group 2 Group 3 1 2 5 2 4 8 3 5 6 2 3 7 Mean: 2 Mean: 3.5 Mean: 6 The grand mean (i.e. the mean across all 12 pieces of data) is 4. Keep this in mind. Time to strap in - there are a lot of formulae involved! 7.2.2 Sums of squares The sum of squares quantifies how far each observation is from the average - just like how it is calculated in the formula for standard deviations. For an ANOVA, we have two sets of SS to calculate - one for the between-groups term, and one for the within-groups/error. Between The formula for the between-groups sum of squares (\\(SS_b\\)) is: \\[ SS_b = \\Sigma n(\\bar x - \\bar X)^2 \\] In words, this means: Take each group’s mean (\\(\\bar x\\)), and subtract them from the grand mean (\\(\\bar X\\)) Square that difference Multiply it by n, the size of each group Add them all up. Let’s do that for our fictional data. Our group means are Group 1 = 2, Group 2 = 3.5 and Group 3 = 6.5. \\(SS_b = 4(2-4)^2 + 4(3.5-4)^2 + 4(6.5-4)^2\\) \\(SS_b = (4 \\times 4) + (4 \\times 0.25) + (4 \\times 6.25)\\) \\(SS_b = 42\\) Within For the within-group sum of squares, the formula is: \\[ SS_w = \\Sigma (x - \\bar x)^2 \\] This one is a bit more tedious. It means: Take each observation (\\(x\\)), and subtract them from their group mean (\\(\\bar x\\)) Square that difference Add them all up. So for our data, it would look something like.. \\[ SS_w = (1-2)^2 + (2-2)^2 + (3-2)^2 + (2-2)^2 + (2-3.5)^2 + (4-3.5)^2 + (5-3.5)^2 + (3-3.5)^2 + (5-6.5)^2 + (8-6.5)^2 + (6-6.5)^2 + (7-6.5)^2 \\] \\[SS_w = 12\\]. 7.2.3 Degrees of freedom Because we have a term for between-groups and within-groups effects, we also have degrees of freedom for both (think back to the previous page). Thankfully, unlike the mess above the formulae here are relatively simple. Between The between-groups df is given as: \\[ df_b = k - 1 \\] Where k = the number of groups. So, in our data, \\(df_b = 3 -1 = 2\\) (as we have 3 groups). Within The within-groups df is given as: \\[ df_b = N - k \\] Where k = the number of groups, and N is the total sample size (in our case, 12). So \\(df_w = 12 -3 = 9\\) (as we have 3 groups and 12 data points in total). 7.2.4 Mean squares The mean squares is another value for variance that essentially standardises the sum of squares by the degrees of freedom. The formula for MS between and within is the same: \\[ MS = \\frac{SS}{df} \\] We’ve calculated SS and df for both our between and within-groups effects, so we can substitute these values in to calculate a mean square value for both: \\[MS_b = \\frac{SS_b}{df_b}\\] \\[MS_w = \\frac{SS_w}{df_w}\\] Putting in our values that we calculated earlier, we get: \\[MS_b = \\frac{42}{2}\\] \\[MS_w = \\frac{12}{9}\\] This gives us \\(MS_b = 21\\) and \\(MS_w = 1.33\\). 7.2.5 Calculating F Remember on the previous page, how we talked about the F-statistic being a ratio between two variances (between divided by within)? That’s exactly what we’re going to do next, using our calculated mean-square values: \\[ F = \\frac{MS_b}{MS_w} \\] This gives us: \\[ F = \\frac{21}{1.33} = 15.75 \\] 7.2.6 Putting it all together Phew! Now we’ve calculated everything we need to for our ANOVA - in essence, we’ve done the majority of the ANOVA by hand. Let’s put all of our values into the table below: Sums of squares (SS) Degrees of freedom (df) Mean square (MS) F p Group (our effect) 42 2 21 15.75 Error/Residual 12 9 1.33 Total 54 Before we move on, it’s also worth noting that \\(SS_b + SS_w = SS_{total}\\) ; this goes back to the fundamental we discussed on the previous page, where an ANOVA breaks down total variance into between and within-groups variance. 7.2.7 Consulting the F-table Now that we have our observed F-value, we can now consult an F-table and use our two degrees of freedom to find out what our critical F-value is (setting alpha = .05): Reading the column for df1 = 2, and the row for df2 = 9, we get a critical F-statistic of 4.2565. Now we can say that because our observed test statistic (15.75) is greater than this critical value, our ANOVA is significant at the alpha = .05 level. In reality, like many of the other tests, we would calculate a p-value by observing where our test statistic falls on the F-distribution, and the associated probability of getting that value or greater. Our software will do all of this stuff for us, but it helps to know exactly how an ANOVA works! If you want to calculate the p-value manurally in R though, this is entirely possible using the pf() function: pf(q = 15.75, df1 = 2, df2 = 9, lower.tail = FALSE) ## [1] 0.001149592 "],["multiple-comparisons.html", "7.3 Multiple comparisons", " 7.3 Multiple comparisons On this page we talk about what happens after a significant ANOVA result, as well as some more detail about the multiple comparisons problem, and how it can be accounted for. 7.3.1 Post-hoc tests The ANOVA table on the previous page allows us to calculate by hand whether there is a significant effect of our IV. However, it doesn’t tell us where that difference lies. Remember, we’re comparing at least three groups with each other - so we need some way of figuring out where the actual significant differences between means are. Enter post-hoc comparisons (post-hoc = after the fact). These are essentially a series of t-tests where we compare each group with each other. So, if we have groups A, B and C, and we get a significant omnibus ANOVA that tells us there is a significant difference somewhere in those means, we would run post-hoc comparisons by running individual t-tests on A vs B, then B vs C, then A vs C. However, we can’t just do this blindly, and there’s a good reason why… 7.3.2 The multiple comparisons problem In Module 6, we talked a fair bit about Type I and II error rates - i.e., the rate of making a false positive and false negative judgement respectively. Here, we’ll focus on Type I error. Recall that whenever we do a hypothesis test, we set a value for alpha, which forms our significance level. Alpha is essentially the probability of Type I errors we are willing to accept in a given test. In other words, when we use the conventional alpha = .05 as our criterion for significance, we are saying that we’re willing to accept a Type I error occurring 5% of the time - or 1 in 20. This becomes problematic when we have to conduct multiple tests at once, like what we have to do in an ANOVA. For example, let’s say a variable has 5 levels - A, B, C, D and E. If we ran an ANOVA on this data and found a significant omnibus effect, we would want to find out where that effect is. But that means that we’d have to compare A, B, C, D and E all against each other, in a round-robin tournament way (e.g. A vs B, then A vs C…). Where this becomes an issue is that each comparison will have its own 5% Type I error rate (assuming we use alpha = .05). So in this scenario, the Type I error rate will stack with each new comparison, meaning that our overall Type I error rate will climb higher and higher. This means that the chance of finding a false positive will increase (see graph below). This overall rate of Type I errors is called the family-wise error rate (FWER). ‘Family’ in this context refers to a family of comparisons - like the comparison across A to E that we saw above. 7.3.3 Correction methods One way of dealing with this is to correct the p-values to set the family-wise error rate back to 5%. We’ll go through three main types (two of which are already covered in the seminar). Tukey’s HSD Tukey’s Honest Significant Differences (Tukey HSD for short) is appropriate specifically for post-hoc tests after ANOVAs. It essentially works by first calculating the largest possible difference between each group’s means, then using that to calculate a critical mean difference. Any mean difference that is greater than this critical threshold is significant. How does the Tukey test work? Tukey’s HSD works very similarly in principle to a t-test, although the exact maths involved are slightly different. The test statistic for a Tukey test is called q which denotes the Studentized range distribution (a specific distribution). For a given comparison between two group means, the formula for q is: \\[ \\Large q = \\frac{|M_1 - M_2|}{SE} \\] Where \\(M_1\\) and \\(M_2\\) denote the means of groups 1 and 2, and SE denotes the standard error of the sum of the means. The q-statistic is then compared to a Studentized range distribution to estimate a p-value for that comparison. The shape of the Studentized range distribution is determined by alpha, k (the number of groups being compared) and df (in this case, the **error degrees of freedom***). Bonferroni The Bonferroni correction is a method that can be used both within ANOVAs and in more general contexts. The Bonferroni correction works by multiplying each p-value in a set of comparisons by the number of comparisons (i.e. the number of p-values). For example, if a set of comparisons gives the following p-values: 0.05, 0.25 and 0.10, we would apply a Bonferroni correction by multiplying each p-value by 3 (as there are 3 p-values) to get 0.15, 0.75 and 0.30 respectively. (Note: in reality, Bonferroni works by dividing alpha by the number of comparisons, i.e. 0.05/3 - but for the purpose of significance testing, the maths works out to be the same). Holm The Holm correction is another correction method. In this method, each p-value is first ranked from smallest to largest. and then numbered in a descending fashion. Using the same p-values as above, we would get 0.05, 0.10 and 0.25. The p = .05 would be ranked as 3, the p = .10 would be 2 and p = .25 would be 1. You then multiply the p-value by its rank, like the table below, to get your adjusted p-values: Original p-value Sorted p-value Rank p-value x rank 0.05 0.05 3 0.15 0.25 0.10 2 0.20 0.10 0.25 1 0.25 The Bonferroni procedure is very conservative, in that it may actually overcorrect (and subsequently reject too many); the Holm correction still corrects the overall FWER without also increasing the overall Type II rate as well. "],["eta-squared.html", "7.4 Eta-squared", " 7.4 Eta-squared TIme to go over a new effect size measure! This time, we use eta-squared (\\(\\eta^2\\)) as an effect size for ANOVAs. 7.4.1 Eta-squared and variance At the start of this module, we introduced the concept of how ANOVA partitions total variance into both between and within-subject variance: \\[ Variance_{total} = Variance_{between} + Variance_{within} \\] This partitioning allows for a simple but important way of calculating effect sizes for ANOVAs. If we’re interested in the effect of our IV (group, which is between-subject variance), we simply need to know how much of the total variance is explained by this variable. This is essentially eta-squared (\\(\\eta^2\\)), our effect size for ANOVAs. Eta-squared gives us a percentage of how much variance can be attributed to the main effect. So an eta-squared of .778 means that 77.8% of the total variance is because of the effect. It is calculated as follows: \\[ \\eta^2 = \\frac{SS_{effect}}{SS_t} \\] In other words, we divide the sum of squares for the main effect by the total sum of squares. For repeated-measures ANOVAs, the formula is the same in practice (i.e. divide the SS in the top row in the ANOVA table by the total). A related version of this is partial eta-squared (\\(\\eta^2_p\\)). This describes the effect of the IV after accounting for the variance explained by other factors. This is not so relevant for one-way designs - regular and partial eta-squared will give the same answer - but becomes more relevant when you move into factorial designs. The formula for partial eta-squared is: \\[ \\eta^2_p = \\frac{SS_{effect}}{SS_{effect} + SS_{error}} \\] Where \\(SS_{error}\\) is the sums of squares for the error/residual term in an ANOVA. In general, it’s good practice to default to at least reporting partial eta-squared. Again, in a one-way ANOVA this will give the same answer as regular eta-squared, but in a factorial design (where you have more than one IV) it will give a more precise estimate of each variable’s effect size. One more variant you will see is generalised eta-squared, which is similar to the partial variant. While partial eta-squared is great, it is sensitive to design - in other words, what variables are included in the analysis will influence the calculation of \\(\\eta^2_p\\), meaning that it is only really comparable across studies of similar design. However, not every design will manipulate every predictor (e.g. gender), and so generalised eta-squared (\\(\\eta^2_G\\)) can handle this. This effect size is best used in meta-analyses. By default, anova_test() (and other ANOVA-related packages in R) will calculate generalised eta squared (labelled ges in the output). If you want partial eta-squared, you just need to give anova_test() the extra argument effect.size = \"pes\" as follows: w9_memory %&gt;% anova_test(dv = memory_score, wid = Participant, within = time, effect.size = &quot;pes&quot;) ## ANOVA Table (type III tests) ## ## $ANOVA ## Effect DFn DFd F p p&lt;.05 pes ## 1 time 2 188 132.625 1.19e-36 * 0.585 ## ## $`Mauchly&#39;s Test for Sphericity` ## Effect W p p&lt;.05 ## 1 time 0.939 0.054 ## ## $`Sphericity Corrections` ## Effect GGe DF[GG] p[GG] p[GG]&lt;.05 HFe DF[HF] p[HF] ## 1 time 0.943 1.89, 177.21 1.05e-34 * 0.961 1.92, 180.72 2.45e-35 ## p[HF]&lt;.05 ## 1 * Otherwise, our trusty effectsize package provides functions for calculating effect sizes in R. We use a function called - you guessed it - eta_squared(). This function works in very much the same way as the other effectsize family of functions do - you need to give them an anova model to calculate effect sizes for. eta_squared(w9_slices_aov, alternative = &quot;two.sided&quot;) ## For one-way between subjects designs, partial eta squared is equivalent ## to eta squared. Returning eta squared. For repeated-measures ANOVAs, the best way to calculate eta-squared using this function is to first create a car::Anova() model using the syntax described above, and then passing the resulting model to the function. eta_squared(w9_memory_alternate, alternative = &quot;two.sided&quot;) 7.4.2 An example Here’s the ANOVA table we worked out by hand in Section 9.3: Sums of squares (SS) Degrees of freedom (df) Mean square (MS) F p Group (our effect) 42 2 21 15.75 Error/Residual 12 9 1.33 Total 54 If we were to calculate an eta-squared value for this, we would use SS effect (42) and SSt (54) like so: \\[ \\eta^2 = \\frac{42}{54} = .778 \\] 7.4.3 Interpreting eta-squared Cohen (1988) provided the following guidelines for interpreting eta-squared: \\(\\eta^2\\) = .01 is a small effect \\(\\eta^2\\) = .06 is a medium effect \\(\\eta^2\\) = .14 is a large effect With these guidelines (which aren’t perfect), a \\(\\eta^2\\) of .778 is astronomically huge. To give yourself a bit of practice, go back into Sections 9.5 and 9.6, find the eta-squared values for each in the outputs (they’re in the omnibus tables) and interpret them a) as percentages of total variance and b) using Cohen’s (1988) guidelines. "],["one-way-anova.html", "7.5 One-way ANOVA", " 7.5 One-way ANOVA In the previous section, we looked at how to compare differences between either two groups, or two points in time. But how do we compare more than that? The answer is the ANOVA (analysis of variance) - one of the most common general statistical models for hypothesis testing. If you do some statistical testing as part of your thesis, the ANOVA is likely going to be one of your most useful tools to have. 7.5.1 The basic one-way ANOVA The between-groups one-way ANOVA is the most basic form of ANOVA, which aims to test differences between two or more groups. (Typically, ANOVAs are used when there are three or more groups, but can easily be used in situations where there are only two groups.) We’ve briefly mentioned the general hypotheses for all ANOVAs, but here they are again: \\(H_0\\): The means between groups are not significantly different. (i.e. \\(\\mu_1 = \\mu_2 = ... \\mu_k\\)) \\(H_1\\): The means between groups are significantly different. (i.e.\\(\\mu_1 \\neq \\mu_2 \\neq ... \\mu_k\\)) 7.5.2 Example data In the seminar, we talk about an example from Watts et al. (2003). Below is another simple example, comparing taste ratings across three different types of slices: caramel slices, vanilla slices and lemon slices. Participants were randomly allocated to taste one of the three slices blindfolded, and were then asked to verbally rate its taste on a scale from 1-10 (10 being super tasty). w9_slices &lt;- read_csv(here(&quot;data&quot;, &quot;week_9&quot;, &quot;w9_slices.csv&quot;)) ## Rows: 63 Columns: 2 ## ── Column specification ──────────────────── ## Delimiter: &quot;,&quot; ## chr (1): group ## dbl (1): rating ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Create factor w9_slices$group &lt;- factor(w9_slices$group) head(w9_slices) 7.5.3 Assumption checks In R, to test assumptions we need to run the ANOVA first. This is because some assumptions rely on values that are calculated when the ANOVA is run. Other programs, such as Jamovi and SPSS, will hide this manual work and present the information to you automatically. This is not quite the case in R, but that’s ok! The basic function for running an ANOVA in R is the aov() function, which takes a formula input (much like t.test()). Anything created using aov() should be assigned to a new variable so that we can use it later. This creates an aov object that will a) test our main effects and b) let us check some of our assumptions. w9_slices_aov &lt;- aov(rating ~ group, data = w9_slices) There are four main assumptions for a basic ANOVA. Data must be independent of each other - in other words, one person’s response should not be influenced by another. This should come as a feature of good experimental design. The residuals should be normally distributed. We can assess this using the same methods as t-tests: either a QQ plot or a normality test (e.g. Shapiro-Wilks). You can access the residuals from the aov object directly like this: w9_slices_aov$residuals This lets us do the SW test: shapiro.test(w9_slices_aov$residuals) ## ## Shapiro-Wilk normality test ## ## data: w9_slices_aov$residuals ## W = 0.95117, p-value = 0.01409 A SW test on this data suggests the assumption is violated (W =.951, p = .014), suggesting that the residuals are not normally distributed. This is a bit of a problem in our dataset because our sample is relatively small. However - the ANOVA is fairly robust to violations of the normality assumption, meaning that non-normal residuals aren’t a major problem so long as a) you have a big enough sample size and b) the skew isn’t huge (or driven by outliers). The equality of variance (homoscedasticity) assumption. This means that the variances within each group are the same. We test this using Levene’s test, using the levene_test() function. Helpfully, levene_test() is flexible enough to work with either a formula or an aov object we have already fitted: # Either one of these will work w9_slices %&gt;% levene_test(rating ~ group, center = &quot;mean&quot;) levene_test(w9_slices_aov, center = &quot;mean&quot;) The assumption doesn’t appear to be violated (F(2, 60) = .721, p = .491). If it was, we might consider using a Welch’s ANOVA, which can be done with welch_anova_test() from rstatix. For now though, we’ll press ahead. 7.5.4 Output Here’s our main output from the ANOVA, generated using the summary() function. This is called an omnibus, because it is a general test of the hypotheses: summary(w9_slices_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group 2 22.22 11.111 6.897 0.00201 ** ## Residuals 60 96.67 1.611 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Our main results suggest that there is a significant difference between means (F(2, 60) = 6.90, p = .002). 7.5.5 Post-hoc tests The output above told us that we could reject our basic null hypothesis - that there was no difference between means. However, remember that it doesn’t tell us where those differences are. To figure out where they are, we need to follow up that ANOVA with post-hoc tests. To generate post-hocs, there are a couple of ways. Tukey tests can be calculated with TukeyHSD() or tukey_hsd(): TukeyHSD(w9_slices_aov) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = rating ~ group, data = w9_slices) ## ## $group ## diff lwr upr p adj ## lemon-caramel -0.4761905 -1.4175618 0.4651809 0.4486738 ## vanilla-caramel 0.9523810 0.0110096 1.8937523 0.0467882 ## vanilla-lemon 1.4285714 0.4872001 2.3699428 0.0015928 # These will give the same output tukey_hsd(w9_slices_aov) w9_slices %&gt;% tukey_hsd(rating ~ group) Here, you can see that we run a test for caramel vs lemon, caramel vs vanilla and lemon vs vanilla. For post-hocs in one-way ANOVAs, it is ok to stick to the Tukey-adjusted p-values. We can see that: There is no significant difference between ratings of caramel and lemon slices (mean difference, MD = .48; p = .449). There is a (marginal) significant difference between caramel and vanilla slices; participants rated vanilla slices higher than caramel slices (MD = .95, p = .047). There is a significant difference between lemon and vanilla slices, in that participants preferred vanilla slices (MD = 1.43, p = .002). To generate Bonferroni or Holm-corrected p-values, you need to use the pairwise.t.test() function: pairwise.t.test(x = w9_slices$rating, g = w9_slices$group, p.adjust.method = &quot;bonferroni&quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: w9_slices$rating and w9_slices$group ## ## caramel lemon ## lemon 0.6866 - ## vanilla 0.0541 0.0017 ## ## P value adjustment method: bonferroni rstatix::pairwise_t_test() will also work, and looks like this: w9_slices %&gt;% pairwise_t_test(rating ~ group, p.adjust.method = &quot;bonferroni&quot;) In both instances, you must provide p.adjust.method as an argument. By default, both versions will use Holm corrections. A one-way ANOVA was conducted to examine whether there were differences in preferences for types of slices. There was a significant large main effect of slice type (F(2, 60) = 6.90, p = .002, \\(\\eta^2\\) = .187). Post-hoc tests with Tukey HSD corrections showed that participants rated vanilla slices higher than caramel slices (mean difference, MD = .95, p = .047), as well as significantly higher than lemon slices (MD = 1.43, p = .002). However, there was no significant differencce between caramel and lemon slices (p = .449). "],["one-way-repeated-measures-anova.html", "7.6 One-way repeated measures ANOVA", " 7.6 One-way repeated measures ANOVA The second form of ANOVA that we will cover in this module is the repeated measures ANOVA (sometimes abbreviated as RM-ANOVA). While it shares many features with the basic one-way ANOVA, the nature of repeated measures data introduces some key differences in the interpretation and calculation of this test. 7.6.1 Repeated measures ANOVAs In principle, a repeated-measures ANOVA is very similar to the paired-samples t-test: both are repeated measures versions of their respective between-groups versions. So naturally, repeated-measures ANOVAs are used when we test one sample two or more times (again, like a regular ANOVA, it’s typically for 3+ times but can be used for two). It shares many similarities with the basic one-way ANOVA, albeit with one additional step in calculation: because we now test the same sample repeatedly, we need to account for variance within subjects. We won’t go too into detail about how that’s calculated here, but essentially we split within-groups variance into subject variance and error/residual variance. This essentially adds an extra step to the ANOVA table. 7.6.2 Example data In this example, we’ll use a dataset derived from McPherson (2005). This is a subset of data where children were scored on their ability to play songs from memory over three years - 1997 - 1999. We’re interested in seeing whether this change over time is significant - therefore, time (3 levels) is our independent variable, while playing from memory is our dependent variable. w9_memory &lt;- read_csv(here(&quot;data&quot;, &quot;Week_9&quot;, &quot;w9_playing_from_memory.csv&quot;)) ## Rows: 95 Columns: 4 ## ── Column specification ──────────────────── ## Delimiter: &quot;,&quot; ## dbl (4): Participant, PFM_97, PFM_98, PFM_99 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(w9_memory) For further analyses, we’ll shape this into long format: w9_memory_wide &lt;- w9_memory w9_memory &lt;- w9_memory %&gt;% pivot_longer( cols = PFM_97:PFM_99, names_to = &quot;time&quot;, values_to = &quot;memory_score&quot; ) 7.6.3 Assumption checks There are two main assumptions for a repeated-measures ANOVA. Note that while the independence assumption as we know it doesn’t apply here (by definition, repeated data is dependent), good experimental design should still aim to ensure that participants are independent of each other. The residuals should be normally distributed. Our usual tests apply here too. Below is a QQ plot, which might suggest that our residuals aren’t normally distributed. (Use the data and perform a SW test on it to see what happens!) aov(memory_score ~ time, data = w9_memory) %&gt;% broom::augment() %&gt;% ggplot(aes(sample = .std.resid)) + geom_qq() + geom_qq_line() ## Warning: The `augment()` method for objects of class `aov` is not maintained by the broom team, and is only supported through the `lm` tidier method. Please be cautious in interpreting and reporting broom output. ## ## This warning is displayed once per session. The sphericity assumption. Sphericity is assumed if, the variances of the differences between each level of the IV are equal. Think of it as a form of the equality of variances assumption, where we assumed that the variances within groups were equal. The sphericity assumption applies to the differences between T1 and T2, then T2 and T3… etc etc. Note that sphericity only applies when you have at least 3 levels of your IV (i.e. 3 timepoints). We can formally test it using Mauchly’s test of sphericity. If sphericity is violated, it means that our degrees of freedom are too high for the data, which inflates the Type I error rate. What next? We need to apply a correction to the omnibus ANOVA, which will alter the p-value. There are two on offer: Greenhouse-Geisser and Huynh-Feldt corrections. To help decide which one to use, R also calculates a value called epsilon (\\(\\epsilon\\)). Epsilon, in short, is a measure of sphericity; if sphericity is assumed, \\(\\epsilon\\) = 1. If \\(\\epsilon\\) is below 1, sphericity is violated; the smaller it is, the greater the violation and therefore the greater the correction needs to be. Therefore, these corrections alter the degrees of freedom for each test to account for this higher error rate. (Mathematical note; the corrected dfs are calculated by multiplying the original dfs by \\(\\epsilon\\). So e.g. if your original df is 10 and \\(\\epsilon\\) = 0.9, your new corrected df will be 9.) Broadly: If epsilon (\\(\\epsilon\\)) &gt; .75, use the Huynh-Feldt correction. If epsilon (\\(\\epsilon\\)) &lt; .75, use the Greenhouse-Geisser correction. In R, most main packages will test the sphericity assumption along with the main ANOVA, so we will see this below. 7.6.4 ANOVA output R-Note: Repeated measures ANOVAs in R are not trivial in the slightest, and adapting this was a genuine challenge. To some extent this probably reflects the differences in approach between point-and-click software compared to actually having to code the ANOVA model: the former is easy but you perhaps make many assumptions about what’s going on along the way. Below is the easiest way to run this analysis. Here’s our overall output from the ANOVA. It looks (and reads) pretty much the same as our previous example, but just bear in mind that the top row in the within-subjects effect (i.e. the effect of interest). In addition, this time you can see that there are now three rows to be read: w9_memory_aov &lt;- w9_memory %&gt;% anova_test(dv = memory_score, wid = Participant, within = time) w9_memory_aov ## ANOVA Table (type III tests) ## ## $ANOVA ## Effect DFn DFd F p p&lt;.05 ges ## 1 time 2 188 132.625 1.19e-36 * 0.231 ## ## $`Mauchly&#39;s Test for Sphericity` ## Effect W p p&lt;.05 ## 1 time 0.939 0.054 ## ## $`Sphericity Corrections` ## Effect GGe DF[GG] p[GG] p[GG]&lt;.05 HFe DF[HF] p[HF] ## 1 time 0.943 1.89, 177.21 1.05e-34 * 0.961 1.92, 180.72 2.45e-35 ## p[HF]&lt;.05 ## 1 * Knowing which row to read is crucial here, as each one corresponds to a correction (or the original ANOVA). Here, our p-value is almost significant (p = .054); rather than going by a strict arbitrary cutoff, let’s assume that it has been violated (both for teaching and for methodological purposes). Here, our epsilon value is high (~.95), so let’s apply the Hyunh-Feldt correction. The associated epsilon value is \\(\\epsilon = .943\\); when reporting the repeated measures ANOVA we need to look at the rows corresponding to the Huynh-Feldt correction. The two columns to look for are the ones labelled DF[HF] - this gives the adjusted degrees of freedom with the Huynh-Feldt corrections (DF[GG] would give Greenhouse-Geisser dfs) - and p[HF], which gives the adjusted p-value. Thus, with the corrected output we can see our effect of time is significant (F(1.92, 180.72) = 132.63, p &lt; .001). 7.6.5 Post-hoc tests As per usual, we follow up a significant overall ANOVA with a series of post-hoc comparisons (note this time we set paired = TRUE: w9_memory %&gt;% pairwise_t_test(memory_score ~ time, paired = TRUE, p.adjust.method = &quot;bonferroni&quot;) From this, we can see that all three means are significantly different from each other (p &lt; .001, for brevity’s sake). Scores increased year on year, i.e. 1997 &lt; 1998 &lt; 1999. A one-way repeated measures ANOVA was conducted to examine whether scores for playing songs from memory changed over three years. We applied Huynh-Feldt corrections to account for non-sphericity. There was a significant large effect of year on performance scores (F(1.92, 180.72) = 132.63, p &lt; .001, \\(\\eta^2\\) = .231). Post-hoc tests with Tukey HSD corrections showed that participants scored higher in 1998 than in 1997 (mean difference, MD = 29.61, p &lt; .001). Participants also scored higher in 1999 compared to 1998 (MD = 15.27, p &lt; .001) and 1997 (MD = 44.88, p &lt; .001). 7.6.6 Alternative using car::Anova() Below is an alternative way of running a repeated measures ANOVA, this time using the car package. The function factorial_design() from the rstatix package is a useful helper function that can generate the necessary arguments to Anova() for repeated measures tests. w9_memory_design &lt;- factorial_design( data = w9_memory, dv = memory_score, within = time, wid = Participant ) The important things that are generated by this function are: name$model: this essentially creates an ANOVA model name$idata: this specifies the levels of the within-subject (repeated measures) factor name$idesign: this creates a formula-style notation specifying what the within-subjects IV(s) are We then just need to feed this to car::Anova() as follows. Setting type = 3 isn’t strictly necessary here (this is more relevant for factorial ANOVAs): w9_memory_alternate &lt;- car::Anova( mod = w9_memory_design$model, idata = w9_memory_design$idata, idesign = w9_memory_design$idesign, type = 3 ) Now that we’ve built our ANOVA model we can ask for the output like so. multivariate = FALSE is just an argument to specify that we’re doing a univariate analysis (i.e. one dependent outcome). summary(w9_memory_alternate, multivariate = FALSE) ## ## Univariate Type III Repeated-Measures ANOVA Assuming Sphericity ## ## Sum Sq num Df Error SS den Df F value Pr(&gt;F) ## (Intercept) 3490869 1 259027 94 1266.83 &lt; 2.2e-16 *** ## time 98948 2 70130 188 132.63 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Mauchly Tests for Sphericity ## ## Test statistic p-value ## time 0.93912 0.053894 ## ## ## Greenhouse-Geisser and Huynh-Feldt Corrections ## for Departure from Sphericity ## ## GG eps Pr(&gt;F[GG]) ## time 0.94261 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## HF eps Pr(&gt;F[HF]) ## time 0.9612827 2.447568e-35 "],["linear-relationships.html", "Chapter 8 Linear relationships", " Chapter 8 Linear relationships Up until now we’ve been dealing almost exclusively with categorical variables in some way. For example, chi-square tests are for testing relationships between categorical variables; likewise, t-tests and ANOVAs deal with categorical IVs against continuous DVs. In reality though, many of the things we’re interested in are inherently continuous in nature. There are very few psychological constructs that aren’t continuous in some way, and so working with continuous variables forms a core part of doing statistical analyses. Enter the linear regression and its many other forms - in some ways, the bedrock of many of the research that we do (more on this in Week 11). When we’re dealing with continuous variables, linear regressions are generally the first place to start. We won’t go much further beyond the basics here (certainly as it applies to a lot of psychological research), but even these concepts are foundational for many reasons. By the end of this module you should be able to: Describe how hypothesis testing works in regressions - including what is being tested Conduct appropriate assumption tests for a linear regression Run a linear regression and interpret the output Make predictions based on the output of a linear regression Run and interpret a multiple regression Figure 8.1: xkcd: Linear Regression "],["correlations.html", "8.1 Correlations", " 8.1 Correlations We’ve talked a surprising amount about correlations in this subject, but we haven’t considered how to actually test if two things are correlated to begin with. We change that this week with an overview of correlation coefficients. 8.1.1 Correlation coefficients We can quantify the strength of two variables using a correlation coefficient, which gives us a measure of how tightly these two variables are related. To start, we need to know what covariance is. Covariance simply describes how two variables change with each other. For instance, if two variables have a positive covariance, this means that as one variable increases, so does the other. Similarly, if two variables have a negative covariance, this means that as one increases the other decreases. See if you can describe what the covariances would be like below: Correlation coefficients are just another way of describing this. Correlation coefficients have the following properties: They are between -1 and +1 The sign of the correlation describes the direction - a positive value represents a positive correlation The numerical value describes the magnitude A correlation of 1 means a perfect correlation; a correlation of 0 means a negative correlation A rough guideline for this subject, r = .20 is weak, r = .50 is moderate, r = .70 is strong Visually, a magnitude of 0 corresponds to a flat line; the steeper the line, the higher the magnitude There are many types of correlation coefficients, but the most common is the Pearson’s correlation coefficient. It’s calculated using the below (simplified) formula: \\[ r = \\frac{Cov_{xy}}{SD_x \\times SD_y} \\] In this subject we won’t expect you to calculate a correlation coefficient by hand, but the key takeaway here is that by dividing a value by a standard deviation (or, in this case, a product of two SDs), we are standardising the covariance. Hence, a correlation coefficient is a standardised measure, meaning that we can compare correlation coefficients quite easily across variables regardless of their scale. 8.1.2 Activity 8.1.3 Testing correlations in R Statistical programs like Jamovi and R will allow us to not only quantify a correlation between two variables, but test whether this correlation is significant. Generally, when working with continuous data it never hurts to run a basic correlation. Here is an example using a simple dataset containing the gender, height and speed (the fastest they had ever driven). w10_speed &lt;- read_csv(here(&quot;data&quot;, &quot;week_10&quot;, &quot;W10_speed.csv&quot;)) ## New names: ## Rows: 1325 Columns: 4 ## ── Column specification ## ──────────────────── Delimiter: &quot;,&quot; chr ## (1): gender dbl (3): ...1, speed, height ## ℹ Use `spec()` to retrieve the full column ## specification for this data. ℹ Specify the ## column types or set `show_col_types = ## FALSE` to quiet this message. ## • `` -&gt; `...1` Let’s correlate height and speed. This can easily be done in R by using the cor.test() function. You simply need to give it the names of the two columns you want to correlate. By default, this function will run a Pearson’s correlation. cor.test(w10_speed$height, w10_speed$speed) ## ## Pearson&#39;s product-moment correlation ## ## data: w10_speed$height and w10_speed$speed ## t = 9.2871, df = 1300, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.1977889 0.2997013 ## sample estimates: ## cor ## 0.2494356 We can see that our correlation is r = .249, which is a relatively weak to moderate correlation. This correlation is also significant (p &lt; .001). We also get a confidence interval around the size of the correlation, which is great for showing the range of possible values. So we might write this up as something like: There was a significant weak positive correlation between students’ heights and their fastest ever driving speed (r(1300) = .25, p &lt; .001; 95% CI [.20, .30]). If you need to run multiple correlations at once, there are two ways to visualise them. Below are two types, using fictional questionnaire data. The first is simply a table, like below: Q1 Q2 Q3 Q4 Q1 1.00 0.24 -0.56 0.72 Q2 0.24 1.00 -0.38 0.43 Q3 -0.56 -0.38 1.00 -0.11 Q4 0.72 0.43 -0.11 1.00 The second is a correlation heatmap, which is especially effective with many correlations at once (common when working with huge questionnaires or neuroimaging). As shown by the legend on the right, the colour and shade of each square are determined by the strength of the correlation. This can be easily done by the `ggcorrplot package, if you have a correlation matrix formatted in R: library(ggcorrplot) ggcorrplot(cor_mat, type = &quot;lower&quot;, show.diag = TRUE) "],["regression-hypotheses.html", "8.2 Regression hypotheses", " 8.2 Regression hypotheses Let’s move on from correlations to regressions, where we test whether one variable can predict another. To do that, let’s start by considering what it is we actually hypothesise in doing a linear regression. 8.2.1 Terminology Let’s kick off the regression portion of this module with a bit of terminology: - The line is called the line of best fit. The slope of the line is, well, the slope. It describes how much Y changes if X changes by one unit. - The point at which the line crosses the y-axis is called the intercept (the y-intercept in full). The intercept is one of the two paramaters of a regression line (the other being the slope). Linear regressions involve plotting a line of best fit to the data, and using this line to make predictions. If you think back to high school, you may have learned something like y = mx + c or y = ax + b in algebra to describe a straight line. In linear regression, we use the same concepts to both describe our line and make predictions using it (more on that later). The key difference is that we change the letters a bit: \\[ y = \\beta_0 + \\beta_1x + \\epsilon_i \\] Let’s break this down: y is simply our predicted value (i.e. the line of best fit) \\(\\beta_0\\) is our intercept (the c in y = mx + c) \\(x\\) simply refers to our independent variable \\(\\beta_1\\) is the slope for the independent variable - in other words, how much y increases for every unit increase of x. We also call this B \\(\\epsilon_i\\) is error, which is essentially random variation (due to sampling). This error is normally distributed. Keep this in mind for now - we’ll come back to this later in the module! 8.2.2 Hypothesis testing in regressions When we conduct a linear regression, in part we’re testing if the two variables are correlated. However, we’re also testing whether we can predict our DV from our IV, so our statistical hypotheses are formulated around this idea. Our null and alternative hypotheses are therefore about the slope (\\(\\beta_1\\)): \\(H_0: \\beta_1 = 0\\), i.e. the slope is 0 \\(H_1: \\beta_1 \\neq 0\\), i.e. the slope is not equal to 0 Consider the two graphs below. On the left is a graph where the line of best fit has a slope of 0 (the null). No matter what value X is, the value of Y is always the same (2.5 in this example) - in other words, X does not predict Y. The graph on the right side, on the other hand, is an example of the alternative hypothesis in this scenario. Here, X does clearly predict Y - as X increases, Y increases as well. But how do we actually test this? The answer is something we’ve seen before - we do a t-test! We came across t-tests in context of comparing two means against each other. The logic here is exactly the same, except now we compare two slopes with each other - the slope we actually observe (B) minus the null hypothesis slope (0). We can use this logic to calculate a t-statistic using the below formula: \\[ t = \\frac{B}{SE_B} \\] Where B = observed slope and SE = standard error of B. This is the same formula as the calculation for a t-test, albeit that the top row is just B (because it is B - 0). We can do this for each predictor, and then use the same t-distribution to test whether this slope is significant - in other words, whether our predictor (IV) significantly predicts our outcome (DV). "],["least-squares-regression.html", "8.3 Least squares regression", " 8.3 Least squares regression Linear regression starts with trying to fit a line to our continuous data. But… how on earth do we figure out where that line sits? 8.3.1 The regression line See if you can take a guess where we should draw a line of best fit on the plot below: You might have some good guesses, and there’s every chance that you’ve drawn a line that fits the data points pretty well. But for this data, it’s pretty obvious where the line would sit, and the data that we do work with won’t always be as clear cut. The line of best fit sits where the distance between the line and every point is minimised. See the example below, where we have 10 data points to illustrate. Each dot represents a single data point, while the solid blue line is our line of best fit. The dashed lines represent the difference between the blue line (which is what we predict - more on this later) and the actual data point. This difference is called a residual. So, in other words, the line of best fit sits where the residuals are minimised. We do this using the least squares principle. In essence, we calculate a squared deviation for each residual using the formula below - it might be familiar… \\[ SS_{res} = \\Sigma (y_i - \\bar y_i)^2 \\] Where \\(y_i\\) is each individual (ith) value on the y-axis, and \\(\\bar y_i\\) is the predicted y-value (i.e. the regression line). Essentially, we calculate the residual for each data point, square it and add it all up. The actual minimising part is something we won’t concern ourselves with for this subject (or at all), because quite frankly it’s complicated - the key here is to understand how a regression line is fit to the data. "],["assumption-tests.html", "8.4 Assumption tests", " 8.4 Assumption tests As usual, there are several assumptions that we need to test for regressions. Some people call these regression diagnostics - they mean the same as assumption testing. 8.4.1 Linearity Believe it or not, for a linear regression your data should be… linear. Wild, right? Bitter sarcasm aside, linearity is an important assumption that needs to be met before conducting a linear regression. Not all data will follow a linear pattern; some data may instead sit on a curve. Compare the two examples below, where one is clearly non-linear: If the data shows no clear linear relationship between your IV and DV, it is likely because the two are weakly correlated (and so a linear regression would not be useful anyway). If instead the data lies on a very obvious curve, you can either: Transform the data to make it linear (but you must be clear about what this represents) Attempt to fit a curve and see whether this has more explanatory power, but madness lies this way for the unprepared 8.4.2 Homoscedasticity The homoscedasticity assumption is one that we’ve seen before - it is essentially a version of the equality of variance test. In a linear regression context, however, this assumption works a little bit differently; if this assumption is met, then the variance should be equal at all levels of our predictor (i.e. the x-axis). The easiest way to test this is to create a fitted vs residuals graph. As the name implies, we take the fitted values for each level of the predictor in our data, and plot that against the residuals (fitted - actual). Here are some fitted vs residual plots for two sets of data. The data on the left has an even spread of variance as X increases, meaning that it is homoscedastic; the data on the right, on the other hand, spreads out like a cone. The data on the right therefore is likely heteroscedastic. ## Warning: Removed 7 rows containing missing values or ## values outside the scale range ## (`geom_point()`). There are a couple of ways to overcome this, such as either transforming the raw variables or weighting them. 8.4.3 Independence This one is the same - data points should be independent of each other. Like we have seen with other tests, this should ideally be a feature of good experimental design. In linear regression, a specific issue is autocorrelation - where the residuals between two values of X are not independent. If the residuals are not independent, your data likely exhibits signs of autocorrelation (i.e. it is correlated with itself). This can distort the relationship between your IV and DV, and indicates that your data are connected through time. We can test this using a test called the Durbin-Watson test of autocorrelation. The Durbin-Watson test will estimate a coefficient/test statistic that quantifies the degree of autocorrelation (dependence) between observations in the data. The DW test statistic ranges from 0 to 4, with the following interpretations: A DW test statistic of ~2 indicates no autocorrelation. A DW test statistic of &lt; 2 indicate positive autocorrelation - i.e. one data point will positively influence the next. A DW test statistic of &gt; 2 indicates negative autocorrelation. The general principle of the Durbin-Watson test, therefore, is to have a test statistic close to 2 and (ideally) a non-significant result. The value of the test statistic is generally more useful than the significance of the test alone. A common guideline for interpreting this value is that a DW test statistic below 1 or above 3 is problematic, which we will also use for this subject. The DurbinWatsonTest() function from the DescTools package will let us test this quite easily. DescTools::DurbinWatsonTest(mod_1) ## ## Durbin-Watson test ## ## data: mod_1 ## DW = 2.0317, p-value = 0.5619 ## alternative hypothesis: true autocorrelation is greater than 0 8.4.4 Normality The normality assumption is the same here as it has been elsewhere - the residuals must be normally distributed. Here, it’s a little bit easier to visualise what these ‘residuals’ are because we can calculate and see them. That being said, the way we test for these are exactly the same - either use a Q-Q plot or a Shapiro-Wilks test. 8.4.5 Multicollinearity The multicollinearity (sometimes just called collinearity) assumption only applies to multiple regressions, where you have more than one predictor in your test. multicollinearity occurs when two predictors are too similar to one another (i.e. they are highly correlated with each other. This becomes a problem at the individual predictor level, because what happens is that the effect of predictor A becomes muddled by predictor B - in other words, if two predictors are collinear, it becomes impossible to tell apart which one is contributing what to the regression. There are three basic ways you can assess this. The simplest is to test a correlation between the two predictors. If they are very highly correlated (use r = .80 as a rule of thumb), this is likely to be a problem. The variance inflation factor (VIF) is a more formal measure of multicollinearity. It is a single number, and a higher value means greater collinearity. If a VIF is greater than 5 this suggests an issue. Tolerance is a related value to VIF - in fact, it is just 1 divided by the FID - and works in much the same way - except smaller values mean greater collinearity. As a rule of thumb, if tolerance is smaller than 0.20 this suggests an issue. To calculate VIF, you can use the VIF() function from the DescTools package - more on the relevant page. "],["linear-regressions-in-r.html", "8.5 Linear regressions in R", " 8.5 Linear regressions in R Let’s now turn to an example of how to do a linear regression in R. 8.5.1 Scenario Musical sophistication describes the various ways in which people engage with music. In general, the more ways in which people engage with music, the more musically sophisticated they are. One hypothesis is that years of musical training will clearly influence musical sophistication. While this is a bit of a no-brainer hypothesis, we’ll test it using some example data. w10_goldmsi &lt;- read_csv(here(&quot;data&quot;, &quot;week_10&quot;, &quot;w10_goldmsi.csv&quot;)) ## Rows: 74 Columns: 2 ## ── Column specification ──────────────────── ## Delimiter: &quot;,&quot; ## dbl (2): years_training, GoldMSI ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # This extra code is here for later w10_goldmsi &lt;- w10_goldmsi %&gt;% mutate( id = seq(1, nrow(w10_goldmsi)) ) head(w10_goldmsi) 8.5.2 Assumptions In R the linear regression needs to be coded first before testing assumptions. Linear regression models can be built using lm(), and the same formula notation used in aov(): w10_goldmsi_lm &lt;- lm(GoldMSI ~ years_training, data = w10_goldmsi) Let’s test our assumptions from the previous page. Linearity: a simple scatterplot tells us that our data probably is suitable for a linear regression: w10_goldmsi %&gt;% ggplot( aes(x = years_training, y = GoldMSI) ) + geom_point() Independence: our DW test is significant (p = .021), but our test statistic is 1.54. So, even though it’s still significant it probably isn’t much of a problem.4 DurbinWatsonTest(w10_goldmsi_lm) ## ## Durbin-Watson test ## ## data: w10_goldmsi_lm ## DW = 1.5409, p-value = 0.02123 ## alternative hypothesis: true autocorrelation is greater than 0 Homoscedasticity: To plot a residual vs fitted plot in R, there are two ways: - plot(model, which = 1). plot(w10_goldmsi_lm, which = 1) Using ggplot, but giving the model name instead of the data and setting the x and y aesthetics as follows: ggplot(w10_goldmsi_lm, aes(x = .fitted, y = .resid)) + geom_point() Normality: our SW test is non-significant, so this assumption is not violated. Like aov models, residuals in lm models can be accessed using the $ operator. shapiro.test(w10_goldmsi_lm$residuals) ## ## Shapiro-Wilk normality test ## ## data: w10_goldmsi_lm$residuals ## W = 0.98995, p-value = 0.8289 8.5.3 Output The output that R gives us for a linear regression comes in two parts: a test of the overall model, and a breakdown of the predictors and their slopes. summary(w10_goldmsi_lm) ## ## Call: ## lm(formula = GoldMSI ~ years_training, data = w10_goldmsi) ## ## Residuals: ## Min 1Q Median 3Q Max ## -35.617 -9.227 2.025 9.773 33.666 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 53.901 5.233 10.300 8.33e-16 *** ## years_training 4.858 1.138 4.271 5.86e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.61 on 72 degrees of freedom ## Multiple R-squared: 0.2021, Adjusted R-squared: 0.191 ## F-statistic: 18.24 on 1 and 72 DF, p-value: 5.859e-05 Let’s start with the overall model. This tells us whether the regression model as a whole is significant. A couple of things to note here. \\(R^2\\) is called the coefficient of determination. This value tells how much variance in the outcome is explained by the predictor. Our value is .202, which means that 20.2% of the variance in Gold-MSI scores is explained by years of training. The overall model test is essentially an ANOVA (we won’t go too deep into why just yet!), which tells us whether the model is significant. In this case, it is (F(1, 72) = 18.24, p &lt; .001). Now we can look at the middle of the output, which gives the model coefficients. This part of the output tells us whether the predictors are significant. (Given the overall model is, with one predictor we’d expect this to be significant as well!). The Estimate column gives us the B coefficient, which is our slope. This tells us how much our outcome increases when our predictor increases by 1 unit. From this, we can see that years of training is a significant predictor of Gold-MSI scores (t = 4.27, p &lt; .001); for every year of training, Gold-MSI scores increase by 4.86 (given by the value under Estimate, next to our predictor variable). Finally, we can use the confint function to return 95% confidence intervals on each regression slope: confint(w10_goldmsi_lm) ## 2.5 % 97.5 % ## (Intercept) 43.46925 64.332391 ## years_training 2.59058 7.125814 Therefore, the 95% CI around the estimated regression coefficient of 4.86 is [2.59, 7.13]. Here is how we can write up these results. Note that for a regression, it is important to discuss both the overall model fit and the specific effect of the predictor. We conducted a simple linear regression to examine whether years of training predicted Gold-MSI scores. The overall model was significant (F(1, 72) = 18.24, p &lt; .001), with years of training explaining 20.2% of the variance in Gold-MSI scores (\\(R^2\\) = .202). Years of training was a significant positive predictor of Gold-MSI scores (B = 4.86, t = 4.27, p &lt; .001). Given that this data was randomly generated, it probably is due to a simulation outlier more than anything.↩︎ "],["predictions.html", "8.6 Predictions", " 8.6 Predictions A significant result from a linear regression tells us that our IV significantly predicts our outcome. We can actually use the results of the regression to make predictions about our outcome. This can be really useful in a number of contexts. 8.6.1 Revisiting the linear regression equation Let’s come back to the equation for a linear regression: \\[ y = \\beta_0 + \\beta_1x + \\epsilon_i \\] The results from the linear regression on the previous page allow us to construct a line of best fit. Using this line of best fit, we can make predictions about a participant’s score on the dependent variable, given their score on the independent/predictor variable. 8.6.2 Building a regression equation Here’s the coefficient table from the previous page: summary(w10_goldmsi_lm) ## ## Call: ## lm(formula = GoldMSI ~ years_training, data = w10_goldmsi) ## ## Residuals: ## Min 1Q Median 3Q Max ## -35.617 -9.227 2.025 9.773 33.666 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 53.901 5.233 10.300 8.33e-16 *** ## years_training 4.858 1.138 4.271 5.86e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.61 on 72 degrees of freedom ## Multiple R-squared: 0.2021, Adjusted R-squared: 0.191 ## F-statistic: 18.24 on 1 and 72 DF, p-value: 5.859e-05 This table tells us the following things: The value of the intercept, \\(\\beta_0\\), is 53.901 The value of the slope, \\(\\beta_1\\), is 4.858 Now we can make our equation as such: \\[ GoldMSI = 53.901 + (4.858 \\times Years) \\] We can now use this to predict scores! 8.6.3 An example prediction Participant 47, highlighted in green below, has 5 years of musical training. What would their predicted Gold-MSI score to be? ## `geom_smooth()` using formula = &#39;y ~ x&#39; We can use the equation we just built to calculate a predicted score: Therefore, we would predict someone with 5 years of musical training to have a Gold-MSI score of 78.191. This is where the line sits. Notice however, that the predicted value is noticeably different to the participant’s actual value (which in this instance is 56). The difference between the predicted and the actual value is called the residual - precisely the same residual that we aim to minimise when we fit a regression line to begin with (as well as the same residuals we do assumption tests on). \\(GoldMSI = 53.901 + (4.858 \\times Years)\\) \\(GoldMSI = 53.901 + (4.858 \\times 5)\\) \\(=78.191\\) A warning While these predictions can be useful, there are two warnings that should be kept in mind. Extrapolation is dangerous. While we might get data that appears linear, there is nothing to say that this data will remain linear outside of the bounds of our data. Extrapolating data refers to making inferences beyond the available range, and should be avoided. Don’t forget that some data have logical boundaries. For example, the Gold-MSI’s maximum possible score is 126 across all scales. Any preditions that are higher than this are therefore quite easily nonsensical. "],["multiple-regression-theory.html", "8.7 Multiple regression: Theory", " 8.7 Multiple regression: Theory If all of that stuff on the previous page made sense then great! You’re now ready to tackle multiple regressions, which are an extension of the simple linear regression. You’ll see that much of the same stuff applies here, but a few things change… 8.7.1 Multiple regression Multiple regression is used when we want to test multiple predictors against an outcome variable. It’d be a safe bet to say that multiple regression and its various forms are probably one of the most used statistical tests in music psychology literature as a whole - you’ll see them everywhere! No introduction to linear regression would really be complete without at least scratching the surface of multiple regression. The name “multiple regression” is actually a fairly generic term in some respects, as it describes any instance of regression with two or more predictors. I say that because there are several forms of multiple regression, such as: Standard multiple regression, which we will cover in this subject Hierarchical multiple regression, where you split your analysis into blocks Stepwise multiple regression, where algorithms attempt to select the best predictors And more! 8.7.2 The regression equation, once again Recall that in a simple linear regression, we had this formula to describe the line of best fit: \\[ y = \\beta_0 + \\beta_1x + \\epsilon_i \\] In a multiple regression, we work with an extension of this formula. The key part here is the part of the equation labelled \\(\\beta_1x\\) - this is the part of the equation that relates to the individual predictor and its slope (i.e. how it predicts the outcome). We extend this in a multiple regression. For example, say we now had two predictors: \\[ y = \\beta_0 + \\beta_1x_1 +\\beta_2x_2 + \\epsilon_i \\] We now have a term for our first predictor \\(\\beta_1x_1\\) and for our second: \\(\\beta_2x_2\\). x1 and x2 simply mean predictor 1 and predictor 2. The betas here are still regression slopes; the subscript numbers just indicate which predictor they correspond to. From here on, much of the same reasoning that we saw in the early pages of this module apply. The primary hypotheses are now about whether each slope is significantly different from zero - which would indicate whether each predictor does significantly predict the outcome. 8.7.3 Assumption testing in multiple regressions All of the following assumption tests apply: Linearity Independence Homoscedasticity Normality Multicollinearity We’ll test these on the next page! "],["multreg-intro.html", "8.8 Multiple regressions in R", " 8.8 Multiple regressions in R Phew - we’re almost there! Let’s round out this week’s module by doing a standard multiple regression in Jamovi. 8.8.1 Example data Let’s look at a real set of data from Rakei et al. (2022). Their study looked at what predicts how prone people are to flow - the experience of being ‘in the moment’ and extremely focused while doing a task, such as performing. They measured a wide range of personality and emotion-related variables, but we’ll focus on just a subset here: Trait anxiety: broadly, refers to people’s tendency to feel anxious Openness to experience: a personality trait that describes how likely people are to seek new experiences DFS_Total: a measure of proneness to flow. Let’s see if trait anxiety and openness predict flow proneness. w10_flow &lt;- read_csv(here(&quot;data&quot;, &quot;week_10&quot;, &quot;w10_flow.csv&quot;)) ## Rows: 811 Columns: 6 ## ── Column specification ──────────────────── ## Delimiter: &quot;,&quot; ## dbl (6): id, age, GoldMSI, DFS_Total, trait_anxiety, openness ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. To build a multiple regression model in R, we simply need to add (literally) more terms to lm(): w10_flow_lm &lt;- lm(DFS_Total ~ trait_anxiety + openness, data = w10_flow) 8.8.2 Assumption checks Our Durbin-Watson test suggests no issues with independence of observations. DurbinWatsonTest(w10_flow_lm) ## ## Durbin-Watson test ## ## data: w10_flow_lm ## DW = 1.9456, p-value = 0.2182 ## alternative hypothesis: true autocorrelation is greater than 0 The Shapiro-Wilks test suggests that our normality assumption is violated - though if you look at the relevant Q-Q plot, it doesn’t appear to be very severe. shapiro.test(w10_flow_lm$residuals) ## ## Shapiro-Wilk normality test ## ## data: w10_flow_lm$residuals ## W = 0.99311, p-value = 0.0008488 The residuals-fitted plot for our data looks ok - no obvious conical shape, suggesting that the homoscedasticity assumption is intact. ggplot(w10_flow_lm, aes(x = .fitted, y = .resid)) + geom_point() Lastly, our multicollinearity statistics look good - no violations suggested here. # Variance DescTools::VIF(w10_flow_lm) ## trait_anxiety openness ## 1.025955 1.025955 # Tolerance 1/DescTools::VIF(w10_flow_lm) ## trait_anxiety openness ## 0.9747012 0.9747012 8.8.3 Output Let’s start as always by taking a look at our output. Our regression explains 13.6 of the variance in flow proneness (\\(R^2\\) = .136), and our overall model is significant (F(2, 808) = 63.81, p &lt; .001). With that in mind, let’s turn to our individual predictors. Remember, now we have two predictors to consider - and we need to interpret them individually as well. summary(w10_flow_lm) ## ## Call: ## lm(formula = DFS_Total ~ trait_anxiety + openness, data = w10_flow) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16.596 -2.331 -0.151 2.308 12.794 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 32.65078 1.13377 28.798 &lt; 2e-16 *** ## trait_anxiety -0.11116 0.01278 -8.697 &lt; 2e-16 *** ## openness 0.83372 0.14538 5.735 1.38e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.705 on 808 degrees of freedom ## Multiple R-squared: 0.1364, Adjusted R-squared: 0.1343 ## F-statistic: 63.81 on 2 and 808 DF, p-value: &lt; 2.2e-16 confint(w10_flow_lm) ## 2.5 % 97.5 % ## (Intercept) 30.4252962 34.87625896 ## trait_anxiety -0.1362446 -0.08606874 ## openness 0.5483533 1.11907751 We can see that both trait anxiety and openness are significant predictors of flow proneness, but interpreting the direction is also really important here. For that, we turn to our estimates: The estimate for trait anxiety is B = -.111 (95% CI = [-.136, .086]), suggesting that higher trait anxiety predicts lower flow proneness (p &lt; .001) On the other hand, the estimate for openness is B = .834 (95% CI = [.548, 1.119]), suggesting that higher openness predicts higher flow proneness (p &lt; .001). Therefore, we can conclude that both are significant predictors and have opposite effects to each other. We can write this up as follows: We ran a multiple regression with trait anxiety and openness as predictors, and flow proneness as an outcome. The overall regression model was significant (F(2, 808) = 63.81, p &lt; .001), and explained 13.6% of the variance in flow proneness. Trait anxiety significantly predicted flow proneness (B = -.11, 95% CI = [-.14, -.09], t = -8.70, p &lt; .001); higher trait anxiety predicted lower flow proneness. Openness was also a significant predictor (B = .83, 95% CI = [.55, 1.12], t = 5.74, p &lt; .001); higher openness predicted higher flow proneness. But there’s one more thing we can consider here… 8.8.4 Comparing predictive strength When we do a multiple regression, we can actually identify which of our predictors is the strongest predictor of the outcome. Our estimate column alone won’t tell us that, because these are unstandardised coefficients. They are unstandardised because they describe the relationship in terms of the original units of each measure. An increase of 1 unit on trait anxiety is not necessarily the same thing as a 1 unit increase in openness because they are on different scales, so we can’t compare them directly! However, if we were to standardise these coefficients, we would then bring them all into the same scale (think back to z-scores!) - which would allow us to directly compare which predictor leads to a greater change in the outcome. These are simply regression coefficients that have been standardised, meaning that they are all on the same scale. Jamovi will calculate a standardised estimate (sometimes called standardised coefficient), which is presented in the rightmost column of the above table. Standardised estimates are denoted as beta (\\(\\beta\\)), and thus are sometimes called standard betas. Standard betas allow us to interpret our coefficients in terms of changes in standard deviations; namely, a 1 SD increase in predictor X leads to a \\(\\beta\\) SD change in predictor Y. In R, we need to use an extra function to calculate standard betas. We can call on the lm.beta package: library(lm.beta) lm.beta(w10_flow_lm) ## ## Call: ## lm(formula = DFS_Total ~ trait_anxiety + openness, data = w10_flow) ## ## Standardized Coefficients:: ## (Intercept) trait_anxiety openness ## NA -0.2879944 0.1899044 Alternatively, we can simply standardise our predictors before running our regression (which is a lot easier in R than in other programs): # Standardise predictors w10_flow_std &lt;- w10_flow %&gt;% mutate( trait_anxiety = scale(trait_anxiety), openness = scale(openness), DFS_Total = scale(DFS_Total) ) w10_flow_lm_std &lt;- lm(DFS_Total ~ trait_anxiety + openness, data = w10_flow_std) summary(w10_flow_lm_std) ## ## Call: ## lm(formula = DFS_Total ~ trait_anxiety + openness, data = w10_flow_std) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.1675 -0.5855 -0.0379 0.5795 3.2128 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.281e-16 3.267e-02 0.000 1 ## trait_anxiety -2.880e-01 3.311e-02 -8.697 &lt; 2e-16 *** ## openness 1.899e-01 3.311e-02 5.735 1.38e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9304 on 808 degrees of freedom ## Multiple R-squared: 0.1364, Adjusted R-squared: 0.1343 ## F-statistic: 63.81 on 2 and 808 DF, p-value: &lt; 2.2e-16 confint(w10_flow_lm_std) ## 2.5 % 97.5 % ## (Intercept) -0.06413295 0.06413295 ## trait_anxiety -0.35299439 -0.22299437 ## openness 0.12490436 0.25490437 This means that we can just compare the numbers of the standardised estimates (ignoring positive/negative signs) to see which one is the greatest predictor of the outcome. In the example above, trait anxiety has a standard coefficient of .288, whereas openness has a standard coefficient of .190. Trait anxiety is therefore a stronger predictor of flow proneness, because a 1 standard deviation increase in trait anxiety leads to a greater increase in flow proneness - namely, a change of .288 standard deviations. "],["factorial-anova.html", "Chapter 9 Factorial ANOVAs", " Chapter 9 Factorial ANOVAs In this module we return to the humble ANOVA, and to working with categorical predictors again. Specifically, we move beyond just having one categorical predictor in an ANOVA model to having two (and all of the complexities that come with doing so). We’ll also take a look at interactions, which form a crucial part of more complex models specifying relationships between our predictors. This module covers: Two-way between-subjects ANOVA Two-way repeated measures ANOVA TWo-way mixed ANOVA Three-way between-subjects ANOVA "],["factorial-intro.html", "9.1 Introduction", " 9.1 Introduction In Week 9 we talked about analyses of variance - tests for comparing means across one categorical IV/predictor (typically with at least 3 levels) on one continuous outcome. In this extension module we move to some more complex ANOVA models - specifically two-way ANOVAs, where we have two categorical predictors and one continuous outcome. The main goal is still largely the same - comparing means across groups - but becomes a bit more complex with the involvement of two variables. We’ll also briefly consider the instance of a three-way ANOVA (i.e. three predictors), but won’t spend too much time on this for good reason. We’ll also talk briefly about some statistical concepts that we haven’t covered in the main subject content, specifically about interactions and contrasts. By the end of this module you should be able to: Describe what an interaction is Conduct and interpret an omnibus two-way ANOVA Distinguish between post-hoc tests, simple effects tests and planned contrasts Attempt to think about a three-way ANOVA (but see warnings on the relevant page) "],["factorial-designs.html", "9.2 Factorial designs", " 9.2 Factorial designs In Week 9, when we talk about ANOVAs we conduct one-way ANOVAs. These tests from Week 9 are called one-way ANOVAs because there is only one IV with multiple levels being tested. However, in many research designs we will want to test the effect of two or more categorical variables at the same time (for example, experimental conditions or variables that capture important categories, such as participant sex). When we want to test the effect of more than one IV, we start getting into what we call factorial ANOVAs. Factorial ANOVAs are used when we have two or more IVs, each with at least two levels per variable. This is common in a lot of research designs, where either multiple categorical variables are collected as part of the data collection phase or categorical variables are created as part of the analysis process. We will talk about this more on the next page, but factorial designs are particularly useful for testing interactions, and the effects of both of your IVs together. When reporting results for factorial designs, it is expected that you report how many levels each variable had. For instance, let’s say your two IVs are participant sex (male and female) and experimental group (group A, B and C). If you were to test this factorial design, there are a couple of ways you could report this: A Sex (2) x Group (3) factorial ANOVA A 2 x 3 factorial ANOVA A Sex x Group (2 x 3) factorial ANOVA The first one is the preferable because it lays out the conditions clearly. 9.2.1 What is an interaction? Pretend you’ve been enacting a singing intervention in a school of kids, where one group of kids have been singing daily and another group have not been. You’re interested in whether the singing intervention has an effect on their wellbeing. By and large, the singing intervention does - there is a clear difference between the kids who get singing sessions and kids who don’t. However, you notice that how effective the intervention is depends on whether they are boys or girls. The girls appear to benefit the most, while the boys don’t seem to as much. In other words, the effectiveness of the intervention is contingent on the biological sex of the child. This is an example of an interaction, where the effect of one IV depends on the effect of another IV. The consequence of an interaction is that the two IVs both influence the DV together (in a non-additive manner). Interactions can be important for understanding how certain phenomena work. Consider the two plots below, that show the relationship between two predictors (X and Group) and one outcome (on the y-axis). In the graph on the left, there is a clear difference between groups 1 and 2. There is also a clear difference between A, B and C on X; however, this is constant. In the graph on the right, there’s still a clear difference between groups 1 and 2. However, the difference is greater between different groups. For group 1, there is no change from A to C, but there is for group 2; in other words, the effect of X depends on the effect of Group. The easiest way of demonstrating an interaction is by using an interaction plot, like the one above. This kind of graph plots means as dots, and joins different groups/IVs together by lines. Interaction plots with error bars (e.g. +/- 1 standard error) provide the clearest way of graphing of an interaction effect. 9.2.2 Testing for interactions We can test for interactions when we have at least two independent variables/predictors, using both ANOVAs and regressions. The majority of this module will focus on instances with two predictors in an ANOVA context, as they are easiest to conceptualise. By default, if we have two predictors - A, and B, and an outcome, Y - our model will have the following terms: A, or the main effect of A (i.e. of A only) B, the other main effect A x B, which is our interaction effect Therefore, we end up with two types of effects that we need to interpret: main effects, and interaction effects. An interaction effect is what we call a higher-order term, in that it is a more complex term in our model. We test the significance of each term, giving us three p-values and sets of test statistics. "],["simple-effects-tests.html", "9.3 Simple effects tests", " 9.3 Simple effects tests Remember from the previous page that when we have two predictors, we end up with three model terms: A, a main effect B, a main effect A x B, which is our interaction effect Therefore, we end up with two types of effects that we need to interpret: main effects, and interaction effects. An interaction effect is what we call a higher-order term, in that it is a more complex term in our model. We test the significance of each term, giving us three p-values and sets of test statistics. Here’s an example of a two-way ANOVA with a significant interaction. Notice that there are three effects here: one for gender, one for education and the gender x education level interaction. (We’ll go through how to run these models a bit later.) ## Anova Table (Type 3 tests) ## ## Response: score ## Effect df MSE F pes p.value ## 1 gender 1, 52 0.30 0.59 .011 .448 ## 2 education_level 2, 52 0.30 189.17 *** .879 &lt;.001 ## 3 gender:education_level 2, 52 0.30 7.34 ** .220 .002 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 How do we interpret this? Clearly, we have no main effect of gender (p = .448) but we do have an effect of education level (p &lt; .001). We also have a significant interaction term: gender x education (p = .002). Here is where something called the principle of marginality kicks in. The principle of marginality, states that if two variables interact with each other, the main effects of each variable are marginal to their interaction. In more simple terms, this means that a significant interaction is a better explanation of the main effects than the main effects themselves. In context, this means that the significant effect of education level is actually best explained by decomposing the gender x education level interaction. Therefore, if you have a significant interaction you want to break this down first. If the interaction is not significant, you can run post-hocs on the main effects only. But like a regular ANOVA, this only tells us that there is an interaction. How do we find out which means are different? 9.3.1 Simple effects tests One option is to conduct post-hoc tests like normal, and run post-hocs on the interaction term. But this is not necessarily meaningful: ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = tmp_formula, data = dat.ret, contrasts = contrasts) ## ## $gender ## diff lwr upr p adj ## male-female 0.1932143 -0.09680436 0.4832329 0.1870895 ## ## $education_level ## diff lwr upr p adj ## school-college -0.7573684 -1.187898 -0.3268386 0.0002637 ## university-college 2.4944417 2.069328 2.9195559 0.0000000 ## university-school 3.2518102 2.826696 3.6769243 0.0000000 ## ## $`gender:education_level` ## diff lwr upr p adj ## male:college-female:college -0.2396667 -0.9873581 0.508024749 0.9317495 ## female:school-female:college -0.7220000 -1.4497494 0.005749384 0.0529764 ## male:school-female:college -1.0363333 -1.7840247 -0.288641918 0.0019203 ## female:university-female:college 1.9430000 1.2152506 2.670749384 0.0000000 ## male:university-female:college 2.8290000 2.1012506 3.556749384 0.0000000 ## female:school-male:college -0.4823333 -1.2300247 0.265358082 0.4086560 ## male:school-male:college -0.7966667 -1.5637819 -0.029551460 0.0374890 ## female:university-male:college 2.1826667 1.4349753 2.930358082 0.0000000 ## male:university-male:college 3.0686667 2.3209753 3.816358082 0.0000000 ## male:school-female:school -0.3143333 -1.0620247 0.433358082 0.8132166 ## female:university-female:school 2.6650000 1.9372506 3.392749384 0.0000000 ## male:university-female:school 3.5510000 2.8232506 4.278749384 0.0000000 ## female:university-male:school 2.9793333 2.2316419 3.727024749 0.0000000 ## male:university-male:school 3.8653333 3.1176419 4.613024749 0.0000000 ## male:university-female:university 0.8860000 0.1582506 1.613749384 0.0087499 A more targeted approach is to conduct simple effects tests. Simple effects tests are a form of pairwise comparisons that are run to break down an interaction. It involves running pairwise comparisons between one predictor at every level of the other predictor. Using the example above, this might include running pairwise comparisons between education levels for males and females separately: ## gender = female: ## contrast estimate SE df t.ratio p.value ## college - school 0.722 0.246 52 2.935 0.0050 ## college - university -1.943 0.246 52 -7.899 &lt;.0001 ## school - university -2.665 0.246 52 -10.834 &lt;.0001 ## ## gender = male: ## contrast estimate SE df t.ratio p.value ## college - school 0.797 0.259 52 3.073 0.0034 ## college - university -3.069 0.253 52 -12.143 &lt;.0001 ## school - university -3.865 0.253 52 -15.295 &lt;.0001 Or, to spin it the other way, you might compare males and females for each education level separately: ## education_level = college: ## contrast estimate SE df t.ratio p.value ## female - male 0.240 0.253 52 0.948 0.3473 ## ## education_level = school: ## contrast estimate SE df t.ratio p.value ## female - male 0.314 0.253 52 1.244 0.2191 ## ## education_level = university: ## contrast estimate SE df t.ratio p.value ## female - male -0.886 0.246 52 -3.602 0.0007 Generally, it is wise to run simple effects tests both ways - as this decomposes the interaction into something that is interpretable. This is usually guided by theoretical reasons (i.e. a hypothesis about which simple effects to run). Of course, a good graph will tell the rest of the story: "],["r-specific-considerations.html", "9.4 R-specific considerations", " 9.4 R-specific considerations This page discusses some basic considerations about performing factorial ANOVAs in R. 9.4.1 The afex package In the sections on one-way ANOVAs we largely stuck to using base R’s aov() function, or the anova_test() function from the rstatix package. While we can absolutely continue to use these for factorial ANOVAs, one problem is that factorial ANOVAs are inherently more complex models than their one-way progenitors. For instance, aov() only really works with balanced designs, which is where every cell (i.e. group x group combination of your IVs/predictors) has the same number of participants. To use an example, a 2 x 2 ANOVA where each of the four possibilities (e.g. Level 1 Variable A + Level 1 Variable B…) has 20 participants is a balanced design. In contrast, if A1-B2 had 40 participants and the other possibilities had 20 participants, this would be an unbalanced design. Unbalanced designs introduce several complexities for ANOVAs. For that reason, now is a good time to introduce the afex package, which stands for “analysis of factorial experiments”. The afex package is designed for factorial ANOVAs in particular. It provides a very nice and convenient way of running factorial ANOVAs in a way that’s not too hard, while still being flexible enough for more hardcore R users. In particular, the function you will want to keep in mind is aov_ez(). 9.4.2 Writing interactions in R The aov_ez() function that was just mentioned doesn’t use the same kind of formula notation that we saw in the other test sections, and that’s to make it easier to run. However, these analyses absolutely can be recreated in formula notation, and you will see many instances of this. Recall the basic layout for a formula in R: lm(outcome ~ predictor, data = dataset) aov(outcome ~ predictor, data = dataset) To extend this to two variables with no interaction, you can simply add the second predictor as such: aov(outcome ~ predictor_1 + predictor_2, data = dataset) To code for an interaction term, however, there either needs to be an explicit term for the interaction (denoted using a colon between the two interaction variables): aov(outcome ~ predictor_1 + predictor_2 + predictor_1:predictor_2, data = dataset) OR as the shorthand for above, which uses the asterisk. Note that the asterisk will include both the interaction term and all of its main effects (a la principle of marginality): aov(outcome ~ predictor_1 * predictor_2, data = dataset) For what we do in this section, this is ok - but you may find in certain circumstances that you don’t want all of these terms together, so the first approach will let you specify what terms to include in the model. 9.4.3 Simple effects tests in R Simple effects tests, weirdly, are difficult to do in Jamovi - the gamlj module in Jamovi will do simple effects tests for linear models (which includes ANOVAs), but it’s difficult to do so for anything other than a two-way between-groups ANOVA without some prerequisite knowledge on linear mixed models. In R, simple effects are relatively easy to estimate through the use of the emmeans package. emmeans stands for estimated marginal means, which are model-derived estimates of each group’s mean. Simple effects tests are generally run on the estimate marginal means, and thus this package allows us a handy way to run these comparisons. There are two steps to using emmeans: Define the estimated marginal means for the simple effects test you wish to run. Imagine an ANOVA model named model_aov and two predictors named A and B. If we wanted to run a simple effects test between each level of A for every level of B, we can define the following: model_em &lt;- emmeans(model_aov, ~ A, by = &quot;B&quot;) This will estimate the EM means for every level of A, at each level of B. We assign this to a variable called model_em, as we will use this later. Generate pairwise comparisons for the simple effects test. Continuing on from the example above, we can use the pairs() function on our EM object, model_em, to run the simple effects test. infer = TRUE will generate confidence intervals for the difference in EM means. By default this will be set at 95% confidence, which is typically what we want. pairs(model_em, infer = TRUE) If we want to adjust the p-values at this stage, we can give pairs() an additional argument adjust, with the name of the adjustment method as a string (e.g. pairs(x, adjust = \"holm\".)) To run simple effects tests for variable two, the variable names simply need to be swapped. Optionally, to do this all in one go we could simply pipe from emmeans() to pairs as follows: emmeans(model_aov, ~ A, by = &quot;B&quot;) %&gt;% pairs(infer = TRUE) Final note on this matter: Rstatix does provide a function called emmeans_test to perform a similar test. However, using the original emmeans package and its functions give you much greater flexibility, so it helps to learn how to use this as is. "],["two-way-between-groups-anova.html", "9.5 Two-way between groups ANOVA", " 9.5 Two-way between groups ANOVA 9.5.1 Introduction Two-way between-groups ANOVAs are used when you have two categorical IVs, both of which are between-groups variables. For example, you might have the following IVs: Sex: Male or female Experimental group: Group A or Group B There are four possibilities here - males in Group A, females in Group A, males in Group B and females in Group B. These are mutually exclusive categories in the context of this design. 9.5.2 Example One of the earliest modern studies on music’s effect on consumer behaviour came from Hargreaves and North, who played either French or German music in a wine shop. They looked at how much people spent on French and German wines, and tested to see whether the effect of background music and wine type had an effect on spending. We’re going to step through an analogous (fictional) example. Pretend we played either country or classical music, and looked at when people purchased beer or wine. (We’re going to assume we’ve removed people who bought both beer and wine together.) We’re interested in whether these two variables had an effect on how much people spent. Therefore, we have two between-groups IVs: Alcohol type being purchased: was the person buying beer or wine? Genre of background music: was country or classical music being played at the time? We therefore has two independent variables: alcohol type (beer, wine) and genre of background music (country, classical). Her dependent variable is total spend. This can be described in two key ways: Two-way ANOVA (alcohol type x genre) 2 x 2 ANOVA: more specifically, 2 (beer, wine) x 2 (country, classical) ANOVA between alcohol type and genre We’d use a two-way ANOVA to model the effects of two IVs on one dependent variable. Let’s start by generating descriptives and a graph to visualise what we’re looking at: twoway_alcohol &lt;- read_csv(here(&quot;data&quot;, &quot;factorial&quot;, &quot;twoway_bganova.csv&quot;)) twoway_alcohol %&gt;% group_by(alcohol_type, genre) %&gt;% summarise( n = n(), mean = mean(spend), na.rm = TRUE, sd = sd(spend, na.rm = TRUE), se = sd/n ) %&gt;% ungroup() %&gt;% ggplot( aes(x = alcohol_type, y = mean, colour = genre, group = genre) ) + geom_point() + geom_line() + geom_errorbar(aes(ymin = mean - 1.96*se, ymax = mean + 1.96*se), width = 0.2) 9.5.3 Assumption testing The assumptions for a two-way ANOVA are the same as a one-way between groups ANOVA: The data should be independent of each other. Equality of variance: Look at Levene’s test: twoway_alcohol_aov &lt;- aov(spend ~ alcohol_type * genre, data = twoway_alcohol) twoway_alcohol %&gt;% levene_test(spend ~ alcohol_type * genre) Normality of residuals: Look at the Shapiro-Wilk test (on the residuals) or a Q-Q plot: shapiro.test(twoway_alcohol_aov$residuals) ## ## Shapiro-Wilk normality test ## ## data: twoway_alcohol_aov$residuals ## W = 0.99647, p-value = 0.9295 Both assumptions appear to be intact. 9.5.4 Output Let’s first look at our output for the omnibus ANOVA. The way to read this table is much like the same for a one-way ANOVA, but now we need to read across each line - as each line represents a different effect. Note that here, we use the aov_ez() function from the afex package. Because in many instances we work with unbalanced data (i.e. each group x group combination does not have the same number of participants), we tend to calculate something called Type III Sums of Squares/ANOVAs. We won’t dive too much into this, but this is essentially about how sums of squares are calculated, and how the effects are tested. Learning Statistics with R has an excellent explanation of what the various types are. So we see that: There is no main effect for alcohol (p = .137), There is a main effect for genre (p &lt; .001), Importantly, there is a significant interaction effect for alcohol x genre (p &lt; .001). twoway_alcohol_aov &lt;- aov_ez( data = twoway_alcohol, id = &quot;ptcpt&quot;, dv = &quot;spend&quot;, between = c(&quot;alcohol_type&quot;, &quot;genre&quot;), anova_table = list(es = &quot;pes&quot;), include_aov = TRUE ) ## Converting to factor: alcohol_type, genre ## Contrasts set to contr.sum for the following variables: alcohol_type, genre twoway_alcohol_aov ## Anova Table (Type 3 tests) ## ## Response: spend ## Effect df MSE F pes p.value ## 1 alcohol_type 1, 196 1.02 2.23 .011 .137 ## 2 genre 1, 196 1.02 4295.08 *** .956 &lt;.001 ## 3 alcohol_type:genre 1, 196 1.02 519.96 *** .726 &lt;.001 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 Because we have a significant interaction term, we now need to turn to conducting simple effects tests to decompose where this effect is. You might want to use something like tukey_hsd() to generate every possible pairwise comparison, like so: tukey_hsd(twoway_alcohol_aov$aov) Remember, for a simple effects test we want to test for differences between the levels of one variable, at every level of the other variable. In our instance, that may be comparing genre type (classical vs country) for beer purchases, and then for wine purchases. Our post-hoc table gives us this information - but not all rows are necessarily important here. Let’s hold alcohol as a constant, and compare genre for each type of alcohol purchased. That means that we need to look for the following comparisons: For beer, country vs classical For wine, country vs classical If we were to frame it in terms of the variables in the dataset, we would be looking for: Beer country - beer classical Wine country - wine classical Conveniently, this is captured in the very top and bottom rows of our post-hoc output table. Let’s also run our simple effects tests using emmeans: emmeans(twoway_alcohol_aov, ~ genre, by = &quot;alcohol_type&quot;) %&gt;% pairs(infer = TRUE) ## alcohol_type = beer: ## contrast estimate SE df lower.CL upper.CL t.ratio p.value ## classical - country 6.11 0.202 196 5.72 6.51 30.242 &lt;.0001 ## ## alcohol_type = wine: ## contrast estimate SE df lower.CL upper.CL t.ratio p.value ## classical - country 12.64 0.202 196 12.24 13.04 62.416 &lt;.0001 ## ## Confidence level used: 0.95 We can see that beer purchases were significantly greater with classical music on in the background than country (t = 30.24, p &lt; .001). Likewise, wine purchases were also significantly greater with classical music than country music (t = 62.42, p &lt; .001). Of course, it might make more sense to do the simple effects tests the other way round; in other words, hold genre constant and compare how much was spent on each alcohol type. You can still absolutely find out this information, but now you would be looking for the following rows: For country, wine vs beer (wine country - beer country): t = 15.34, p &lt; .001 For classical, wine vs beer (wine classical - beer classical): t = 16.85, p &lt; .001 emmeans(twoway_alcohol_aov, ~ alcohol_type, by = &quot;genre&quot;) %&gt;% pairs(infer = TRUE) ## genre = classical: ## contrast estimate SE df lower.CL upper.CL t.ratio p.value ## beer - wine -3.48 0.206 196 -3.88 -3.07 -16.847 &lt;.0001 ## ## genre = country: ## contrast estimate SE df lower.CL upper.CL t.ratio p.value ## beer - wine 3.05 0.198 196 2.66 3.44 15.378 &lt;.0001 ## ## Confidence level used: 0.95 9.5.5 Write-up A two-way ANOVA was conducted to test whether alcohol type and music genre had an effect on spending habits. We found a significant main effect of music genre (F(1, 196) = 4295.08, p &lt; .001) but no significant main effect of alcohol type (p = .137). We found a significant two-way interaction between alcohol type and genre (F(1, 196) = 519.97, p &lt; .001). To follow up this two-way interaction, we conducted simple effects tests. The simple effect of genre was analysed for each level of alcohol type with Holm corrections. On average, people spent $6.11 more on beer if they heard classical music compared to country music (t(196) = 30.242, p &lt; .001). Likewise, on average people spent $12.64 more on wine if they heard classical music compared to country (t(196) = 62.416, p &lt; .001). "],["two-way-repeated-measures-anova.html", "9.6 Two-way repeated measures ANOVA", " 9.6 Two-way repeated measures ANOVA 9.6.1 Introduction Similar to its one-way counterpart, in a two-way ANOVA we assess the effect of two within-subjects variables on a dependent variable. The following example is completely fictional, and has no real grounding in theory (as far as I’m aware). It’s intentionally facetious, but hopefully demonstrates the process of doing a two-way repeated measures ANOVA. 9.6.2 Example If you did this subject in 2023 you would have met Victor and Gloria in the second statistics assignment, who were working with data looking at what predicts high school GPA. They’ve now moved onto a new project about whether listening to happy or sad music may affect exam performance. To do this, they design a nifty little study. They recruit 20 participants and bring them into the lab. This is their procedure: Participants come into the lab and do a series of standard maths exams. The maths exams are either easy, medium or hard, and participants are randomly assigned to do either the easy or the hard one first. They are also randomly assigned either happy or sad background music as they do the exam. Once they have completed the first exam, they take a 10 minute break and then do another exam that is easy, medium, or hard, and with either happy or soft music in the background. This process repeats until they have done all combinations of difficulty and background music. Each exam is scored out of 100. We have two within-subjects independent variables here: Difficulty of the exam (3 levels: easy, medium, hard) Background music (2 levels: happy or sad) Every participant therefore does 6 maths exams: easy-happy, easy-sad, medium-happy, medium-sad, hard-happy and hard-sad. The dependent variable of interest is their exam score. We’re interested in whether the difficulty and the music type have an effect on exam performance. Because our two IVs are within-subject variables, we will want to use a two-way repeated measures ANOVA. twoway_exam &lt;- read_csv(here(&quot;data&quot;, &quot;factorial&quot;, &quot;twoway_rmanova_long.csv&quot;)) ## Rows: 120 Columns: 4 ## ── Column specification ──────────────────── ## Delimiter: &quot;,&quot; ## chr (2): difficulty, music ## dbl (2): ptcpt, grade ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. R will generally treat character variables as factors, but will order the factors alphabetically. In our instance, because we have a specific order for the categories that is not alphabetical and we want our graph to reflect this, we will need to tell R what order our levels are. This is simple to do with the factor() function, which will create factors from columns in your data. factor() first needs to know what column you are wanting to create factors in, and then it will want to know the specific order of the factors. The order is set with the levels argument. We will recode both the difficulty and the music type variables for completion’s sake, and there are two ways to go about doing this. First, you can just use base R functions like so: twoway_exam$music &lt;- factor(twoway_exam$music, levels = c(&quot;happy&quot;, &quot;sad&quot;)) Or you can use factor() within mutate and largely identical syntax: twoway_exam &lt;- twoway_exam %&gt;% mutate( difficulty = factor(difficulty, levels = c(&quot;easy&quot;, &quot;medium&quot;, &quot;hard&quot;)) ) Let’s start with the usual descriptives and graphs: ## `summarise()` has grouped output by ## &#39;difficulty&#39;. You can override using the ## `.groups` argument. Eyeballing the graph, we can see that there might be some sort of effect happening. Unsurprisingly, it looks like the easy maths exams are, well. easier, because people are scoring better on them. There might be an effect of music, because in both easy and hard conditions it looks like people do better with happy music compared to sad music. But the interaction plot tells us there’s something clearly going on, and it paints a really interesting story on its own. It appears that there’s almost a ‘plateau’-ing effect with medium difficulty, in that both happy and sad hit the same point in exam scores. However, it looks like for happy music, harder exams see no further decrease in performance - but there is a sharp drop again for sad music. 9.6.3 Assumptions The assumptions for a two-way RM ANOVA is the same as a one-way: The data from the conditions should be normally distributed. (More specifically, the residuals should be normally distributed.) The data for each subject should be independent of every other subject. Sphericity must be met. As is the case with one-way repeated measures ANOVAs, the assumption of sphericity is only tested when there are three or more levels; with only two levels, the assumption is always met. The output is below with the main ANOVA output. The sphericity for all of our effects is intact, so we don’t have any issues here, but if we did the same principle would apply - we would apply our corrections depending on the value of epsilon. Let’s also see if our variables are normally distributed: R-Note: For a repeated measures ANOVA, for some reason the closest I can get to generating what Jamovi does is to build an aov() model without an explicit Error() term - as would be the case for a fully between-subjects ANOVA. You can then use the standardised residuals to create a Q-Q plot using the below code. Note that broom:augment() is just a helper function that nicely extracts the residuals: aov(grade ~ difficulty * music, data = twoway_exam) %&gt;% broom::augment() %&gt;% ggplot(aes(sample = .std.resid)) + geom_qq() + geom_qq_line() It’s not… great but for the purposes of demonstration, we’ll run with it anyway. 9.6.4 Output Here’s our output from R: twoway_exam_aov &lt;- aov_ez( data = twoway_exam, id = &quot;ptcpt&quot;, dv = &quot;grade&quot;, within = c(&quot;difficulty&quot;, &quot;music&quot;), anova_table = list(es = &quot;pes&quot;), include_aov = TRUE ) twoway_exam_aov ## Anova Table (Type 3 tests) ## ## Response: grade ## Effect df MSE F pes p.value ## 1 difficulty 1.79, 33.92 40.38 189.61 *** .909 &lt;.001 ## 2 music 1, 19 48.16 18.39 *** .492 &lt;.001 ## 3 difficulty:music 1.65, 31.36 39.89 10.29 *** .351 &lt;.001 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 ## ## Sphericity correction method: GG # To get sphericity output summary(twoway_exam_aov) ## ## Univariate Type III Repeated-Measures ANOVA Assuming Sphericity ## ## Sum Sq num Df Error SS den Df F value Pr(&gt;F) ## (Intercept) 363220 1 511.30 19 13497.322 &lt; 2.2e-16 *** ## difficulty 13668 2 1369.60 38 189.613 &lt; 2.2e-16 *** ## music 886 1 915.03 19 18.390 0.0003968 *** ## difficulty:music 677 2 1251.07 38 10.286 0.0002691 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Mauchly Tests for Sphericity ## ## Test statistic p-value ## difficulty 0.87972 0.31556 ## difficulty:music 0.78826 0.11750 ## ## ## Greenhouse-Geisser and Huynh-Feldt Corrections ## for Departure from Sphericity ## ## GG eps Pr(&gt;F[GG]) ## difficulty 0.89263 &lt; 2.2e-16 *** ## difficulty:music 0.82526 0.0007226 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## HF eps Pr(&gt;F[HF]) ## difficulty 0.9789636 4.104076e-20 ## difficulty:music 0.8936983 4.903345e-04 Once again this is quite a busy table! We can see that there is a significant main effect of difficulty (F(2, 38) = 189.61, p &lt; .001), as well as a significant main effect of music type (F(1, 19) = 18.39, p &lt; .001). There is also a significant interaction effect of difficulty and music (F(2, 38) = 10.29, p &lt; .001). We can also see our sphericity output here; neither the difficulty variable (W = .880) nor the difficulty x music interaction (W = .788) terms show significant violation of sphericity (p &gt; .05). Therefore we have not corrected for anything in our main ANOVA output. Note that because music only has two levels, there is no sphericity test for it. To decompose this, we’ll do our usual simple effects tests - holding one variable constant and running pairwise comparisons with the other. For this example, we’ll just hold the difficulty constant and compare music genres. We can absolutely reverse the simple effects, but for the sake of simplicity we’ll just do it the one way round for this example. emmeans(twoway_exam_aov, ~ music, by = &quot;difficulty&quot;) %&gt;% pairs(infer = TRUE, adjust = &quot;none&quot;) ## difficulty = easy: ## contrast estimate SE df lower.CL upper.CL t.ratio p.value ## happy - sad 4.4 1.78 19 0.682 8.12 2.477 0.0228 ## ## difficulty = medium: ## contrast estimate SE df lower.CL upper.CL t.ratio p.value ## happy - sad 0.2 1.58 19 -3.097 3.50 0.127 0.9003 ## ## difficulty = hard: ## contrast estimate SE df lower.CL upper.CL t.ratio p.value ## happy - sad 11.7 2.40 19 6.675 16.72 4.873 0.0001 ## ## Confidence level used: 0.95 From this we can see that for easy exams, on average people scored 4.4 points higher with happy music compared to sad music (p = .023). On medium exams, there was no difference between happy and sad music (p = .900). Lastly, for harder exams people who listened to happy music scored, on average, 11.7 points higher than people who listened to sad music (p &lt; .001). This suggests overall that happy music is probably better for completing exams than sad music, though the medium difficulty exam is a strange one here. If, instead, we wanted to hold music type constant and test differences in difficulty, we would get this output. emmeans(twoway_exam_aov, ~ difficulty, by = &quot;music&quot;) %&gt;% pairs(infer = TRUE, adjust = &quot;none&quot;) ## music = happy: ## contrast estimate SE df lower.CL upper.CL t.ratio p.value ## easy - medium 21.1 1.76 19 17.42 24.78 12.005 &lt;.0001 ## easy - hard 21.4 2.20 19 16.79 26.01 9.718 &lt;.0001 ## medium - hard 0.3 1.72 19 -3.29 3.89 0.175 0.8630 ## ## music = sad: ## contrast estimate SE df lower.CL upper.CL t.ratio p.value ## easy - medium 16.9 1.69 19 13.36 20.44 9.989 &lt;.0001 ## easy - hard 28.7 1.98 19 24.55 32.85 14.483 &lt;.0001 ## medium - hard 11.8 1.74 19 8.16 15.44 6.791 &lt;.0001 ## ## Confidence level used: 0.95 This paints a much more interesting picture about what is going on, and might be a more interesting way of decomposing the interaction. But remember - only do one or the other! "],["two-way-mixed-anova.html", "9.7 Two-way mixed ANOVA", " 9.7 Two-way mixed ANOVA 9.7.1 Introduction In a mixed factorial design, we want to see the effect of a between groups variable and a within-groups variable on a continuous DV. These are also sometimes called repeated measures ANOVAs (i.e. repeated measures designs with a between-group variable), but this can be a little confusing; for that reason, calling them a mixed factorial model is preferred. The data we’ll use for this one is of a mock longitudinal randomised-controlled trial. This kind of design is common when testing the effect of an intervention - and that’s exactly what we’ll do here. This mock dataset is also a bit more complex than the previous ones we’ve looked at just to give the full range of assumption testing and modelling that we have to do. 9.7.2 Example Elaine and Chaise ran an RCT testing the effect of a music listening intervention on anxiety scores. To do this, participants were randomly allocated to three conditions: a music listening intervention (listen to a podcast for 30 minutes a day), a control exercise intervention (walk for 30 minutes a day) and a control nothing intervention (do nothing). Participants were tested at three timepoints: at the start of the study, at the 6 week mark and then at the 12 week mark. (With thanks to the datarium package for providing a perfect test dataset for this example.) In other words, we have two variables: Intervention: a between-groups variable, because participants were randomly assigned to one of three interventions (3 levels; Control, Exercise, Music) Timepoint, a within-groups variable as all participants were measured at 3 timepoints (3 levels: 0 weeks, 6 weeks, 12 weeks) This gives us a two-way, 3 x 3 mixed factorial ANOVA. Let’s start by visualising anxiety scores for the three interventions. A boxplot or bar graph is useful here. twoway_mixed &lt;- read_csv(here(&quot;data&quot;, &quot;factorial&quot;, &quot;twoway_mixed.csv&quot;)) ## Rows: 135 Columns: 4 ## ── Column specification ──────────────────── ## Delimiter: &quot;,&quot; ## chr (2): group, time ## dbl (2): id, anxiety ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # This recodes factors twoway_mixed$group &lt;- factor(twoway_mixed$group) twoway_mixed$time &lt;- factor(twoway_mixed$time) twoway_mixed %&gt;% ggplot( aes(x = time, y = anxiety, colour = group) ) + geom_boxplot(size = 1) + # scale_x_discrete(labels = c(&quot;0 weeks&quot;, &quot;6 weeks&quot;, &quot;12 weeks&quot;)) + labs( x = &quot;Time point&quot;, y = &quot;Anxiety score&quot;, colour = &quot;Intervention\\ngroup&quot; ) + scale_colour_brewer(palette = &quot;Set1&quot;) + theme(legend.position = &quot;right&quot;) In a mixed ANOVA, we test the effect of both a between-groups and a within-groups variable. This means that the specific assumptions that apply to both between- and within-subject designs now carry over into this design. This means that the following assumptions apply: The data should have multivariate normality. Our QQ plot looks a little non-linear, so we might have an issue here. aov(anxiety ~ group * time, data = twoway_mixed) %&gt;% broom::augment() %&gt;% ggplot(aes(sample = .std.resid)) + geom_qq() + geom_qq_line() Homogeneity of variance - the between-subject groups should have the same variance, at each level of the within-subjects variable. None of the tests are significant (p &gt; .05), so we’re ok here. To do this in R, we run Levene’s test at each timepoint: twoway_mixed %&gt;% group_by(time) %&gt;% levene_test(anxiety ~ group, center = &quot;mean&quot;) Sphericity of the within-subject variable must be met. Our sphericity assumption technically isn’t violated (p = .079), but this is so close to an arbitrary threshold that we might consider reporting corrected versions anyway. However, we’ll forge ahead with our original omnibus model and report that. aov_ez() can give you this in the output below. 9.7.3 Output Below is our main output from R. You’ll note that the omnibus outputs are actually split across two tables - one for the within-subject effects, and another for the between-subject effects. This simply makes it easy to identify which variables are what types. Based on the output, we can see that we have a significant main effect of group (F(2, 42) = 4.352, p = .019), and also a significant main effect of time (F(2, 84) = 394.909, p &lt; .001). We also have a significant interaction (F(4, 84) = 110.188, p &lt; .001). twoway_mixed_aov &lt;- aov_ez( data = twoway_mixed, id = &quot;id&quot;, dv = &quot;anxiety&quot;, between = &quot;group&quot;, within = &quot;time&quot;, anova_table = list(es = &quot;pes&quot;) ) ## Contrasts set to contr.sum for the following variables: group twoway_mixed_aov ## Anova Table (Type 3 tests) ## ## Response: anxiety ## Effect df MSE F pes p.value ## 1 group 2, 42 7.12 4.35 * .172 .019 ## 2 time 1.79, 75.24 0.09 394.91 *** .904 &lt;.001 ## 3 group:time 3.58, 75.24 0.09 110.19 *** .840 &lt;.001 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 ## ## Sphericity correction method: GG # To get details on sphericity summary(twoway_mixed_aov) ## ## Univariate Type III Repeated-Measures ANOVA Assuming Sphericity ## ## Sum Sq num Df Error SS den Df F value Pr(&gt;F) ## (Intercept) 34919 1 299.146 42 4902.6660 &lt; 2e-16 *** ## group 62 2 299.146 42 4.3518 0.01916 * ## time 67 2 7.081 84 394.9095 &lt; 2e-16 *** ## group:time 37 4 7.081 84 110.1876 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Mauchly Tests for Sphericity ## ## Test statistic p-value ## time 0.88364 0.079193 ## group:time 0.88364 0.079193 ## ## ## Greenhouse-Geisser and Huynh-Feldt Corrections ## for Departure from Sphericity ## ## GG eps Pr(&gt;F[GG]) ## time 0.89577 &lt; 2.2e-16 *** ## group:time 0.89577 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## HF eps Pr(&gt;F[HF]) ## time 0.9330916 1.037156e-40 ## group:time 0.9330916 1.461019e-30 Let’s decompose this with simple effects. Doing this in Jamovi is not trivial at all for some reason - you have to really get your hands dirty with Jamovi in order to make this work. There are two main ways you can do this: Run a one-way ANOVA at each timepoint - this would be the simple effect of group emmeans(twoway_mixed_aov, ~ group, by = &quot;time&quot;) %&gt;% pairs(infer = TRUE, adjust = &quot;none&quot;) ## time = X0.weeks: ## contrast estimate SE df lower.CL upper.CL t.ratio p.value ## control - exercise 0.4400 0.552 42 -0.673 1.553 0.797 0.4297 ## control - music 0.0733 0.552 42 -1.040 1.187 0.133 0.8949 ## exercise - music -0.3667 0.552 42 -1.480 0.747 -0.665 0.5100 ## ## time = X12.weeks: ## contrast estimate SE df lower.CL upper.CL t.ratio p.value ## control - exercise 0.9800 0.571 42 -0.173 2.133 1.715 0.0937 ## control - music 2.9467 0.571 42 1.794 4.100 5.157 &lt;.0001 ## exercise - music 1.9667 0.571 42 0.814 3.120 3.442 0.0013 ## ## time = X6.weeks: ## contrast estimate SE df lower.CL upper.CL t.ratio p.value ## control - exercise 0.4600 0.584 42 -0.719 1.639 0.787 0.4355 ## control - music 1.9133 0.584 42 0.734 3.092 3.275 0.0021 ## exercise - music 1.4533 0.584 42 0.274 2.632 2.488 0.0169 ## ## Confidence level used: 0.95 Run a one-way repeated measures ANOVA for each group - this would be the simple effect of time emmeans(twoway_mixed_aov, ~ time, by = &quot;group&quot;) %&gt;% pairs(infer = TRUE, adjust = &quot;none&quot;) ## group = control: ## contrast estimate SE df lower.CL upper.CL t.ratio p.value ## X0.weeks - X12.weeks 0.58 0.1230 42 0.3324 0.828 4.727 &lt;.0001 ## X0.weeks - X6.weeks 0.16 0.0950 42 -0.0316 0.352 1.685 0.0994 ## X12.weeks - X6.weeks -0.42 0.0982 42 -0.6182 -0.222 -4.276 0.0001 ## ## group = exercise: ## contrast estimate SE df lower.CL upper.CL t.ratio p.value ## X0.weeks - X12.weeks 1.12 0.1230 42 0.8724 1.368 9.128 &lt;.0001 ## X0.weeks - X6.weeks 0.18 0.0950 42 -0.0116 0.372 1.896 0.0649 ## X12.weeks - X6.weeks -0.94 0.0982 42 -1.1382 -0.742 -9.571 &lt;.0001 ## ## group = music: ## contrast estimate SE df lower.CL upper.CL t.ratio p.value ## X0.weeks - X12.weeks 3.45 0.1230 42 3.2057 3.701 28.144 &lt;.0001 ## X0.weeks - X6.weeks 2.00 0.0950 42 1.8084 2.192 21.063 &lt;.0001 ## X12.weeks - X6.weeks -1.45 0.0982 42 -1.6515 -1.255 -14.797 &lt;.0001 ## ## Confidence level used: 0.95 Let’s bulletpoint the main feature of each, and you can refer to the output below as you go through: For controls, anxiety scores were not significantly different between the 0 week and 6 week mark (p = .065). However, anxiety scores were significantly lower at 12 weeks compared to 6 weeks (p = .004). Scores were also significantly lower at 12 weeks compared to 0 weeks (p = .002). For the exercise group, anxiety scores were not significantly different from 0 weeks to 6 weeks (p = .089). However, anxiety scores significantly decreased from 6 weeks to 12 weeks (p &lt; .001). Scores were also significantly lower after 12 weeks than at 0 weeks (p &lt; .001). For the music group, anxiety scores significantly decreased between 0 weeks and 6 weeks (p &lt; .001). Scores were significantly lower at the 12 week mark again compared to the 6 week mark (p &lt; .001). Scores were also significantly lower at 12 weeks compared to 0 weeks (p &lt; .001). These comparisons are best supplemented with the differences in means. "],["three-way-anova.html", "9.8 Three-way ANOVA", " 9.8 Three-way ANOVA 9.8.1 Introduction Of course, we don’t have to stop at two IVs. Sometimes we may have designs where we want to At this point though, I want to ask you… why are you going to that complex of a design? To illustrate, consider a three-way ANOVA with variables A, B and C. In line with the principle of marginality, an ANOVA with all possible interactions will have: 3 main effects: A, B and C 3 simple (two-way) interaction effects: AxB, AxC, BxC 1 three-way interaction: AxBxC This is not a trivial thing to interpret! A significant three-way interaction means that e.g. the main effect of variable A depends on both variable B and variable C. This quickly becomes really difficult to interpret, particularly when one or more variables have multiple levels. What is the effect of A when B = 2 and C = 4, for instance? What about A when B = 2 and C = 3? etc etc. Nevertheless, three-way ANOVAs (and even beyond that) are not necessarily uncommon, so it doesn’t hurt to know about them. We’ll unpack the main concepts as we step through an example, for the sake of brevity. 9.8.2 Example This dataset is from a fictional experiment looking at how gender (male and female), risk of migraine (low or high) and different treatments (labelled X, Y and Z) impacted pain scores associated with a migraine headache. Below is a snippet of what our data looks like: library(datarium) data(headache) head(headache) We want to run a gender (2) x risk (2) x treatment (3) three-way ANOVA to see whether these predictors have an effect on overall pain scores. Let’s start by visualising this data. We cannot fully visualise a three-way interaction because we inherently need four dimensions (as we have three predictors and one outcome variable). The best way to approach this is to draw a series of two-way interaction plots, split by the third variable. In the example below, the graphs show treatment on the x-axis, pain scores on the y-axis, different lines for low and high risk and separate graphs for male and female participants: ## `summarise()` has grouped output by ## &#39;gender&#39;, &#39;risk&#39;. You can override using ## the `.groups` argument. 9.8.3 Simple simple effects Consider a scenario where a three-way interaction is significant. Naturally, as we saw in the examples with two-way ANOVAs, we now have to turn to breaking down the lower-order terms to unpack this interaction. For a two-way ANOVA, this was easy - we simply interpreted the effect of variable A at each level of variable B, and vice versa. We called this a simple effects test. But now, we need to break down a three-way interaction by looking at every two-way interaction that is now significant. The ‘simple effect’ is now each two-way interaction, hence why the terminology of simple interaction effect/test is sometimes used. That, of course, means that if the two-way is significant, we need to run the next level of simple effects for each main effect. At this level, we call them simple simple main effects. That’s because we’re now looking at main effects that are simple effects of a simple interaction effect, which is itself a simple effect of a three-way interaction. If that didn’t make sense, consider running an alternate analysis to a three-way ANOVA. It helps to visualise this, so I’ll use the headache data to do so. Imagine that the three-way interaction between treatment, risk and gender is significant. Imagine that every other effect is also significant (we’ll test this further down). This would give us three sets of simple interactions to test: Treatment x risk, for each gender Treatment x gender, for each risk level Gender x risk, for each treatment Here is the first interaction visualised - treatment and risk, with separate plots for males and females: ## `summarise()` has grouped output by ## &#39;gender&#39;, &#39;risk&#39;. You can override using ## the `.groups` argument. Here’s the second, which plots treatment x gender for each risk level: And finally, here is gender and risk by each treatment: And then within that, you would also want the main effects. 9.8.4 Assumptions and output Let’s run our basic ANOVA. Just like in Jamovi, we’ll do our usual assumption checks for our ANOVA model too. Based on Levene’s test and the Shapiro-Wilks test, we have no violations in multivariate equality of variance (p = .994) or normality (p = .398). headache %&gt;% levene_test(pain_score ~ gender * risk * treatment, center = &quot;mean&quot;) threeway_aov &lt;- aov_ez( data = headache, id = &quot;id&quot;, dv = &quot;pain_score&quot;, between = c(&quot;gender&quot;, &quot;risk&quot;, &quot;treatment&quot;), anova_table = list(es = &quot;pes&quot;), include_aov = TRUE ) ## Contrasts set to contr.sum for the following variables: gender, risk, treatment shapiro.test(threeway_aov$aov$residuals) ## ## Shapiro-Wilk normality test ## ## data: threeway_aov$aov$residuals ## W = 0.98212, p-value = 0.3981 threeway_aov ## Anova Table (Type 3 tests) ## ## Response: pain_score ## Effect df MSE F pes p.value ## 1 gender 1, 60 19.35 16.20 *** .213 &lt;.001 ## 2 risk 1, 60 19.35 92.70 *** .607 &lt;.001 ## 3 treatment 2, 60 19.35 7.32 ** .196 .001 ## 4 gender:risk 1, 60 19.35 0.14 .002 .708 ## 5 gender:treatment 2, 60 19.35 3.34 * .100 .042 ## 6 risk:treatment 2, 60 19.35 0.71 .023 .494 ## 7 gender:risk:treatment 2, 60 19.35 7.41 ** .198 .001 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 So what do our results tell us? Well: We have significant main effects of gender (F(1, 60) = 16.20, p &lt; .001), risk group (F(1, 60) = 92.69, p &lt; .001) and treatment (F(2, 60) = 7.32, p = .001). We have a significant two-way gender x treatment interaction (F(2, 60) = 3.34, p = .04), but no significant interactions between gender x risk (p = .708) and risk x treatment (p = .494). Our highest-order interaction, gender x risk x treatment is significant (F(2, 60) = 7.41, p = .001). Great, so we know that we’re dealing with a three-way interaction. But… how do we break that down? As in a two-way, we now need to break down simple main effects and simple interaction effects. In a two-way ANOVA, we broke down the two-way interaction by essentially running tests on the main effects (hence, simple main effects). In a three-way ANOVA, we attempt to start by testing for interactions between A and B, at each level of C. Think of it like running multiple two-way ANOVAs. In our case, let’s look at whether risk and treatment interact, for men and women separately. For simplicity, I’m defaulting to using rstatix’s anova_test() here, but an alternative approach is to filer the data by men and women and use these smaller datasets as calls to aov_ez. headache %&gt;% group_by(gender) %&gt;% anova_test(pain_score ~ treatment * risk, type = 3) Here, the risk x treatment interaction is significant for men (p = .016). However, for women the interaction is not quite significant (p = .054). Let’s break this down for the men by doing simple simple effects. Following the principle of starting with the highest-order factor first, we now need to decompose the two-way risk x treatment interaction for men. Is there a difference between treatments when risk is low? Here’s a graph to visualise what comparisons we’re looking at: Here’s one way to run the simple simple effets of treatment (with Holm adjustments by default). When specifying the emmeans() call, note that we use the same interaction terminology emmeans(threeway_aov, ~ treatment * gender * risk) %&gt;% pairs(infer = TRUE, adjust = &quot;holm&quot;, simple = &quot;gender&quot;) ## treatment = X, risk = high: ## contrast estimate SE df lower.CL upper.CL t.ratio p.value ## male - female 13.87 2.54 60 8.794 18.95 5.463 &lt;.0001 ## ## treatment = Y, risk = high: ## contrast estimate SE df lower.CL upper.CL t.ratio p.value ## male - female 1.17 2.54 60 -3.914 6.25 0.459 0.6477 ## ## treatment = Z, risk = high: ## contrast estimate SE df lower.CL upper.CL t.ratio p.value ## male - female -1.35 2.54 60 -6.434 3.73 -0.533 0.5958 ## ## treatment = X, risk = low: ## contrast estimate SE df lower.CL upper.CL t.ratio p.value ## male - female 1.90 2.54 60 -3.184 6.98 0.746 0.4583 ## ## treatment = Y, risk = low: ## contrast estimate SE df lower.CL upper.CL t.ratio p.value ## male - female 4.78 2.54 60 -0.303 9.86 1.881 0.0648 ## ## treatment = Z, risk = low: ## contrast estimate SE df lower.CL upper.CL t.ratio p.value ## male - female 4.68 2.54 60 -0.404 9.76 1.841 0.0705 ## ## Confidence level used: 0.95 From this we can infer the following: - For high risk men, there was a significant difference in treatment between X and Y (p = .004), a significant difference between X and Z (p = .001) but no difference between X and Z (p = .347) - No significant difference between treatments for low risk men "],["planned-comparisons.html", "9.9 Planned comparisons", " 9.9 Planned comparisons 9.9.1 Introduction Up until now, the typical process for testing an ANOVA has been something like this: Determine null and alternative hypotheses Run an omnibus ANOVA Run post-hocs to unpack any significant effects The last point here is of interest to us. Recall that the omnibus ANOVA generally tells us that there is a difference between the means somewhere in our potential list of comparisons. We don’t know where that difference may be ahead of time. Post-hoc tests like Tukey tests allow us to ‘unpack’ these significant differences further. However, post-hocs by definition are a-posteriori - they are only done after a significant result/test, meaning that they are strictly exploratory. Sometimes, however, we may actually have a-priori (i.e. before the test) hypotheses about specific comparisons we want to make. Often, these are specific predictions based on literature that we want to test in our own data. In these instances, we now move into the possibility of doing planned comparisons. 9.9.2 Planned comparisons As mentioned above, we use planned comparisons when we have specific hypotheses we want to test. For instance, you may want to compare two specific groups out of four for a hypothetical reason. Or, alternatively, you may wish to compare one group against all of the others at once. These kinds of scenarios can be vital for testing specific hypotheses. The benefit of planned comparisons is twofold. Firstly, good planned comparisons should be based in either existing theory or on specific hypotheses, meaning that you are generally aiming to test a specific effect for (hopefully) a scientifically sound reason. Secondly, planned comparisons reduce the number of comparisons that need to be made, thereby reducing the overall family-wise Type I error rate. In essence, we only conduct the tests required to evaluate our original research question(s). 9.9.3 Linear contrasts In planned comparisons, we typically compare two specific means with each other. With that in mind, it may be tempting to simply default to running a t-test or something similar to compare just the two groups you want. However, this would not properly test the planned comparison, because in some instances we must average over the other groups in the comparisons. To correctly set up planned comparisons, we must set up linear contrasts. In essence, these allow to specify how the various groups in an ANOVA should be compared. Here’s an intuitive way of thinking about these contrasts. Imagine we want to compare one superstar soccer player (e.g. maybe Messi) to a given football team of 15. To make the comparison fair, we would weight each person in the team of 15 (i.e. make each person’s average worth 1/15), in order for the team’s average to be comparable to the superstar player. In essence, this is what we do when defining linear contrasts, and we do this via defining a contrast matrix. This contrast matrix is a way that lets us weight the categories in a variable to create contrasts. While there are several forms of standard contrasts (which is slightly beyond the scope, but here is a good overview. We assign a positive or negative number to each level in a variable in order to define a contrast. Here, the direction (positive vs negative) matters; any value with a positive contrast coefficient will be on one side of the comparison, and any value with a negative coefficient will be on the other side of the comparison. The linear contrasts must sum to zero, meaning that the total on each side must be the same. Below are three examples of specific contrasts, and their coefficients: a b c d Group A Group B Group C Group A - Group B 1 -1 0 Group B - Group C 0 1 -1 Group A - (Group B and C) 2 -1 -1 It doesn’t really matter what numbers you use to enter these contrasts, so long as they sum to zero. For example, we could also write the first contrast using 0.5 and -0.5 in place of the 1s. 9.9.4 An example Let’s revisit the one-way ANOVA we ran in 9.5 One way ANOVA: Below is another simple example, comparing taste ratings across three different types of slices: caramel slices, vanilla slices and lemon slices. Participants were randomly allocated to taste one of the three slices blindfolded, and were then asked to verbally rate its taste on a scale from 1-10 (10 being super tasty). Now, pretend that this time we have a specific hypothesis we want to test. Namely, pretend that we are only interested in two comparisons: Vanilla - caramel slices Vanilla - caramel and lemon slices Here’s the original ANOVA output for your reference, though we don’t need to do anything with this: ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group 2 22.22 11.111 6.897 0.00201 ** ## Residuals 60 96.67 1.611 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We need to define the contrast coefficients for our two planned contrasts. In R, there are two ways you can do this: either define contrasts at the aov level, or use emmeans. We will do the latter. a b c d Caramel Lemon Vanilla Caramel - Vanilla 1 0 -1 Vanilla - (Caramel and Lemon) -1 -1 2 To set up the contrasts, we first need to know the exact order of our group variable/ # IDentify levels levels(w9_slices$group) ## [1] &quot;caramel&quot; &quot;lemon&quot; &quot;vanilla&quot; Once we know that, we need to create new variables that will contain the contrasts. To do this, we will create a vector of length k, where k is the total number of groups in the independent/categorical variable. In this case, becasue we have three slice flavours, we need to create vectors of length 3. The value of each vector item corresponds to the level of the factor. For emmeans, all we need to do is use a 1 for the group of interest, and 0 of others; emmeans will build the correct contrast coefficients later depending on what we give it. # Define contrasts lemon &lt;- c(0, 1, 0) caramel &lt;- c(1, 0, 0) vanilla &lt;- c(0, 0, 1) Finally, we can perform our contrasts. We start with the same call to emmeans(), so we must first give it our aov`/ANOVA object and indicate our grouping variable. emmeans(w9_slices_aov, ~ group) ## group emmean SE df lower.CL upper.CL ## caramel 5.62 0.277 60 5.06 6.17 ## lemon 5.14 0.277 60 4.59 5.70 ## vanilla 6.57 0.277 60 6.02 7.13 ## ## Confidence level used: 0.95 Next, we use the contrast() function to perform the pairwise comparisons. (pairs(), which we saw earlier, is just a shorthand form of contrast().) Here is where we give the function the speicifc contrasts that we want to run. We do this by supplying the methods argument in contrast(). This argument takes a list of comparisons we want to run, expressed in A - B form (where A and B are the variables of contrast coefficients representing the groups we want to compare). For example, if we want to compare our caramel and vanilla slices, we simply need to give caramel - vanilla - both of which we defined just above - to a list in contrast(). This will generate the following set of output. emmeans(w9_slices_aov, ~ group) %&gt;% contrast( method = list( caramel - vanilla ) ) ## contrast estimate SE df t.ratio p.value ## c(1, 0, -1) -0.952 0.392 60 -2.431 0.0180 The output is a little hard to read through, but we can fix that by assigning a name to the list item: emmeans(w9_slices_aov, ~ group) %&gt;% contrast( method = list( &quot;Caramel - vanilla&quot; = caramel - vanilla ) ) ## contrast estimate SE df t.ratio p.value ## Caramel - vanilla -0.952 0.392 60 -2.431 0.0180 Unlike Jamovi and gamlj, which can only display one given comparison at a time, emmeans() can display multiple. Here, for example, is how we would define our second contrast - that is, vanilla vs the other two slices: list(&quot;Vanilla vs others&quot; = vanilla - (caramel + lemon)/2 Note here that we take the average of ‘caramel’ and ‘lemon’, by adding them up and dividing by 2. This will have the same effect of making their contrast coefficients half the weight of the vanilla level (i.e. it does the maths of calculating the correct contrast coefficient so that it sums to zero). Instead of making this a whole new emmeans() call, we can simply add that list term to the list() part of our code in the main call. emmeans(w9_slices_aov, ~ group) %&gt;% contrast( method = list( &quot;Caramel - vanilla&quot; = caramel - vanilla, &quot;Vanilla - others&quot; = vanilla - (lemon + caramel)/2 ) ) ## contrast estimate SE df t.ratio p.value ## Caramel - vanilla -0.952 0.392 60 -2.431 0.0180 ## Vanilla - others 1.190 0.339 60 3.509 0.0009 Using this output, we can see that the comparison between caramel slices and vanilla slices is significant (t = 2.43, p = .018). Likewise, the comparison between the vanilla slices and the other slices is significant (t = 3.51, p &lt; .001). "],["multiple-regression-continued.html", "Chapter 10 Multiple regression continued", " Chapter 10 Multiple regression continued This section deals with some more advanced topics in ANOVAs and regression. It serves as a continuation to the first chapter on regression, and in particular focuses on multiple regressions. This chapter will cover the following: ANCOVA Hierarchical regressions Model selection This module won’t cover continuous interactions because they are often considered under the topic of moderation - for which there will be a separate module. "],["revisiting-multiple-regression.html", "10.1 Revisiting multiple regression", " 10.1 Revisiting multiple regression Recall that the basic multiple regression looks something like this: \\[ y = \\beta_0 + \\beta_1x_1 +\\beta_2x_2 + \\epsilon_i \\] As a reminder, the coefficients in this formula correspond to the following: \\(\\beta_1\\) is the coefficient for predictor \\(x_1\\); i.e. as \\(x_1\\) increases by 1 unit, \\(\\hat y\\) (the predicted y value) increases by \\(\\beta_1\\) units, assuming \\(x_2\\) does not change \\(\\beta_2\\) is the coefficient for predictor \\(x_2\\), and describes how \\(\\hat y\\) changes assuming \\(x_1\\) does not change \\(\\epsilon_i\\) is the error term, which we assume is normally distributed We can expand this formula out to include \\(n\\) predictors, as follows: \\[ y = \\beta_0 + \\beta_1x_1 +\\beta_2x_2 + ... \\beta_nx_n + \\epsilon_i \\] "],["ancovas.html", "10.2 ANCOVAs", " 10.2 ANCOVAs 10.2.1 Introduction ANCOVA stands for Analysis of Co-variance. Its basic definition is that it is an ANOVA, but with the inclusion of a covariate (or a variable we need to control for). The basic idea is that we are performing an ANOVA between our predictors and outcomes after adjusting our predictors (our model) by another variable. Just like the ANOVA, ANCOVA is a fairly generic term that can refer to a multitude of analyses. In this book we’ll stick with ANOVAs relating to between-subjects predictors only. 10.2.2 Example The following data comes from Plaster (1989). The dataset is described as below: Male participants were shown a picture of one of three young women. Pilot work had indicated that the one woman was beautiful, another of average physical attractiveness, and the third unattractive. Participants rated the woman they saw on each of twelve attributes. These measures were used to check on the manipulation by the photo. Then the participants were told that the person in the photo had committed a Crime, and asked to rate the seriousness of the crime and recommend a prison sentence, in Years. Our main questions are: Does the perceived attractiveness of the “defendant” (the women in the photo) influence the number of years the mock juror (the participant) sentence them for? How does this relationship change after controlling for the perceived seriousness of the crime? Our dataset, jury_data, contains the following variables: Attr: The perceived attractiveness of the defendant (Beautiful, Average, Unattractive) Crime: The crime that was commited by the defendant (Burglary, Swindle) Serious: The perceived seriousness of the crime from 1-10 Years: The number of years the participant sentenced the defendant for 10.2.3 One-way ANCOVA To start us off, let’s begin with a simple one-way ANOVA between attractiveness and years. We can see this below. jury_data &lt;- read_csv(here(&quot;data&quot;, &quot;regression_2&quot;, &quot;anova_mockjury.csv&quot;)) jury_aov &lt;- aov(Years ~ Attr, data = jury_data) summary(jury_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Attr 2 70.9 35.47 2.77 0.067 . ## Residuals 111 1421.3 12.80 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 eta_squared(jury_aov, alternative = &quot;two.sided&quot;) ## For one-way between subjects designs, partial eta squared is equivalent ## to eta squared. Returning eta squared. From this we can infer that the effect of attractiveness is not significant (F(2, 111) = 2.77, p = .067, \\(\\eta^2\\) = .05, 95% CI = [0, .14]). In other words, perceived attractiveness does not appear to relate to the years of sentencing. Let’s now add the covariate Serious in. To do this in R, we simply add in the predictor to the aov model just like we would with adding a second predictor in a multiple regression - by specifying IV + covariate within the relevant function. See below for two ways of running an ANCOVA: # One way using car::Anova - this is useful for getting the correct eta squared confidence intervals options(contrasts = c(&quot;contr.helmert&quot;, &quot;contr.poly&quot;)) jury_acov &lt;- aov(Years ~ Attr + Serious, data = jury_data) jury_acov_sum &lt;- car::Anova(jury_acov, type = 3) jury_acov_sum eta_squared(jury_acov_sum, alternative = &quot;two.sided&quot;) # Another way using rstatix jury_data %&gt;% anova_test(Years ~ Attr + Serious, effect.size = &quot;pes&quot;, type = 3) What do we see? Well, the main effect of attractiveness is now significant (F(2, 110) = 3.87, p = .024, \\(\\eta^2_p\\) = .07, 95% CI = [0, .16]). The effect of the covariate is also significant (F(1, 110) = 40.31, p &lt; .001, \\(\\eta^2_p\\) = .27, 95% CI = [.14, .39]). What’s actually going on here? Well, the first model showed us that by itself, attractiveness was not a significant part of the model. However, once we factored in the effect of Seriousness (and more importantly, controlled for it) we saw that Attractive did in fact relate to the sentence length. Unsurprisingly, the seriousness of the crime also predicted sentence length. 10.2.4 Assumptions By and large, the assumptions required for an ANCOVA are the same as that of a regular ANOVA. However, there are two new ones in bold below: Normality of residuals Homogeneity of variances Linearity of the covariate Homogeneity of regression slopes Let’s check each of these below. First, the normality of residuals can simply be tested as in the usual way. Our residuals are not normally distributed in this model (W = .97, p = .006). shapiro.test(jury_acov$residuals) ## ## Shapiro-Wilk normality test ## ## data: jury_acov$residuals ## W = 0.96616, p-value = 0.005529 The homoegeneity of variance assumption is also largely tested in the same way. Our homogeneity of variance assumption also isn’t met (F(2, 111) = 5.68, p = .004)… jury_data %&gt;% levene_test(Years ~ Attr, center = &quot;mean&quot;) ## Warning in leveneTest.default(y = y, group = group, ...): group coerced to ## factor. 10.2.5 Linearity of the covariate A third assumption tests whether the covariate is linearly related to the DV. This assumption is essentially similar to the linearity assumption in a regression model - because we are still dealing with linear models, our covariates must also be linearly related to our outcome. This is simple enough to test just by visualising the relationship. In general, this assumption appears to hold - it looks like there’s a vague linear relationship in there. (Note that due to the data being in integers - i.e. whole numbers - I’ve used geom_jitter() in place of geom_point() to help visualise this a bit better.) jury_data %&gt;% ggplot( aes(x = Serious, y = Years) ) + geom_jitter() + labs(x = &quot;Perceived seriousness of crime&quot;, y = &quot;Sentence length (Years)&quot;) Finally, the homogeneity of regression slopes assumption specifies that for each group, the slope of the relationship between the covariate and the dependent variable are the same. To test this, we need to run an ANOVA that allows for an interaction between the predictor and covariate. jury_data %&gt;% anova_test(Years ~ Attr * Serious, effect.size = &quot;pes&quot;, type = 3) Uh oh - this isn’t good. A significant interaction suggests that the slope of the relationship between Serious and Years differs for each level of attractiveness, as indicated by the significant interaction effect (p = .03). We can see as much if we fit separate regression lines to the scatterplot above: jury_data %&gt;% ggplot( aes(x = Serious, y = Years, colour = Attr) ) + geom_jitter() + labs(x = &quot;Perceived seriousness of crime&quot;, y = &quot;Sentence length (Years)&quot;) + geom_smooth(method = lm, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; As we can clearly see, the slopes are not identical for each group. In particular, the Unattractive group has a much stronger slope between seriousness and sentence length (indicating that unattractive people basically have it harder if they’re perceived to have committed more serious crimes). Overall, given that many of our assumptions are not met - particularly the important one of homogeneity of regression slopes - this indicates that an ANCOVA isn’t a suitable model for our data. What would we do in this instance, then? We’d probably model a regression that allows for the interaction between attractiveness and seriousness. A final note on ANCOVAs: naturally, we can extend an ANCOVA model to have multiple predictors and multiple covariates. In this instance, we would need to model multi-way interactions to test all of our effects and assumptions. Below is a lightly annotated example of a two-way ANCOVA using attractiveness and type of crime as predictors, seriousness as a covariate and sentence length as an outcome. # Build two way ANCOVA jury_twoway_acov &lt;- aov(Years ~ Attr * Crime + Serious, data = jury_data) # Normality of residuals shapiro.test(jury_twoway_acov$residuals) ## ## Shapiro-Wilk normality test ## ## data: jury_twoway_acov$residuals ## W = 0.96895, p-value = 0.009416 # Homogeneity of variance jury_data %&gt;% levene_test(Years ~ Attr * Crime, center = &quot;mean&quot;) # Linearity of covariate + homogeneity of regression slopes jury_data %&gt;% ggplot( aes(x = Serious, y = Years, colour = Attr) ) + geom_jitter() + labs(x = &quot;Perceived seriousness of crime&quot;, y = &quot;Sentence length (Years)&quot;) + geom_smooth(method = lm, se = FALSE) + facet_wrap(~Crime) ## `geom_smooth()` using formula = &#39;y ~ x&#39; jury_data %&gt;% anova_test(Years ~ Attr * Crime * Serious, effect.size = &quot;pes&quot;) # Output ANCOVA Anova(jury_twoway_acov, type = 3) "],["hierarchical-regression.html", "10.3 Hierarchical regression", " 10.3 Hierarchical regression Hierarchical regression is a form of multiple regression where we test the effects of predictors in blocks. The aim of doing a hierarchical regression is generally to test theoretical predictions about the effects of specific variables, especially before/after we control for other variables. The other aim is to explore how the model changes after we add additional predictors into the model. The basic principle of a hierarchical regression is something like this: Start by defining block 1, which is our basic regression model. This is the regression we start with. Run the regression defined in block 1. Identify which variables will be entered into block 2, which is the first round of additional predictors Run a second multiple regression with all predictors in block 2. Compare block 1 with block 2 in terms of overall model fit. The choice of what variables to enter in which blocks must be guided by theory - in other words, you cannot simply add variables at random. 10.3.1 Example Let’s return to the proneness to flow example introduced in the multiple regression section. As a reminder, here are our variables: Trait anxiety: broadly, refers to people’s tendency to feel anxious Openness to experience: a personality trait that describes how likely people are to seek new experiences DFS_Total: a measure of proneness to flow. age: participant’s age. w10_flow &lt;- read_csv(here(&quot;data&quot;, &quot;week_10&quot;, &quot;w10_flow.csv&quot;)) ## Rows: 811 Columns: 6 ## ── Column specification ──────────────────── ## Delimiter: &quot;,&quot; ## dbl (6): id, age, GoldMSI, DFS_Total, trait_anxiety, openness ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. In the first regressions module, we simply ran everything in one go as a multiple regression. Now let’s imagine we want to run this as a hierarchical regression, with the following blocks: Block 1: GOld MSI predicting proneness to flow (DFS_Total) Block 2: Gold MSI and openness predicting proneness to flow Block 3: Gold MSI, openness and trait anxiety predicting proneness to flow The assumption tests in multiple regressions are identical for hierarchical regressions. 10.3.2 Building blocks and output Let’s start by building block 1. We can do this with lm() as per normal. I will call this flow_block1: flow_block1 &lt;- lm(DFS_Total ~ GoldMSI, data = w10_flow) To build block 2, we simply need to create a new regression model with both predictors, as if we were running this in one go: flow_block2 &lt;- lm(DFS_Total ~ GoldMSI + openness, data = w10_flow) Finally, we do the same thing for block 3: flow_block3 &lt;- lm(DFS_Total ~ GoldMSI + openness + trait_anxiety, data = w10_flow) Now let’s print the summary of each model. We can see in block 1 that Gold MSI scores significantly predict proneness to flow: summary(flow_block1) ## ## Call: ## lm(formula = DFS_Total ~ GoldMSI, data = w10_flow) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.1367 -2.4567 0.0448 2.2783 12.2886 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 16.3429 1.0848 15.06 &lt;2e-16 *** ## GoldMSI 2.9231 0.1983 14.74 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.538 on 809 degrees of freedom ## Multiple R-squared: 0.2118, Adjusted R-squared: 0.2108 ## F-statistic: 217.4 on 1 and 809 DF, p-value: &lt; 2.2e-16 In Block 2, both the Gold MSI and openness are significant predictors of flow proneness. summary(flow_block2) ## ## Call: ## lm(formula = DFS_Total ~ GoldMSI + openness, data = w10_flow) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.3099 -2.3925 0.0643 2.2613 11.6213 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.6287 1.1912 12.281 &lt; 2e-16 *** ## GoldMSI 2.7179 0.2061 13.185 &lt; 2e-16 *** ## openness 0.4818 0.1425 3.382 0.000755 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.515 on 808 degrees of freedom ## Multiple R-squared: 0.2228, Adjusted R-squared: 0.2209 ## F-statistic: 115.8 on 2 and 808 DF, p-value: &lt; 2.2e-16 Finally, in block 3 we can see that all three remain significant predictors. However, the effect of openness to experience has changed slightly (an unreliable heuristic for this is that the p-value has increased): summary(flow_block3) ## ## Call: ## lm(formula = DFS_Total ~ GoldMSI + openness + trait_anxiety, ## data = w10_flow) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.0424 -2.2409 -0.0931 2.1484 12.3474 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 21.08246 1.33150 15.834 &lt;2e-16 *** ## GoldMSI 2.66545 0.19623 13.583 &lt;2e-16 *** ## openness 0.29958 0.13700 2.187 0.0291 * ## trait_anxiety -0.10662 0.01154 -9.237 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.345 on 807 degrees of freedom ## Multiple R-squared: 0.2971, Adjusted R-squared: 0.2945 ## F-statistic: 113.7 on 3 and 807 DF, p-value: &lt; 2.2e-16 On the next page we’ll talk about model comparison in a more formal manner. However, if we wanted to write these results up we would need to talk about the results from each block. For example: A hierarchical regression was conducted to examine the effect "],["comparing-models.html", "10.4 Comparing models", " 10.4 Comparing models On the previous page, we ended up with three models relating to the flow data. That’s all well and good, and seeing how each model changed the predictors was valuable in its own right. But how do we actually… decide which model to run with? 10.4.1 Comparing \\(R^2\\) The most commonly cited method of comparing between regression models is to examine their \\(R^2\\) values, which you may recall is a measure of how much variance in the outcome is explained by the predictors. This is easy enough to do visually. You can see the \\(R^2\\) values in the output. We can extract this easily using the following code. Whenever we use summary() on an lm() model, the summary object will contain a variable for the \\(R^2\\) that we can easily pull: summary(flow_block1)$r.squared ## [1] 0.211779 summary(flow_block2)$r.squared ## [1] 0.22278 summary(flow_block3)$r.squared ## [1] 0.2971018 We can see that Block 3 has the highest \\(R^2\\) at .297, meaning that the Block 3 model explains about 29.7% of the variance in the outcome. Block 2 explains 22.3% while Block 1 explains 21.2%. Therefore, based on this alone we might say that Block 2 explains only a little bit of extra variance in flow proneness than Block 1, while Block 3 explains substantially more - therefore, we should go with Block 3. However… \\(R^2\\) will always increase with more predictors! The very fact that each additional predictor will explain more variance - even if only a tiny amount at a time - means that selecting based on \\(R^2\\) alone will naturally favour models with more predictors. This isn’t necessarily a useful thing! 10.4.2 Nested model tests This is a slightly more ‘formal’ test of whether a more complex model leads to a significant change in fit. This works by comparing nested models. Imagine model A and model B, two linear regressions fit on the same dataset. Model A has three predictors, and is the ‘full’ model of the thing we’re trying to the estimate. Model B drops one of the predictors from Model A, but keeps the other two. Model B is considered a nested model of Model A. The principle of this test is based on the idea of seeing whether a nested (reduced) model is a significantly better fit than a full model. If a nested model is a better fit, the residual sums of squares will decrease - less residuals indicate better fit. The anova() test works on this principle, but in a sort of reverse way. Because we’re testing whether a model with additional predictors is a better fit, naturally we should expect that the ‘nested’ model (in this case, our original model) will be a worse fit than the new model. In that case, a significant result indicates that the more complex model is the better fit. Nested model tests can be done with the anova() function from base R by simply giving it two model names in order. Let’s start by comparing Blocks 1 and 2: anova(flow_block1, flow_block2) And now between Blocks 2 and 3: anova(flow_block2, flow_block3) From this, we can conclude that Block 2 is a better fit to the data than Block 1 (F(1, 808) = 11.437, p &lt; .001), and also that Block 3 is again a better fit than Block 2 (F(1, 807) = 85.329, p &lt; .001). Therefore, using this method we would consider using the model in Block 3 for interpretation, as this provides a better fit of the data. This sort of lines up with what we saw with the \\(R^2\\) change (but this probably won’t always be the case). 10.4.3 Fit indices An alternative approach is to use fit indices, which are various measures that essentially indicate how well a model fits the data. Importantly, unlike \\(R^2\\) these measures penalise based on the complexity of the model - i.e. models with more predictors are penalised more due to their complexity. Two of the most widely used fit indices are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). They work similarly, but are just calculated in slightly different ways. The AIC and BIC are calculated by: \\[ AIC = 2k -2 ln(\\hat L) \\] \\[ BIC = k ln(n) - 2 ln (\\hat L) \\] \\(\\hat L\\) is called the likelihood, which is a whole thing that we won’t dive too much into. However, \\(2 ln (\\hat L)\\) - or -2LL, or minus two log likelihood - goes by the name of deviance (as in deviation). Deviance is essentially the residual sum of squares, and thus serves as a measure of model fit. R provides some really neat functions called - you guessed it - AIC() and BIC(). These will calculate the AIC and BIC values for every model name you give it. So, we can enter all of our values at once: AIC(flow_block1, flow_block2, flow_block3) BIC(flow_block1, flow_block2, flow_block3) "],["logistic-regression.html", "Chapter 11 Logistic regression", " Chapter 11 Logistic regression This section of the book (at least for the time being) deals with logistic regression. In some ways, we have come full circle with the inclusion of this chapter: the first statistical test we looked at were to do with categorical data, and now we make a partial return to categorical data. While I recognise that logistic regression has an awesome application to classification and basic machine learning, the focus of this book is not on classification but prediction. Thus, we won’t be diving into classification accuracy or ROC curves in the context of logistic regression, even though these are not terribly hard to implement in either R or Jamovi. "],["probability-and-odds.html", "11.1 Probability and odds", " 11.1 Probability and odds 11.1.1 Reminder of probabilities Consider the following table: Burnout No burnout Total Musician 20 4 24 Non-musician 10 8 18 Total 30 12 42 You may recall that this is a 2x2 contingency table, which we have seen before in a chi-square context. Using this table, we can work out the probability of certain events or outcomes. If we selected someone randomly from this table, for example, what is the probability that they would be a musician? Well, we can see that there are 24 musicians from the sample of 42, so we could simply say: \\[ P(Musician) = \\frac{24}{42} = 0.57 \\] Likewise, what is the probability that someone is burnt out? That would simply be: \\[ P(Burnout) = \\frac{30}{42} = 0.71 \\] What about the probability that someone is a musician and and burnt out? We could denote this as follows: \\[ P(Musician \\cap Burnout) = \\frac{20}{42} = 0.47 \\] What about the probability that someone is burnt out, given they are a musician? This would be a conditional probability, where we are finding a probability of something on the condition that the person is burnt out. There are 24 participants who reported burnout, so our calculation would be as follows: \\[ P(Burnout | Musician) = \\frac{20}{24} = 0.83 \\] 11.1.2 Odds Now, let’s talk about odds. Odds are simply the likelihood of a particular outcome occuring, and is calculated as the probability that an event will occur, divided by the probability that the event will not occur. In other words, if the probability of an event is denoted as \\(A\\), the probability of event \\(A\\) not occuring is \\(1-A\\). We can then calculate the odds as: \\[ Odds = \\frac{A}{1-A} \\] Let’s return to our example above, and print out the table again for ease of reference. Burnout No burnout Total Musician 20 4 24 Non-musician 10 8 18 Total 30 12 42 What are the odds of burnout in the musician group? To do this, we need to find the probability of burnout given they are musicians, and divide that by the probability of no burnout given they are musicians. The odds of burnout given that someone is a musician is as we saw above: \\[ P(Burnout | Musician) = \\frac{20}{24} \\] And the probability of someone not burning out given that they are a musician must therefore be: \\[ P(No \\ burnout | Musician) = \\frac{4}{24} \\] Now we can divide these two probabilities as follows: \\[ Odds = \\frac{20/24}{4/24} = \\frac{20}{4} = 5 \\] What this means is that musicians are 5 times as more likely to experience burnout than not experience it. One more example. What are the odds of burnout in the non-musician group? Using the same principles as above, we can calculate this as follows. \\[ P(Burnout | Nonmusician) = \\frac{10}{18} \\] \\[ P(No \\ burnout | Nonmusician) = \\frac{8}{18} \\] \\[ Odds = \\frac{10/18}{8/18} = \\frac{10}{8} = 1.25 \\] So even non-musicians are 1.25 times more likely - or, in other words, 25% more likely - to increase burnout than not experience it. 11.1.3 Odds ratios Now we can take a look at the odds ratio. The odds ratio describes how likely one outcome is given an exposure/group, compared to another exposure/group. The odds ratio is calculated by dividing the odds of event A by the odds of event B. The resulting value gives an indication of how much more likely event A is compared to event B, given differences in exposure. We have already calculated two sets of odds ratios: The odds that a musician experiences burnout; \\(Odds = 5\\) The odds that a non-musician experiences burnout; \\(Odds = 1.25\\) We can now calculate an odds ratio for how likely a musician is to experience burnout compared to a non-musician. We simply divide the two sets of odds: \\[ OR = \\frac{Odds(A)}{Odds(B)} \\] \\[ OR = \\frac{5}{1.25} = 4 \\] An odds ratio of 4 indicates that a musician is 4 times as likely to experience burnout compared to a non-musician. Heavens! "],["theory-of-logistic-regression.html", "11.2 Theory of logistic regression", " 11.2 Theory of logistic regression 11.2.1 Introduction All of the concepts on the previous page bring us to the main technique of this model, which is logistic regression. Logistic regression is used when we want to predict a binary outcome - for example, dead/alive status, affected/unaffected status and other scenarios where we have two primary outcomes. In this sense, we are essentially making a prediction about how likely outcome 1 is over outcome 0. Keep this in mind! 11.2.2 Modelling probabilities (sort of) Consider the following example data. We can see that we have two columns of interest: age and outcome. Notice how outcome only takes the values of 0 and 1. This is because this is a binary variable, where 0 = one outcome and 1 = another outcome. Often, we run into situations where we are interested in predicting a binary outcome using a series of predictors, including continuous ones. age_data Your first thought may be to just use a simple linear regression in this instance, and sure, R isn’t going to stop you from doing so: summary( lm(outcome ~ age, data = age_data) ) ## ## Call: ## lm(formula = outcome ~ age, data = age_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.36788 -0.12490 0.01528 0.13212 0.32370 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.55739 0.16515 -3.375 0.00819 ** ## age 0.10281 0.01421 7.235 4.89e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2108 on 9 degrees of freedom ## Multiple R-squared: 0.8533, Adjusted R-squared: 0.837 ## F-statistic: 52.35 on 1 and 9 DF, p-value: 4.893e-05 You might conclude that you have a significant model, with age being a significant predictor of the binary outcome. Nice! … right? Well, the moment you plot your data you may quickly see the problem with this approach: age_data %&gt;% ggplot( aes(x = age, y = outcome) ) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; There are two huge problems here! For starters, the line currently implies that there are values that exist between 0 and 1, but what is that meant to mean? In this instance, how can we have any intermediate values between our binary outcome? The second problem is that a simple linear regression also implies that there are values that exist beyond 0 and 1, as you can hopefully see in the graph above. This also makes no sense! 11.2.3 The logistic model In short, if we want to use our standard regression techniques, we need to model our data in a way where we can have outcomes beyond 0 or 1. Given that probabilities don’t let us do this, that’s a no-go (unless we use probit regression, but that’s a different kettle of fish). Maybe we could use the odds because they let us go past 1 - but as we saw previously, odds are bounded at a minimum of 0. However… log odds are not bounded in this way, as the log of 0 is \\(- \\infty\\). It also turns out that with a large enough sample size, the relationship between a predictor and the log odds is linear. Therefore, we can model a regression against the log odds as follows: \\[ log(Odds) = \\beta_0 + \\beta_1 x_1 + \\epsilon_i \\] This is essentially the equation for logistic regression. We use the linear regression formula to predict the log odds of an outcome. This makes logistic regression a form of the generalised linear model (GLM). We won’t go too into GLMs beyond here, but essentially the GLM uses a linear equation to model an outcome Y using a link function. The link function describes how the predictors relate to the outcome in the model, or in other words it allows us to use a linear regression on a transformed outcome: \\[ f(Y) = \\beta_0 + \\beta_1 x_1 + \\epsilon_i \\] In the formula above, \\(f(Y)\\) is used to describe the link function. In our instance, we are looking at a logit link to do a logistic regression, where our outcome is log odds (as opposed to Y, the dependent variable directly). There are many others out there that are suited for different types of data (e.g. Poisson regression). As another example of a link function, the identity function is \\(f(Y) = Y\\); this gives us linear regression, so really our usual regression models are just an example of the GLM. The logistic function is characterised by a very obvious S-shaped curve: (Technical note: the regression line that is fit is no longer least squares regression. Rather, it uses a procedure called maximum likelihood. Think of it as a different engine under the hood.) In practice, then, this means that we can use the same kinds of thinking as we have in previous regression models to interpret logistic regression outcomes. Namely, given the formula above, a 1-unit increase in \\(x_1\\) will correspond to a \\(\\beta_1\\) increase in the log odds. However, even though mathematically that makes sense, it’s hard to interpret what this actually means. What does an increase in log odds correspond to?? To solve this dilemma, we often will want to convert our outcome back into odds. To do so is simple: we simply exponentiate both sides: \\[ Odds = e^{\\beta_0 + \\beta_1 x_1 + \\epsilon_i} \\] To obtain the probability of an event occuring, we need to convert the odds back into a probability: \\[ P(Y = 1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\epsilon_i)}} \\] "],["example-8.html", "11.3 Example", " 11.3 Example Below is a dataset relating to the presence of sleep disorders (credit to Laksika Tharmalingam on Kaggle for this dataset!), and various metrics relating to lifestyle and physical health. The dataset contains the following variables: id: Participant id gender: Participant gender age: Participant age sleep_duration: The number of hours the participant sleeps per day sleep_quality: The participant’s subjective rating (1-10) of the quality of their sleep physical_activity: How many minutes per day the participant does physical activity stress: Subjective rating of stress level (1-10) bmi: BMI category (Underweight, normal, overweight) blood_pressure: systolic/diastolic blood pressure heart_rate: Resting heart rate of the participant, in bpm sleep_disorder: Whether the participant has a sleep disorder or not (0 = No, 1 = Yes) sleep &lt;- read_csv(here(&quot;data&quot;, &quot;logistic&quot;, &quot;sleep_data.csv&quot;)) ## Rows: 374 Columns: 11 ## ── Column specification ──────────────────── ## Delimiter: &quot;,&quot; ## chr (3): gender, bmi, blood_pressure ## dbl (8): id, age, sleep_duration, sleep_quality, physical_activity, stress, ... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. In this example, we’re interested in seeing whether specific factors predict whether or not the participant has a sleep disorder. We’ll work with an example with one predictor to start, and then move to multiple predictors afterwards. 11.3.1 Assumptions The refreshing thing about the logistic regression is that there are actually very few assumptions that need to be made. For starters, we do not assume the following: Linearity between the IV and the DV Normality Homoscedasticity None of these assumptions apply! What we do assume instead is: Linearity between the IV and the logit (i.e. the log odds) The outcome is a binary variable that is mutually exclusive (someone cannot be Y = 0 and Y = 1 at the same time) Absence of multicollinearity (in a multiple regression context) 11.3.2 Building the model Here is where things start to change a little from what we’re used to. Because we are not building a linear model but a generalised linear model, we now need to use the glm() function in R. glm(), by and large, works exactly the same way as you have seen with lm(); you need to give it arguments in the form of outcome ~ predictor, and once you have run the function you need to call the results using summary(). The first thing that changes is that by virtue of the fact that we are using the GLM, we must specify what link function we are working with. This can be set using the family argument. In the instance of logistic regression, the relevant family is binomial, and specifically binomial(link = \"logit\"): model &lt;- glm(outcome ~ predictor, data = data, family = binomial(link = &quot;logit&quot;)) The formula is a little strange, but binomial() is essentially a function that takes the argument link, for which we set the value as \"logit\". The logit argument is the default for this function though, so we can simply shorten this down to family = binomial specifically for logistic regressions. (For other binomial-based GLMs, you must specify the link.) With that in mind, we can build our logistic regression models. In the first instance, let’s see if age predicts whether someone has a sleep disorder. sleep_glm &lt;- glm(sleep_disorder ~ age, data = sleep, family = binomial) summary(sleep_glm) ## ## Call: ## glm(formula = sleep_disorder ~ age, family = binomial, data = sleep) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.28024 0.65343 -8.081 6.44e-16 *** ## age 0.11549 0.01493 7.738 1.01e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 507.47 on 373 degrees of freedom ## Residual deviance: 433.36 on 372 degrees of freedom ## AIC: 437.36 ## ## Number of Fisher Scoring iterations: 4 Note that our output table is a little bit different than what we’re used to; this is because of the change in procedure mentioned on the previous page (from least squares to maximum likelihood). However, we can still largely read this output the same way as we have. We can see that age is a significant predictor (p &lt; .001). The coefficient for age is .115. We can get the coefficients seperately by using the coef() function on our model: coef(sleep_glm) ## (Intercept) age ## -5.2802414 0.1154897 What does this mean? It means that for every 1 unit increase in age, the log odds increase by .115. This is important because remember that we’re modelling against log odds, not odds or probability! In essence, we have estimated the following: \\[ log(Odds) = -5.280 + (0.115 \\times x_1) \\] We need to make sense of this another way somehow. Recall that log odds and odds relate to each other in the following way: \\[ Odds = e^{\\beta_0 + \\beta_1 x_1 + \\epsilon_i} \\] To obtain the odds, as discussed on the previous page, we need to exponentiate our coefficients: exp(coef(sleep_glm)) ## (Intercept) age ## 0.005091201 1.122423009 The exponentiated coefficient gives us our odds ratio. This describes the multiplied change in odds for every 1 unit increase of our predictor. In this instance, for every 1 year increase in age, the predicted odds of having a sleep disorder are multiplied by 1.12. Another way to describe this is that the predicted odds of having a sleep disorder increase by a factor of 1.12. This coefficient does not mean the following: The odds increase by 1.12 for every unit of x - remember, only the log odds are linearly related to the predictor. The odds are non-linearly related. The probability increases by 1.12 - same deal as above, the probability isn’t linearly related to the predictors. We can get confidence intervals around our estimated coefficients using confint(), just like we have previously. We can do this either on the original coefficients, or the exponentiated ones. The confidence interval around the exponentiated coefficients gives us a 95% CI for our odds ratio.This is probably more useful in terms of interpretation than the log odds coefficients, so we have chosen them here. confint(sleep_glm) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) -6.60646431 -4.0395789 ## age 0.08712071 0.1457569 exp(confint(sleep_glm)) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 0.001351603 0.01760488 ## age 1.091028365 1.15691494 Thus, we can say that the OR of a sleep disorder is 1.12 (95% CI = [1.09, 1.16]). 11.3.3 Predictions Now recall that we can convert from odds to probabilities in the following manner: \\[ P(Y = 1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\epsilon_i)}} \\] Using this, we can make predictions about the probability of our outcome for a given value of our predictor. For example, what is the probability that a 50 year old will have a sleep disorder? That would be given as the following: \\[ P(Y = 1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\epsilon_i)}} \\] \\[ P(Y = 1) = \\frac{1}{1 + e^{-(-5.280 + (0.115 \\times 50)}} \\] \\[ P(Y = 1) = \\frac{1}{1 + e^{-0.47}} \\] We can use R to do the calculation for us: 1/(1 + exp(-0.47)) ## [1] 0.6153838 Thus, a 50 year old person has a 61.5% chance of having a sleep disorder (note that this has been rounded). We can actually plot the expected probabilities across a range of values for age by first asking R to predict the probabilities across a range of ages. This will draw the characteristic S-shaped curve of the logistic model. We use the predict() function to calculate the predicted probabilities of each value of our predictor. The type = response argument is used here to tell R to predict the probabilities (and not the log odds). age_range &lt;- data.frame( age = 20:70 ) age_range$predicted &lt;- predict(sleep_glm, age_range, type = &quot;response&quot;) age_range %&gt;% ggplot( aes(x = age, y = predicted) ) + geom_smooth(method = &quot;glm&quot;, method.args = list(family = binomial), se = FALSE) + theme_pubr() ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## Warning in eval(family$initialize): non-integer #successes in a binomial glm! 11.3.4 Logistic regression with multiple predictors Let’s now expand out the previous example to include two predictors: age and stress. Just like a regular multiple regression, logistic regression can include multiple continuous predictors. This will take on the form of the following: \\[ log(Odds) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 ... \\beta_n x_n + \\epsilon_i \\] This means that our odds formula becomes: \\[ Odds = e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 ... \\beta_n x_n + \\epsilon_i} \\] And so on so forth with our probability formula. Really, this is just an extension of what we have already seen in multiple regression, but applied to a logistic regression context. Let’s see what this looks like in R with the below code: sleep_glm2 &lt;- glm(sleep_disorder ~ age + stress, data = sleep, family = binomial) summary(sleep_glm2) ## ## Call: ## glm(formula = sleep_disorder ~ age + stress, family = binomial, ## data = sleep) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -13.26171 1.41932 -9.344 &lt; 2e-16 *** ## age 0.20510 0.02212 9.272 &lt; 2e-16 *** ## stress 0.77585 0.10607 7.315 2.58e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 507.47 on 373 degrees of freedom ## Residual deviance: 355.62 on 371 degrees of freedom ## AIC: 361.62 ## ## Number of Fisher Scoring iterations: 5 What do we see here? Well, we can see that age is a significant predictor of the presence of a sleep disorder (p &lt; .001), and stress is as well (p &lt; .001). Namely, for every year increase in age, the log odds of a sleep disorder increase by 0.205, holding stress constant. Likewise, for every 1 point increase in stress, the log odds increase by .776, holding age constant. To convert this into odds ratios, we exponentiate the coefficients: exp(coef(sleep_glm2)) ## (Intercept) age stress ## 1.739856e-06 1.227649e+00 2.172449e+00 And let’s generate confidence intervals for our odds ratios too: exp(confint(sleep_glm2)) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 9.083049e-08 0.0000240461 ## age 1.178095e+00 1.2851325926 ## stress 1.782252e+00 2.7038110976 We can see that the for every 1 year increase in age, the predicted odds of having a sleep disorder increase by a factor (i.e. are multiplied by) of 1.23 (95% CI: [1.18, 1.29]), holding stress constant. For every 1 unit increase in stress, the predicted odds of a sleep disorder increase by a factor of 2.17 (95% CI: [1.78, 2.70]), holding age constant. Just like before, we can also predict the probability that a participant will have a sleep disorder given their age and stress level. For example, let’s say we have a 50 year old with a stress level of 5: \\[ P(Y = 1) = \\frac{1}{1 + e^{-(-13.262 + (0.205 \\times 50) + (0.776 \\times 5))}} \\] \\[ P(Y = 1) = \\frac{1}{1 + e^{-0.868}} = 0.704 \\] THus, this individual has a 70.4% chance of having a sleep disorder. Now what about if the person’s stress level is 6? \\[ P(Y = 1) = \\frac{1}{1 + e^{-(-13.262 + (0.205 \\times 50) + (0.776 \\times 6))}} \\] \\[ P(Y = 1) = \\frac{1}{1 + e^{-1.644}} = 0.838 \\] Now the person’s probability is 83.8%. In other words, it helps not to be stressed! "],["pseudo-r2.html", "11.4 Pseudo \\(R^2\\)", " 11.4 Pseudo \\(R^2\\) 11.4.1 Regular \\(R^2\\) Recall that in a linear regression, we often talk about \\(R^2\\), or the coefficient of determination. We interpret this value as the amount of variance that is explained in our outcome by our predictor in the regression (or all of our predictors, in the case of multiple regression). As a reminder, here is the output for the regression on flow proneness in the regression module: w10_flow_lm &lt;- lm(DFS_Total ~ trait_anxiety + openness, data = w10_flow) summary(w10_flow_lm) ## ## Call: ## lm(formula = DFS_Total ~ trait_anxiety + openness, data = w10_flow) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16.596 -2.331 -0.151 2.308 12.794 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 32.65078 1.13377 28.798 &lt; 2e-16 *** ## trait_anxiety -0.11116 0.01278 -8.697 &lt; 2e-16 *** ## openness 0.83372 0.14538 5.735 1.38e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.705 on 808 degrees of freedom ## Multiple R-squared: 0.1364, Adjusted R-squared: 0.1343 ## F-statistic: 63.81 on 2 and 808 DF, p-value: &lt; 2.2e-16 As we can see, our value for \\(R^2\\) is .1364, meaning that 13.64% of the variance in our outcome, flow proneness, can be explained by our predictors trait anxiety and openness. The mathematical properties of least squares regression allow us to derive this value and interpret it fairly cleanly and easily. We can even compare \\(R^2\\) across similar models, or (in the hierarchical case) use it to directly compare model fit. In logistic regression, however, we cannot calculate the same value as we no longer use ordinary least squares regression. Instead, we use maximum likelihood, which provides a different way of calculating the various parameters (i.e. coefficients) in our model. Maximum likelihood methods in the context of logistic regression don’t really give us the same ‘clean’ and easily interpretable \\(R^2\\) as we get in normal regressions, because we don’t operate under the same method of minimising residuals. Rather, these \\(R^2\\) methods are calculated using each model’s likelihood, \\(\\hat L\\). To partially overcome this, several measures of pseudo \\(R^2\\) have been developed. The word ‘pseudo’ is important here, as it’s important to acknowledge that these are not quite the same thing as our usual \\(R^2\\). We can’t use them in the same way to directly compare across models, for instance - each pseudo-\\(R^2\\) has its own suggested interpretation, and thus do not always cohere with each other. The wonderful Statistical Methods and Data Analysis group at UCLA have a great explanation, with formulae for several pseudo-\\(R^2\\) measures. We will focus on three in this module, mainly for parity with Jamovi (but they also happen to be the most popular). 11.4.2 Calculating pseudo-\\(R^2\\) measures As a reminder, let’s print the output from our logistic regression on sleep: summary(sleep_glm) ## ## Call: ## glm(formula = sleep_disorder ~ age, family = binomial, data = sleep) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.28024 0.65343 -8.081 6.44e-16 *** ## age 0.11549 0.01493 7.738 1.01e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 507.47 on 373 degrees of freedom ## Residual deviance: 433.36 on 372 degrees of freedom ## AIC: 437.36 ## ## Number of Fisher Scoring iterations: 4 The DescTools package provides a very convenient function, PseudoR2(), to calculate these pseudo-\\(R^2\\) measures for us. This function requires a) the name of the glm model (i.e. our logistic regression object), and b) a character specifying which type(s) of \\(R^2\\) should be calculated. You’ll see this in the examples below. McFadden’s \\(R^2\\) is roughly analogous to a regular \\(R^2\\), in that it is intended to give an estimate of how much total variability is explained by the logistic model. It is calculated by comparing the fit of a logistic model against a null (i.e. no predictor) model. library(DescTools) PseudoR2(sleep_glm, which = &quot;McFadden&quot;) ## McFadden ## 0.1460373 Cox and Snell’s \\(R^2\\) is also calculated by comparing a full model to an null/no predictor model. The underlying calculation, however, is different, and a particular oddity of the Cox and Snell \\(R^2\\) is that the maximum possible value is less than 1. PseudoR2(sleep_glm, which = &quot;CoxSnell&quot;) ## CoxSnell ## 0.1797558 Nagelkerke’s \\(R^2\\) is an adjustment of the Cox and Snell \\(R^2\\) - specifically, it adjusts the value of \\(R^2\\) so that it ranges from 0-1. PseudoR2(sleep_glm, which = &quot;Nagelkerke&quot;) ## Nagelkerke ## 0.2420843 Finally, Tjur’s \\(R^2\\) is a relatively new pseudo-\\(R^2\\). It is calculated by first calculating the average predicted probabilities of the outcomes, and then taking the differences between those two probabilities. It is bounded between 0-1 and is also roughly analogous to a normal \\(R^2\\). PseudoR2(sleep_glm, which = &quot;Tjur&quot;) ## Tjur ## 0.1888425 11.4.3 When do you report it? Truthfully, as the UCLA help page states, pseudo-\\(R^2\\) methods are not as useful or as cleanly interpretable as normal OLS-based \\(R^2\\). They are only useful when comparing models using the same pseudo-\\(R^2\\) value, using the same data and variables. In other words, these measures are useful to select between competing models on the same data; they are not valid for comparing across datasets. Another thing to consider is that different measures perform differently. Simulation studies (e.g. Veall and Zimmermann 1992) have shown that Nagelkerke and McFadden’s \\(R^2\\) both severely underestimate the ‘true’ value of \\(R^2\\). Other methods exist, which can be calculated with PseudoR2(), but are not as widely implemented. Some will argue that \\(R^2\\) values are pointless outside of the model selection context. Others will say that it never hurts to report them anyway even if you are reporting just the one model. The decision, ultimately, is probably best left to you as the researcher to figure out what is most appropriate for what you are doing. "],["exploratory-factor-analysis.html", "Chapter 12 Exploratory factor analysis", " Chapter 12 Exploratory factor analysis Many of the things we are interested in psychological research are things that are both unobservable and not directly measurable. Consider, for instance, someone’s height: this is an observable and directly measurable quantity, in that we can a) see how tall they are and b) take a tape measure to them and get a direct, precise measurement of their height. In contrast, consider the things that we are often interested in when it comes to psychological constructs: motivations, wellbeing attitudes, beliefs, cognitive abilities, values, personality features… None of these kinds of constructs are directly measurable or even tangible. We can’t take a rule to measure how much someone enjoys listening to music, for example, or identify the extent to which they believe that listening to certain genres is healthy or unhealthy for you. However, we might be able to observe behaviours that might relate to these constructs, or people may respond to questions in ways that are indicative of said constructs. In this module we talk about factor analysis, which is one method of using statistics to identify these latent constructs. Specifically, we will focus on exploratory factor analysis, which aims to take a series of variables and identify the latent constructs that may underlie these variables. "],["introduction-to-efa.html", "12.1 Introduction to EFA", " 12.1 Introduction to EFA We generally conduct a survey because we’re interested in how different people hold different attitudes, ideas or beliefs about things. All of the standard statistical tools that you learned in Modules 5 and 6 are useful in this regard, and can absolutely be used with survey data. But given how highly dimensional survey data is, it opens up a new way of analysing data - exploratory factor analysis. 12.1.1 Dimension reduction Sometimes it’s easy to forget that designing a questionnaire and administering it gives rise to quite complex data. After all, a single question item may simply ask participants to rate themselves on a statement using a Likert scale, as we have seen earlier in this module. However, consider how many questions we might collect across a scale and it quickly becomes evident that this kind of data is highly dimensional - that is, with lots of individual questions we end up with a lot of data to sift through. This kind of scenario is where dimension reduction techniques become extremely useful. Dimension reduction techniques allow us to essentially collapse data into ‘supervariables’ that can simplify the analyses that we do by capturing the commonalities across questionnaire items. Principal components analysis (PCA) is the most common form of dimension reduction. Principal components analysis lets us take highly dimensional data, such as a questionnaire/scale with multiple items, and collapse that down into a smaller number of components. 12.1.2 Factor analysis Factor analysis is another analytical technique like dimension reduction. However, the key conceptual difference is that while PCA lets us collapse multiple variables into a smaller number of components, factor analysis lets us identify latent factors in our data. Latent factors are the factors underlying the behaviours and responses that we observe in our questionnaire items. We might find, for example, that performance on a series of tests is actually underlaid by multiple distinct, theoretically meaningful factors. Therefore, factor analysis lets us build and test theories about latent psychological constructs. Factor analysis allows us to indirectly measure these latent factors - essentially, are our questions tapping into the same ‘thing’? Factor analysis can be split into exploratory factor analysis (EFA) and confirmatory factor analysis (CFA). We will focus specifically on EFA in this module. A terminology note You’ll note that we’ve described PCA as generating components, while FA generates factors. It’s worth remembering that this is intentional, and the two terms should not be used interchangeably. We will talk more about this on the pages that follow, but components are simply linear combinations of multiple variables. Factors are estimates of latent variables that drive behaviour. The latter specifically is what we use to test theories about psychological constructs. 12.1.3 The steps of exploratory factor analysis EFA is quite an involved analysis, and there are several considerations that must be taken into account: Prepare data and assess for suitability Decide on the extraction method Decide on how many factors to retain Decide on the rotation method Interpret the results 12.1.4 Example dataset The example we’ll be using to work through this data is from a brilliant statistician and educator, Professor Andy Field, who is very highly regarded for his Discovering Statistics series - including Discovering Statistics with SPSS and Discovering Statistics with R. I highly recommend checking them out if you plan on using them! As part of his book, Prof. Field came up with a questionnaire called the SPSS Anxiety Questionnaire (SAQ). For the purposes of the next few pages we’ll be using a reduced version with just 9 questions, which we’ll call the SAQ-9. The questions in this survey are: Q1: Statistics makes me cry Q2: My friends will think I’m stupid for not being able to cope with SPSS Q4: I dream that Pearson is attacking me with correlation coefficients Q5: I don’t understand statistics Q6: I have little experience of computers Q14: Computers have minds of their own and deliberately go wrong whenever I use them Q15: Computers are out to get me Q19: Everybody looks at me when I use SPSS Q22: My friends are better at SPSS than I am saq &lt;- read_csv(here(&quot;data&quot;, &quot;efa&quot;, &quot;SAQ-9.csv&quot;)) ## Rows: 2571 Columns: 9 ## ── Column specification ──────────────────── ## Delimiter: &quot;,&quot; ## dbl (9): q01, q02, q04, q05, q06, q14, q15, q19, q22 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. As we walk through the content, we will use this dataset to illustrate how to conduct an exploratory factor analysis. (Note the questions were specifically chosen for demonstration purposes.) To actually conduct the EFA, we will primarily rely on two packages: psych and EFA.dimensions. The psych package is a fairly big package designed to run many common analyses in psychological science, specifically analyses that relate to psychometrics. It’s an incredibly useful package to be aware of in general. EFA.dimensions is another great package that provides functions to help with certain parts of the EFA process. "],["theory-of-efa.html", "12.2 Theory of EFA", " 12.2 Theory of EFA On this page, we (briefly) touch on a bit of the statistical theory underlying factor analysis and PCA. We won’t dive too deeply into the maths underlying EFA, but will focus on the high-level conceptual stuff. 12.2.1 PCA vs EFA Here is a good point to formally differentiate PCA vs EFA, following on from the brief disclaimer on the previous page. Both PCA and EFA will extract up to k factors that attempt to explain the observed variables. k, in this instance, is capped at the number of observed variables; so, if you input 24 variables into a PCA, the maximum number of components that you can estimate is 24. However, what defines these components/factors differs: In PCA, where we are just interested in collapsing variables, we assume that all variance in the observed variables is explained by the factors. Ultimately, a PCA will extract k components that ultimately explain 100% of the variance in all of the observed variables. In EFA, the goal is only to explain common variance between the observed variables. EFA explicitly models the variance in the items to be comprised of common variance, which is variance due to shared underlying factors, and unique variance. Unique variance can be further broken down into specific variance, which is variance that is specific to each item, and error variance. By partitioning variance in this way, EFA is able to test models of factors as it allows for disconfirmation - a vital part of any model testing. In short, EFA allows us to generate theoretical entities that we can test in subsequent analyses. PCA only provides us with a measure that combines the effects of multiple variables, but does not test latent factors. The common factor model The basis of factor analysis is the common factor model. Broadly speaking, the common factor model suggests that variables that relate to each other are likely driven by underlying latent variables. For example, if five questions in a survey all ask about a specific aspect of motivation, and these items correlate with each other, we would expect the same thing - or latent factor - to be driving responses on these five items. Exploratory factor analysis operates under this common factor model. Below is a basic diagram of the common factor model. The squares represent observed variables, which are the variables that we measure. The big circle denotes the latent factor that we want to estimate. The circles leading to the observed variables are error terms. In an exploratory factor analysis, the primary thing we want to investigate is the factor loadings, denoted by the various lambdas (\\(\\lambda\\)). We will see precisely what the factor loadings are later, but generally they are how strongly the latent factor predicts each observed variable. Now let’s take a look at the common factor model with two factors. As you can see, we allow every observed variable to load onto every factor - thus, we estimate factor loadings for every possible path going from latent factors to observed variables. This is what we estimate in exploratory factor analysis (and PCA - sort of). For brevity’s sake, only three lambdas have been shown, but hopefully they are illustrative enough to get the general gist across. Every path leading from a latent variable to an observed variable is a parameter to be estimated in an exploratory factor analysis. 12.2.2 Partial correlations In Module 10, we talked about the concept of correlation - i.e. how related two variables are. Recall that correlation coefficients are scaled from -1 to 1. Now we move to the concept of partial correlation - the relationship between two variables while controlling for a third. Let’s come back to the flow data in Week 10. Imagine we want to test the correlation between Gold MSI scores and flow proneness. However, we might suspect that openness to experience may play a role in the relationship between these variables. One thing we could do is to calculate a partial correlation between MSI scores and flow proneness while controlling for openness. If we know the correlation between: MSI scores and flow proneness, MSI scores and openness, Flow proneness and openness, We could calculate the partial correlation between Gold-MSI and flow proneness, controlling for openness, using the below formula: Both PCA and EFA rely on estimating partial correlations. Specifically, factor analysis aims to estimate latent factors that minimise the partial correlations among observed variables. If a latent factor perfectly explains the relationship between two variables, the partial correlations between the observed variables should be zero. A lot of the ‘under the hood’ maths, which we won’t touch on, essentially relates to identifying the latent factors that maximise the amount of variance explained in each variable by the factor solutions. "],["initial-considerations-for-efa.html", "12.3 Initial considerations for EFA", " 12.3 Initial considerations for EFA We’ll start with some basic considerations for EFA/PCA. These are generally things that should be thought about/considered before an EFA, or at least before you interpret the results. 12.3.1 Sample size For adequate power, EFA typically needs a fairly big sample size. There is no clear agreement about what constitutes a ‘good’ sample size, and it’s difficult to give concrete recommendations. Many guides and sources will often mention a n:p rule of thumb, where n is sample size and p is number of variables (which is the number of parameters that needs to be estimated). The idea is that an ideal EFA sample size will have n participants for every variable you are analysing. These can range from as low as 3:1 to 20:1, with a typical ‘ok’ range being from 10-20:1. However, there is no clear support for these rules, and no minimum is truly sufficient (Hogarty et al. 2005). In general - the bigger the better, and the more variables you have the more participants you need. Aim for at least 300+ no matter the circumstance (this is a very blunt rule of thumb!). Below are our descriptives. While we can use a bit of tidyverse to get descriptives, the describe() function from the psych package is also convenient for getting basic descriptives for every column in a dataset. We can see that we have n = 2571, which should be more than adequate. describe(saq) 12.3.2 Assumptions EFA can be conducted with one of multiple algorithms that determines the final factor structure to be extracted. Different methods rely on different assumptions, so basic assumption checks are useful: Data must be interval/ratio data. Ordinal data is problematic unless you generally have at least 5 scale points; in which case, you can broadly approximate this to be continuous. Normality is important, depending on the method. The usual QQ-plot or Shapiro-Wilks tests on individual items can be useful here.a Multicollinearity: observed variables should not be collinear with each other. Note though that this only tests the normality of one variable, i.e. univariate normality. EFA is ideal with multivariate normality, i.e. the joint dimensions of the entire dataset are normally distributed. Univariate normality is necessary but not sufficient for multivariate normality. Sadly, however, Jamovi doesn’t give an easy way to test multivariate normality so let’s run with univariate normality for now. 12.3.3 Factorability Factorability broadly describes whether the data are likely to be amenable to factor analysis. If data are factorable, it suggests that there is likely to be at least one latent factor underlying the observations. We can test factorability in three ways: Correlations A simple matrix of correlations can give us a first-pass indication of factorability. If most items correlate with each other, this can indicate that there are underlying latent factors. There is no hard and fast rule for what counts as ‘acceptable’, but if most variables are not significantly correlated that indicates that the data may not be factorable. In our SAQ-9 data, we can see that all correlations between variables are significant, which is generally a good sign. cor(saq) ## q01 q02 q04 q05 q06 q14 ## q01 1.00000000 -0.09872403 0.43586018 0.4024399 0.21673399 0.3378797 ## q02 -0.09872403 1.00000000 -0.11185965 -0.1193466 -0.07420968 -0.1646999 ## q04 0.43586018 -0.11185965 1.00000000 0.4006722 0.27820154 0.3508096 ## q05 0.40243992 -0.11934658 0.40067225 1.0000000 0.25746014 0.3153381 ## q06 0.21673399 -0.07420968 0.27820154 0.2574601 1.00000000 0.4022441 ## q14 0.33787966 -0.16469991 0.35080964 0.3153381 0.40224407 1.0000000 ## q15 0.24575263 -0.16499581 0.33423089 0.2613719 0.35989309 0.3801148 ## q19 -0.18901103 0.20329748 -0.18597751 -0.1653221 -0.16675017 -0.2540581 ## q22 -0.10440866 0.23087487 -0.09838349 -0.1325359 -0.16513541 -0.1698375 ## q15 q19 q22 ## q01 0.2457526 -0.1890110 -0.10440866 ## q02 -0.1649958 0.2032975 0.23087487 ## q04 0.3342309 -0.1859775 -0.09838349 ## q05 0.2613719 -0.1653221 -0.13253593 ## q06 0.3598931 -0.1667502 -0.16513541 ## q14 0.3801148 -0.2540581 -0.16983754 ## q15 1.0000000 -0.2098023 -0.16790617 ## q19 -0.2098023 1.0000000 0.23392259 ## q22 -0.1679062 0.2339226 1.00000000 # Alternatively, the lowerCor() fucntion from psych prints this more nicely lowerCor(saq) ## q01 q02 q04 q05 q06 q14 q15 q19 q22 ## q01 1.00 ## q02 -0.10 1.00 ## q04 0.44 -0.11 1.00 ## q05 0.40 -0.12 0.40 1.00 ## q06 0.22 -0.07 0.28 0.26 1.00 ## q14 0.34 -0.16 0.35 0.32 0.40 1.00 ## q15 0.25 -0.16 0.33 0.26 0.36 0.38 1.00 ## q19 -0.19 0.20 -0.19 -0.17 -0.17 -0.25 -0.21 1.00 ## q22 -0.10 0.23 -0.10 -0.13 -0.17 -0.17 -0.17 0.23 1.00 Bartlett’s test of sphericity Bartlett’s test of sphericity tests the null hypothesis that all correlations between variables are zero at the population level. In other words, if Bartlett’s test is non-significant it suggests that all of the indicator variables are not correlated. As a result, Bartlett’s test is pretty much always significant as it is very sensitive to sample size. Unsurprisingly, then, our Bartlett’s test result is significant. To run this, we use the cortest.bartlett() function from psych. Note that base R does include a function called bartlett.test(), but this is not the same test! (same Bartlett, though.) cortest.bartlett(saq) ## R was not square, finding R from data ## $chisq ## [1] 3674.737 ## ## $p.value ## [1] 0 ## ## $df ## [1] 36 Kaiser-Meyer-Olkin (KMO) Test/Kaiser’s Measure of Sampling Adequacy This test is often referred to as the KMO Test or Kaiser’s MSA, but both respectively mean the same thing. It is a measure of how much variance among all variables might be due to common variance. Higher KMO/MSA values indicate that more variance is likely due to common factors, thus indicating suitability for factor analysis. Kaiser (1974) provided the following (hilarious) interpretations of MSA values: MSA values are typically calculated for each variable, and for overall. It helps to report both. The KMO() function in psych will calculate both sets of measures of sampling adequacy. Here are our variables below. Overall they are generally in the meritorious range (except for one): KMO(saq) ## Kaiser-Meyer-Olkin factor adequacy ## Call: KMO(r = saq) ## Overall MSA = 0.82 ## MSA for each item = ## q01 q02 q04 q05 q06 q14 q15 q19 q22 ## 0.81 0.76 0.82 0.84 0.82 0.84 0.85 0.84 0.77 "],["how-many-factorscomponents.html", "12.4 How many factors/components?", " 12.4 How many factors/components? A crucial element of doing an EFA is deciding on the number of factors that should be extracted for the final solution. This is not a trivial decision, and essentially determines the final factor structure you derive and interpret in your factor analysis. Note that while we mainly talk about factors on this page, the same considerations apply when thinking of components in PCA. 12.4.1 Deciding on the number of factors Recall that an EFA/PCA will extract up to k factors/components, where k is the number of observed variables. At k factors/components, this will have explained all of the possible variance there is to explain in the observed variables. The basic idea behind how these factors are calculated is by essentially drawing straight lines through our data, much like a regression line. The idea is that each straight line (factor/component) should explain as much variance as possible, and each successive factor/component that is drawn explains the remaining variance. The first factor/component will always attempt to explain the most variance possible. The second factor/component will then be drawn through what remains after the first factor/component is calculated, in a way that both maximises the variance captured and is uncorrelated with the first factor/component. This lets us capture as much variance as possible in a clean way, where we can identify the relative contributions of each successive factor/component. The amount of variance that is captured by each factor/component is represented by a number called the eigenvalue. Naturally, the first factor/component will have the highest eigenvalue, and the eigenvalue of each factor/component afterwards will decrease. This means that at some point, we reach a stage where an additional factor doesn’t add much in terms of the variance explained. This indicates that there isn’t much utility in retaining factors after a certain point - i.e. we get diminishing returns on increasing the number of factors we have to interpret. We must strike a balance between having a relatively straightforward factor structure to interpret and how much variance is explained. Too few factors means we may not accurately capture enough variance to be meaningful or miss very crucial relationships, but too many factors means we lose parsimony and interpretability. There are several ways in which we can identify where the most optimal number of factors to retain is. 12.4.2 The Kaiser-Guttman rule The Kaiser-Guttman, Kaiser or simply the “eigenvalue &gt; 1” rule states that we should simply keep any factor with an eigenvalue above 1. To do this, we first need a correlation matrix from our data. We then feed this to the eigen() function in base R, which will calculate eigenvalues. I’ve piped it here, but you can also go straight to eigen(cor(saq)). Our data suggests that we retain 2 factors using this rule. saq_eigen &lt;- cor(saq) %&gt;% eigen() saq_eigen$values ## [1] 2.9515869 1.2029159 0.9339932 0.7897083 0.7671243 0.6453702 0.6078335 ## [8] 0.5663971 0.5350707 12.4.3 The scree plot The scree plot is a plot of each factor’s eigenvalue. This method relies on visual inspection - namely, you want to identify the ‘elbow’ of the line, or the point where the graph levels off. This is the point where the amount of variance explained by additional factors reaches that diminishing returns phase. psych will plot two sets of eigenvalues - one ‘component’-based set (which is what we calculated above), and one ‘factor’-based set (which is what Jamovi gives you). This is inherently a bit subjective, and sometimes isn’t very clear. On our scree plot below, it looks like four factors is the point where the diminishing returns begin, so we would go with retaining four factors. However, a more conservative interpreter could reasonably argue that we should only retain 2 factors. scree(saq) 12.4.4 Parallel analysis Parallel analysis (Horn, 1965) is a sophisticated technique that involves simulating random datasets of the same size as our actual dataset, and comparing our dataset’s eigenvalues against the random dataset’s eigenvalues. Parallel analysis is generally demonstrated using a scree plot with an additional scree line for the simulated datasets. The fa.parallel() function will run this for us. The number of factors to retain is determined by the number of factors where our actual data’s eigenvalue exceeds the simulated dataset’s eigenvalue. In this instance, we would choose to retain three factors, as the actual data eigenvalues clearly drop below the simulated data at four factors. fa.parallel(saq) ## Parallel analysis suggests that the number of factors = 3 and the number of components = 2 12.4.5 How to decide? Let’s summarise our interim decisions so far: Kaiser’s rule suggests 1 factor Visual scree plot inspection suggests either 2 or 4 factors Parallel analysis suggests 3 factors How do we decide what to use? Decades of empirical and simulation literature have shown a couple of things: Parallel analysis is one of the best methods of identifying how many factors should be retained. While it is sensitive to various things like sample size, simulation studies have shown that parallel analysis consistently outperforms other methods in terms of how many factors should be retained. In contrast, do not use the Kaiser rule! The Kaiser rule will consistently misestimate the number of factors - often, the misestimation will be quite severe. It is an extremely popular rule because a) it is simple to interpret and b) SPSS, which was the dominant statistical program of choice for a very long time, defaults to only using the Kaiser rule for PCA/EFA. Theoretical and practical considerations should also inform your decision making. If parallel analysis suggests 7 factors, for example, but those 7 factors are hard to interpret then you should probably not run with that by default. Instead, the next thing to do would be to step through solutions that remove one factor at a time until an acceptable, interpretable model has been reached. There are other methods of identifying factors, such as Vellicer’s Minimum Average Partials test, but Jamovi does not provide these options. In the absence of any strong justification for anything else, it is best to fall back on parallel analysis. Thus, it’s best if we go with three factors. "],["interpreting-output.html", "12.5 Interpreting output", " 12.5 Interpreting output Let’s now look at how to actually interpret the output of a factor analysis, including how to make sense of the main numbers that you get out of a basic EFA/PCA output. 12.5.1 Extraction methods PCA only has one method of deriving the eigenvalues of the components, and so it will give the same answer every time you run it on the same dataset. EFA, however, has multiple possible means of estimating factors, which are typically termed extraction methods. We won’t go into the details of how exactly they work, but the key thing to know is that the extraction method essentially changes There are three extraction methods available in Jamovi: Maximum likelihood. One of the most common options, and provides the most generalisable and robust estimates. ML methods assume normality and generally require large datasets, however. Principal axis factoring. Principal axis factoring does not make particular assumptions about the normality or distribution of the data, meaning that it is good at handling more complex datasets. Minimum residuals. The minimum residual method is sort of a middle-ground option between the two, but isn’t as commonly used in psychological research. Under ideal conditions, maximum likelihood (ML) and PA (principal axis) methods will generally give very similar estimates of the factors. When data are severely non-normal (or you want to anticipate that it will be), it is better to go with PA in the first instance. Otherwise, ML estimates are generally the way to go. To run a factor analysis in r, we use the fa function from psych. At minimum, we must specify the following: The dataset as the first argument The number of factors we want to extract The type of rotation - for now set this to “none”, as we’ll talk about this later The factoring method, fm - as above. \"ml\" stands for maximum likelihood, \"pa\" stands for principal axis factoring and \"minres\" stands for minimum residuals. saq_efa &lt;- fa( saq, nfactors = 3, rotate = &quot;none&quot;, fm = &quot;ml&quot; ) 12.5.2 Interpreting output Below is the main output of our EFA. This is what we call a factor matrix: saq_efa ## Factor Analysis using method = ml ## Call: fa(r = saq, nfactors = 3, rotate = &quot;none&quot;, fm = &quot;ml&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## ML1 ML2 ML3 h2 u2 com ## q01 0.59 0.31 -0.12 0.46 0.54 1.6 ## q02 -0.25 0.25 0.32 0.23 0.77 2.8 ## q04 0.62 0.23 -0.02 0.44 0.56 1.3 ## q05 0.56 0.19 -0.06 0.36 0.64 1.3 ## q06 0.55 -0.21 0.32 0.45 0.55 2.0 ## q14 0.63 -0.12 0.10 0.42 0.58 1.1 ## q15 0.55 -0.15 0.10 0.34 0.66 1.2 ## q19 -0.37 0.21 0.22 0.23 0.77 2.3 ## q22 -0.28 0.31 0.24 0.24 0.76 2.9 ## ## ML1 ML2 ML3 ## SS loadings 2.33 0.47 0.35 ## Proportion Var 0.26 0.05 0.04 ## Cumulative Var 0.26 0.31 0.35 ## Proportion Explained 0.74 0.15 0.11 ## Cumulative Proportion 0.74 0.89 1.00 ## ## Mean item complexity = 1.8 ## Test of the hypothesis that 3 factors are sufficient. ## ## df null model = 36 with the objective function = 1.43 with Chi Square = 3674.74 ## df of the model are 12 and the objective function was 0.01 ## ## The root mean square of the residuals (RMSR) is 0.01 ## The df corrected root mean square of the residuals is 0.02 ## ## The harmonic n.obs is 2571 with the empirical chi square 28.51 with prob &lt; 0.0046 ## The total n.obs was 2571 with Likelihood Chi Square = 33.87 with prob &lt; 0.00071 ## ## Tucker Lewis Index of factoring reliability = 0.982 ## RMSEA index = 0.027 and the 90 % confidence intervals are 0.016 0.037 ## BIC = -60.35 ## Fit based upon off diagonal values = 1 ## Measures of factor score adequacy ## ML1 ML2 ML3 ## Correlation of (regression) scores with factors 0.89 0.65 0.59 ## Multiple R square of scores with factors 0.79 0.43 0.35 ## Minimum correlation of possible factor scores 0.59 -0.15 -0.31 What do these numbers mean? Firstly, each value in each factor column (labelled ML1, ML2 etc) gives us our loadings. Note that as per the common factor model we described before, we estimate a loading for every variable on every factor. However, this can be a bit gross to interpret, so we generally choose to suppress (not remove) loadings below a certain threshold. By default, Jamovi will hide loadings below 0.3. We can also sort the items based on their loadings. To do this, we use the fa.sort() function and feed in our EFA object. We can then use print() to clean up this output with two arguments: digits to show rounding, and cut to suppress values below a certain size. This gives us a nicer output: saq_efa &lt;- fa.sort(saq_efa) print(saq_efa, digits = 3, cut = .30) ## Factor Analysis using method = ml ## Call: fa(r = saq, nfactors = 3, rotate = &quot;none&quot;, fm = &quot;ml&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## ML1 ML2 ML3 h2 u2 com ## q14 0.631 0.420 0.580 1.12 ## q04 0.622 0.441 0.559 1.27 ## q01 0.589 0.311 0.458 0.542 1.61 ## q05 0.562 0.358 0.642 1.26 ## q15 0.551 0.337 0.663 1.23 ## q06 0.546 0.324 0.448 0.552 1.97 ## q19 -0.371 0.229 0.771 2.27 ## q22 0.314 0.238 0.762 2.88 ## q02 0.319 0.229 0.771 2.84 ## ## ML1 ML2 ML3 ## SS loadings 2.333 0.474 0.352 ## Proportion Var 0.259 0.053 0.039 ## Cumulative Var 0.259 0.312 0.351 ## Proportion Explained 0.738 0.150 0.111 ## Cumulative Proportion 0.738 0.889 1.000 ## ## Mean item complexity = 1.8 ## Test of the hypothesis that 3 factors are sufficient. ## ## df null model = 36 with the objective function = 1.432 with Chi Square = 3674.737 ## df of the model are 12 and the objective function was 0.013 ## ## The root mean square of the residuals (RMSR) is 0.012 ## The df corrected root mean square of the residuals is 0.021 ## ## The harmonic n.obs is 2571 with the empirical chi square 28.515 with prob &lt; 0.00465 ## The total n.obs was 2571 with Likelihood Chi Square = 33.874 with prob &lt; 0.000706 ## ## Tucker Lewis Index of factoring reliability = 0.982 ## RMSEA index = 0.0266 and the 90 % confidence intervals are 0.0163 0.0374 ## BIC = -60.351 ## Fit based upon off diagonal values = 0.998 ## Measures of factor score adequacy ## ML1 ML2 ML3 ## Correlation of (regression) scores with factors 0.892 0.652 0.587 ## Multiple R square of scores with factors 0.795 0.425 0.345 ## Minimum correlation of possible factor scores 0.590 -0.150 -0.310 Statistically speaking, in a factor matrix each loading is a regression coefficient for the latent factor predicting the variable. We can interpret them as we would with normal regressions, except this is a regression coefficient for our latent variable predicting each observed variable. For example, the loading for q14 on factor 1 is 0.631. This means that for every 1 unit increase on latent factor 1, scores on Q14 increase by .631 units. Jamovi typically only gives you the uniqueness column, u2, which gives us the value of unique variance as a percentage. This is how much variance in each item is not explained by the factors we have chosen. In this instance, 58% of the variance in Q14 is not explained by the three factors. Generally though, it’s easier to think in terms of communalities, or the values in the h2 column, which is the amount of variance that is explained by the factors. Communalities are simply 1 - uniqueness; thus, the communality of Q14 is 0.420, which indicates that 42% of the variance in Q14 is explained by the three factors. Higher communalities indicate that the factors collectively explain more variance in the observed variable. How is this value calculated? Communalities are the sum of the squared factor loadings. Therefore, we can look at Q14’s factor loadings in the first (pre-sorted) output table, and calculate the communality as: \\[ h^2 = .631^2 + -.116^2 + .0963^2 \\] Which gives us an answer of approximately 0.420: .631^2 + (-.116)^2 + .0963^2 ## [1] 0.4208907 12.5.3 Total variance explained The output of psych will also give you a brief table of the total amount of variance explained by each factor. This is shown in the output of fa, but can also be accessed again below. It is generally useful to at least report the total cumulative variance explained by all factors. In this case, the three factors collectively explain 35.1% of the total variance in the data (shown by the row saying “Cumulative var”). saq_efa$Vaccounted ## ML1 ML2 ML3 ## SS loadings 2.3325379 0.47409809 0.35202983 ## Proportion Var 0.2591709 0.05267757 0.03911443 ## Cumulative Var 0.2591709 0.31184845 0.35096287 ## Proportion Explained 0.7384567 0.15009441 0.11144890 ## Cumulative Proportion 0.7384567 0.88855110 1.00000000 "],["rotation-and-interpreting-output-again.html", "12.6 Rotation, and interpreting output again", " 12.6 Rotation, and interpreting output again You may have noticed that on the previous page, we didn’t make much of an effort to actually talk about what the factors were or what they meant. That’s because the output that we got on the previous page isn’t actually terribly informative or easy to interpret. To help with this, in EFA we perform a technique called rotation. 12.6.1 Rotations Rotations are a technique in EFA that are done to help with the interpretability of the factor solution. The key aim of rotation is to achieve simple structure where possible. You may have noticed that the matrix from the previous page has quite a few variables with high enough loadings on multiple factors. This is called cross-loading, and indicates that two factors explain the variable. This is not easily interpretable! Ideally, in a robust simple structure we want: Only one loading per variable At least three loadings per factor print(saq_efa, digits = 3, cut = 0.3) ## Factor Analysis using method = ml ## Call: fa(r = saq, nfactors = 3, rotate = &quot;none&quot;, fm = &quot;ml&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## ML1 ML2 ML3 h2 u2 com ## q14 0.631 0.420 0.580 1.12 ## q04 0.622 0.441 0.559 1.27 ## q01 0.589 0.311 0.458 0.542 1.61 ## q05 0.562 0.358 0.642 1.26 ## q15 0.551 0.337 0.663 1.23 ## q06 0.546 0.324 0.448 0.552 1.97 ## q19 -0.371 0.229 0.771 2.27 ## q22 0.314 0.238 0.762 2.88 ## q02 0.319 0.229 0.771 2.84 ## ## ML1 ML2 ML3 ## SS loadings 2.333 0.474 0.352 ## Proportion Var 0.259 0.053 0.039 ## Cumulative Var 0.259 0.312 0.351 ## Proportion Explained 0.738 0.150 0.111 ## Cumulative Proportion 0.738 0.889 1.000 ## ## Mean item complexity = 1.8 ## Test of the hypothesis that 3 factors are sufficient. ## ## df null model = 36 with the objective function = 1.432 with Chi Square = 3674.737 ## df of the model are 12 and the objective function was 0.013 ## ## The root mean square of the residuals (RMSR) is 0.012 ## The df corrected root mean square of the residuals is 0.021 ## ## The harmonic n.obs is 2571 with the empirical chi square 28.515 with prob &lt; 0.00465 ## The total n.obs was 2571 with Likelihood Chi Square = 33.874 with prob &lt; 0.000706 ## ## Tucker Lewis Index of factoring reliability = 0.982 ## RMSEA index = 0.0266 and the 90 % confidence intervals are 0.0163 0.0374 ## BIC = -60.351 ## Fit based upon off diagonal values = 0.998 ## Measures of factor score adequacy ## ML1 ML2 ML3 ## Correlation of (regression) scores with factors 0.892 0.652 0.587 ## Multiple R square of scores with factors 0.795 0.425 0.345 ## Minimum correlation of possible factor scores 0.590 -0.150 -0.310 Rotation can help us achieve this. What rotations essentially do is change how variance is distributed within each factor. That wording is extremely important to note. It does not change our data in any way - the actual amount of variance explained by each factor does not change. What does change is how variance is distributed across the factors, which has the effect of then changing the loadings. But the actual data does not change!! There are two families of rotations that we can employ. Orthogonal rotations force factors to be uncorrelated. Oblique rotations allow factors to be correlated. The below diagram visualises what rotations do: Which rotation to choose? In psychology, everything tends to be correlated with everything else, and it’s extremely rare that we would get an instance where two factors do not correlate at all. For that reason, oblique rotations are generally the way to go. Orthogonal rotations are extremely hard to justify without strong a-priori evidence - and even if two factors are uncorrelated, oblique rotations will give the same solution as orthogonal ones. In short, there’s generally no reason to prefer an orthogonal rotation by default. 12.6.2 Rotated factor solution Let’s apply an oblique rotation to our factor analysis. The default oblique rotation is called oblimin. To do this, we need to re-run our fa() function with a rotation specified. saq_efa_rot &lt;- fa( saq, nfactors = 3, rotate = &quot;oblimin&quot;, fm = &quot;ml&quot; ) This produces the following output, which we now term a pattern matrix: saq_efa_rot %&gt;% fa.sort() %&gt;% print(digits = 3, cut = 0.3) ## Factor Analysis using method = ml ## Call: fa(r = saq, nfactors = 3, rotate = &quot;oblimin&quot;, fm = &quot;ml&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## ML1 ML3 ML2 h2 u2 com ## q01 0.714 0.458 0.542 1.02 ## q04 0.609 0.441 0.559 1.06 ## q05 0.553 0.358 0.642 1.02 ## q06 0.704 0.448 0.552 1.01 ## q14 0.440 0.420 0.580 1.57 ## q15 0.434 0.337 0.663 1.32 ## q02 0.506 0.229 0.771 1.04 ## q22 0.488 0.238 0.762 1.08 ## q19 0.412 0.229 0.771 1.11 ## ## ML1 ML3 ML2 ## SS loadings 1.360 1.043 0.756 ## Proportion Var 0.151 0.116 0.084 ## Cumulative Var 0.151 0.267 0.351 ## Proportion Explained 0.431 0.330 0.239 ## Cumulative Proportion 0.431 0.761 1.000 ## ## With factor correlations of ## ML1 ML3 ML2 ## ML1 1.000 0.591 -0.411 ## ML3 0.591 1.000 -0.448 ## ML2 -0.411 -0.448 1.000 ## ## Mean item complexity = 1.1 ## Test of the hypothesis that 3 factors are sufficient. ## ## df null model = 36 with the objective function = 1.432 with Chi Square = 3674.737 ## df of the model are 12 and the objective function was 0.013 ## ## The root mean square of the residuals (RMSR) is 0.012 ## The df corrected root mean square of the residuals is 0.021 ## ## The harmonic n.obs is 2571 with the empirical chi square 28.515 with prob &lt; 0.00465 ## The total n.obs was 2571 with Likelihood Chi Square = 33.874 with prob &lt; 0.000706 ## ## Tucker Lewis Index of factoring reliability = 0.982 ## RMSEA index = 0.0266 and the 90 % confidence intervals are 0.0163 0.0374 ## BIC = -60.351 ## Fit based upon off diagonal values = 0.998 ## Measures of factor score adequacy ## ML1 ML3 ML2 ## Correlation of (regression) scores with factors 0.852 0.823 0.736 ## Multiple R square of scores with factors 0.726 0.678 0.541 ## Minimum correlation of possible factor scores 0.453 0.356 0.082 Now we can see our simple structure taking effect, and this output is much more interpretable from before. These values are still regression coefficients between each latent factor and each variable, but now we can group these variables into their underlying latent factors much more easily. We can see that questions 1, 4 and 5 are best captured by factor 1, questions 6, 14 and 15 by factor 2 and questions 2, 19 and 22 by factor 3. One warning here. On the previous page, where we had an unrotated solution, the communalities were calculated by summing the squared factor loadings for each variable. That rule no longer applies here because by allowing the factors to correlate, the regression loadings now capture non-specific variance. Summing the squared factor loadings will lead to greater communality values than what they actually are. However, as you can hopefully see in the uniqueness column, the actual communalities have not changed. Rotation does not change how much variance is explained in total - only how that variance is distributed! Oblique rotations will also generate a factor correlation matrix. This calculates the correlations between the factors - remembering that by specifying an oblique rotation, we allowed them to correlate: saq_efa_rot$Phi ## ML1 ML3 ML2 ## ML1 1.0000000 0.5909483 -0.4107233 ## ML3 0.5909483 1.0000000 -0.4475027 ## ML2 -0.4107233 -0.4475027 1.0000000 Finally, we get the table of variance explained. This is now not as useful because of the same reason we cannot sum the squared factor loadings - each factor on its own now captures shared variance across the other factors as they are allowed to correlate. However, the total amount of variance explained is still 35.1%; once again, this does not change. saq_efa_rot$Vaccounted ## ML1 ML3 ML2 ## SS loadings 1.3601670 1.0425634 0.75593543 ## Proportion Var 0.1511297 0.1158404 0.08399283 ## Cumulative Var 0.1511297 0.2669700 0.35096287 ## Proportion Explained 0.4306144 0.3300645 0.23932111 ## Cumulative Proportion 0.4306144 0.7606789 1.00000000 "],["mediation-and-moderation.html", "Chapter 13 Mediation and Moderation", " Chapter 13 Mediation and Moderation Sometimes, the hypotheses we want to test are ones where the effect of interest works in a very specific way. In some cases, we might hypothesise that the relationship between two variables is actually best explained by a third variable that acts between them. For instance, we might predict that a relationship between sugar levels in food and happiness is best explained by food intake - we might eat more sugar, and eating more makes us happier (or something like that).5 This kind of hypothesis is called a mediation, and is common in psychological research as a lot of our work deals with processes. In other instances, we might expect interaction effects like what we saw in two-way ANOVA, but between continuous predictors. Note that up until this point we have only considered categorical x categorical interactions within an ANOVA context, so now is a good time to explore continuous x continuous interactions. We describe these effects as moderation effects. This chapter will cover both mediation and moderation. I’m no dietitian or food psychology expert so I make no claims as to the veracity or accuracy of this statement.↩︎ "],["mediation-intro.html", "13.1 Mediation: Intro", " 13.1 Mediation: Intro We start with an overview of the concept of mediation, with special consideration to its interpretation for the time being. 13.1.1 Introduction to mediation Mediation refers to a specific model where we hypothesise that the relationship between a predictor X and an outcome Y is explained by the effect of a third mediating variable. You can imagine a simple linear regression in the following way, with c representing the direct effect (or the regression coefficient) between the predictor and the outcome. But what happens if we believe that there actually isn’t as direct of a relationship? In other words, we believe that our predictor X actually predicts/affects Y through a mediating variable, M? That might look like the following: In other words, mediation occurs when the effect of X on Y is explained through the effect of M. When paired with clever and careful study designs, mediations can be used to test causality. The underlying principle of the diagram above is that X causes M, and M causes Y; i.e. we implicitly specify directional relationships between our predictor, mediator and outcome. 13.1.2 In mathematical terms The basic idea of mediation is that the total direct effect, c, should reduce once the mediator is accounted for. This new direct effect is denoted as c’ (c-prime). If a mediating effect is significant, then c’ should be smaller than c; after all, if the mediator explains what is going on then this should explain the direct effect between the predictor and outcome. The a and b paths denote the relationship between the predictor and mediator (a), and the mediator and the outcome (b). Together, they denote the indirect effect of the mediator, which we denote as ab - literally, the product of the two coefficients. What is the relationship between the direct and indirect effect? Well, if the original effect c is the total effect, we can decompose this as: Total effect = direct effect (c’) + indirect effect (ab) Without a mediator, the indirect effect ab is equal to zero, and so we only have a direct effect. However, if the indirect effect fully explains the relationship between the predictor and the outcome then we would expect the direct effect (c’) to be close to 0, and the indirect effect to fully explain/comprise the total effect. "],["mediation-causal-steps.html", "13.2 Mediation: Causal steps", " 13.2 Mediation: Causal steps No introduction to mediation is complete without considering the causal steps approach, which held dominance as the main method of testing mediations for a while. Below is an overview of this approach. 13.2.1 The causal steps approach The causal steps approach outlined by Baron and Kenny (1986) was one of the first defined ways to test for a mediation. The name refers to a sequence of analytical steps that must be taken in order to apparently demonstrate a mediation, in line with the figure we saw on the previous page: To illustrate the causal steps approach (and subsequent approaches), we will use a fictional dataset below. This dataset contains scores on students’ grades, self-esteem and happiness. We theorise that the relationship between a student’s grades and their happiness is mediated by their self-esteem, which gives rise to the hypothetical mediation below: med_data &lt;- read_csv(here(&quot;data&quot;, &quot;medmod&quot;, &quot;med_grades.csv&quot;)) 13.2.2 Step 1: Test the direct effect (c) The first step in the classic causal steps approach is simply to see whether our predictor X predicts the outcome Y, using a linear regression. This tests the direct effect, c. The magnitude of this direct effect is 0.396. step_1 &lt;- lm(happiness ~ grade, data = med_data) summary(step_1) ## ## Call: ## lm(formula = happiness ~ grade, data = med_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.0262 -1.2340 -0.3282 1.5583 5.1622 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.8572 0.6932 4.122 7.88e-05 *** ## grade 0.3961 0.1112 3.564 0.000567 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.929 on 98 degrees of freedom ## Multiple R-squared: 0.1147, Adjusted R-squared: 0.1057 ## F-statistic: 12.7 on 1 and 98 DF, p-value: 0.0005671 13.2.3 Step 2: Test the predictor and mediator (a) The next step is to see whether the predictor significantly predicts the mediator, which denotes the a path. The estimate for a in this case is 0.561. step_2 &lt;- lm(self_esteem ~ grade, data = med_data) summary(step_2) ## ## Call: ## lm(formula = self_esteem ~ grade, data = med_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.3046 -0.8656 0.1344 1.1344 4.6954 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.49952 0.58920 2.545 0.0125 * ## grade 0.56102 0.09448 5.938 4.39e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.639 on 98 degrees of freedom ## Multiple R-squared: 0.2646, Adjusted R-squared: 0.2571 ## F-statistic: 35.26 on 1 and 98 DF, p-value: 4.391e-08 13.2.4 Step 3: Test the predictor and mediator on the outcome (b) This step involves running a multiple regression with both X and M as predictors, and Y as the outcome. This is because as in the model above, the effect of Y is determined by both the effect of the predictor and the mediator. The estimate for b is the regression coefficient for the mediator in this analysis - in this case, b = 0.6355. step_3 &lt;- lm(happiness ~ grade + self_esteem, data = med_data) summary(step_3) ## ## Call: ## lm(formula = happiness ~ grade + self_esteem, data = med_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.7631 -1.2393 0.0308 1.0832 4.0055 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.9043 0.6055 3.145 0.0022 ** ## grade 0.0396 0.1096 0.361 0.7187 ## self_esteem 0.6355 0.1005 6.321 7.92e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.631 on 97 degrees of freedom ## Multiple R-squared: 0.373, Adjusted R-squared: 0.3601 ## F-statistic: 28.85 on 2 and 97 DF, p-value: 1.471e-10 13.2.5 Step 4: Decide the extent of mediation Recall on the previous page that the idea of mediation is to see how close c, the direct effect, gets to zero after mediation (i.e. c’ = 0). The significance of the mediated direct effect, c’, as well as its magnitude are generally used to determine the extent to which this direct effect has been mediated. If full mediation has occurred, the effect of c’ should be non-significant and close to 0. If c’ is still significant and not close to 0, but all the other steps are significant, then partial mediation is said to have occurred. In our instance, the estimate of c’ is 0.0396, with a p-value of .719. Given that the c’ effect is non-significant, we would say that self-esteem fully mediates the relationship between grades and happiness. One final check - also recall that the total effect, c, can be broken down into the direct effect c’ and the indirect effect ab. You can manually check the maths here - ab (0.561 * 0.6355) + 0.0396 (the coefficient for c’) gives us 0.396, which was our initial estimate of the total effect c. 13.2.6 Problems with the causal steps approach The causal steps approach is not often used in mediation analyses anymore for a couple of reasons: Step 1 is not always necessary. A non-significant c does not necessarily indicate that there is no mediation - in fact, sometimes we can observe suppression effects, where the indirect effect is positive while the direct effect is negative (or vice versa). This can happen when the causal chains act in opposite directions, thereby cancelling out the overall effect. This approach does not really test the indirect path - only its individual components. Because we are interested in the effect of the mediating path, i.e. ab, it makes sense that we would want to know if that overall indirect effect is significant. However, this approach doesn’t actually test for that. Related to the above, Type II errors can be higher with this approach - some mediation effects can be missed. "],["mediation-sobels-test-and-bootstrapping.html", "13.3 Mediation: Sobel’s test and bootstrapping", " 13.3 Mediation: Sobel’s test and bootstrapping So the causal steps approach doesn’t really work for mediation… What now? Well, there are two more contemporary methods of testing mediations. While the latter (the bootstrap method) is the most recommended, we demonstrate both for completion’s sake. 13.3.1 Running mediations in R There are many packages available to run mediations in R, including the well-known package mediation package. One of the most popular software tools for running mediations (and moderations), particularly for SPSS users, is the PROCESS macro by Andrew Hayes. PROCESS is essentially a set of code that can run a series of what we call conditional process models - including mediation and moderation. Note that due to copyright reasons, the R version of process isn’t available on this book’s corresponding GitHub repo; however, it can be downloaded for free from the official site using the link above. Detailed documentation is available in Hayes (2022). Loading PROCESS is not quite like loading a package in R as it isn’t available in package form. To load it, you must use the source() function in base R, which will run another R script. Place the process.R file somewhere you can find it, and pass the file path to source(). It will take a little bit to load depending on your computer’s specs, but if it has loaded correctly then you should see the following: source(here(&quot;code&quot;, &quot;process.R&quot;)) ## ## ********************* PROCESS for R Version 4.3.1 ********************* ## ## Written by Andrew F. Hayes, Ph.D. www.afhayes.com ## Documentation available in Hayes (2022). www.guilford.com/p/hayes3 ## ## *********************************************************************** ## ## PROCESS is now ready for use. ## Copyright 2020-2023 by Andrew F. Hayes ALL RIGHTS RESERVED ## Workshop schedule at http://haskayne.ucalgary.ca/CCRAM ## Key to the PROCESS macro is the process() function, which is basically a does-it-all function that will run both mediations and moderations. 13.3.2 Sobel’s test The first, and perhaps most common method, goes by a couple of names: Sobel’s test or the delta method are perhaps the two most well known. Jamovi calls this the Standard method. Sobel’s test is a direct test of the significance of the ab pathway in a mediation. The principle behind Sobel’s test is actually relatively simple: it is analogous to a t-test, in that a t-statistic is calculated by dividing the estimate of ab by its estimated standard error: \\[ t_{ab} = \\frac{ab}{SE_{ab}} \\] From there, a p-value can be calculated by using this t-statistic and comparing it against a null t-distribution, much like you would with a t-test or a regression slope. Note that Sobel’s test assumes normality. To run a mediation, we use the process() function as follows: - data specifies the dataset. - x, y and m specify the predictor, outcome and mediator respectively. - model = 4 specifies that we want to run a mediation. (PROCESS comes with 20-odd different model types!) - normal = 1 and boot = 0 specifies that we only want a Sobel test for the indirect effect. med_sobel &lt;- process( data = med_data, y = &quot;happiness&quot;, x = &quot;grade&quot;, m = &quot;self_esteem&quot;, model = 4, normal = 1, boot = 0 ) ## ## ********************* PROCESS for R Version 4.3.1 ********************* ## ## Written by Andrew F. Hayes, Ph.D. www.afhayes.com ## Documentation available in Hayes (2022). www.guilford.com/p/hayes3 ## ## *********************************************************************** ## ## Model : 4 ## Y : happiness ## X : grade ## M : self_esteem ## ## Sample size: 100 ## ## ## *********************************************************************** ## Outcome Variable: self_esteem ## ## Model Summary: ## R R-sq MSE F df1 df2 p ## 0.5144 0.2646 2.6868 35.2586 1.0000 98.0000 0.0000 ## ## Model: ## coeff se t p LLCI ULCI ## constant 1.4995 0.5892 2.5450 0.0125 0.3303 2.6688 ## grade 0.5610 0.0945 5.9379 0.0000 0.3735 0.7485 ## ## *********************************************************************** ## Outcome Variable: happiness ## ## Model Summary: ## R R-sq MSE F df1 df2 p ## 0.6107 0.3730 2.6613 28.8524 2.0000 97.0000 0.0000 ## ## Model: ## coeff se t p LLCI ULCI ## constant 1.9043 0.6055 3.1452 0.0022 0.7026 3.1059 ## grade 0.0396 0.1096 0.3612 0.7187 -0.1780 0.2572 ## self_esteem 0.6355 0.1005 6.3212 0.0000 0.4360 0.8350 ## ## **************** DIRECT AND INDIRECT EFFECTS OF X ON Y **************** ## ## Direct effect of X on Y: ## effect se t p LLCI ULCI ## 0.0396 0.1096 0.3612 0.7187 -0.1780 0.2572 ## ## Indirect effect(s) of X on Y: ## Effect ## self_esteem 0.3565 ## ## Normal theory test for indirect effect(s): ## Effect se Z p ## self_esteem 0.3565 0.0829 4.2994 0.0000 ## ## ******************** ANALYSIS NOTES AND ERRORS ************************ ## ## Level of confidence for all confidence intervals in output: 95 Here is an output of our mediation using Sobel’s test. You can see that it gives a p-value of not just the individual paths, but of the indirect and direct paths overall. Our indirect effect is significant (z = 4.299, p &lt; .001), while our direct effect is not (z = .367, p = .719) - suggesting a full mediation, as we saw before. Note that the output helpfully adds a note stating that the “Normal theory test” was used, which corresponds to Sobel’s test. Sobel’s test is probably the most common method used, at least in most recent psychological literature. However, it too has a noticeable problem in that it only works well in large samples. This is for a couple of reasons, but primarily that the distribution of ab is only normal at very large sample sizes. Therefore, using Sobel’s test in smaller sample sizes is likely to be skewed, which leads to incorrect standard errors. 13.3.3 Bootstrapping To overcome the problem described above, another approach is to bootstrap the standard errors. This is where we continually resample a large number of times from our original dataset and calculate ab for each sampled dataset. Once we do this enough times, we can build an empirical distribution of possible ab values, and then use that to calculate an empirical p-value for each effect. The advantage of bootstrapping is that we can calculate more robust confidence intervals around our estimates without needing to assume normality - instead, these are directly derived from our data (in a sense). To that effect, here is an output from a bootstrapped mediation (specifically, a bias-corrected bootstrapping method). A couple of extra notes here: bc = 1 means we want bias-corrected bootstrap intervals. boot = 1000 means that we want to bootstrap/resample 1000 times. While this is ok for now (and relatively fast), depending on the data you may want to up this to 10000 or higher. seed = 2024 sets a seed for this run. Because bootstrapping involves random sampling, every run will give slightly different results (which isn’t good for replicability/transparency). Setting a seed locks R into generating the same values on every run. med_bootstrap &lt;- process( data = med_data, y = &quot;happiness&quot;, x = &quot;grade&quot;, m = &quot;self_esteem&quot;, model = 4, bc = 1, boot = 1000, seed = 2024 ) ## ## ********************* PROCESS for R Version 4.3.1 ********************* ## ## Written by Andrew F. Hayes, Ph.D. www.afhayes.com ## Documentation available in Hayes (2022). www.guilford.com/p/hayes3 ## ## *********************************************************************** ## ## Model : 4 ## Y : happiness ## X : grade ## M : self_esteem ## ## Sample size: 100 ## ## Custom seed: 2024 ## ## ## *********************************************************************** ## Outcome Variable: self_esteem ## ## Model Summary: ## R R-sq MSE F df1 df2 p ## 0.5144 0.2646 2.6868 35.2586 1.0000 98.0000 0.0000 ## ## Model: ## coeff se t p LLCI ULCI ## constant 1.4995 0.5892 2.5450 0.0125 0.3303 2.6688 ## grade 0.5610 0.0945 5.9379 0.0000 0.3735 0.7485 ## ## *********************************************************************** ## Outcome Variable: happiness ## ## Model Summary: ## R R-sq MSE F df1 df2 p ## 0.6107 0.3730 2.6613 28.8524 2.0000 97.0000 0.0000 ## ## Model: ## coeff se t p LLCI ULCI ## constant 1.9043 0.6055 3.1452 0.0022 0.7026 3.1059 ## grade 0.0396 0.1096 0.3612 0.7187 -0.1780 0.2572 ## self_esteem 0.6355 0.1005 6.3212 0.0000 0.4360 0.8350 ## ## *********************************************************************** ## Bootstrapping progress: ## | | | 0% | | | 1% | |&gt; | 1% | |&gt; | 2% | |&gt;&gt; | 2% | |&gt;&gt; | 3% | |&gt;&gt; | 4% | |&gt;&gt;&gt; | 4% | |&gt;&gt;&gt; | 5% | |&gt;&gt;&gt; | 6% | |&gt;&gt;&gt;&gt; | 6% | |&gt;&gt;&gt;&gt; | 7% | |&gt;&gt;&gt;&gt;&gt; | 7% | |&gt;&gt;&gt;&gt;&gt; | 8% | |&gt;&gt;&gt;&gt;&gt; | 9% | |&gt;&gt;&gt;&gt;&gt;&gt; | 9% | |&gt;&gt;&gt;&gt;&gt;&gt; | 10% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 10% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 11% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 12% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 12% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 13% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 14% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 14% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 15% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 15% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 16% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 17% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 17% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 18% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 19% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 20% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 20% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 21% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 22% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 22% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 23% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 23% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 24% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 25% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 25% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 26% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 27% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 27% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 28% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 28% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 29% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 30% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 30% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 31% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 32% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 33% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 33% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 34% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 35% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 35% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 36% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 36% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 37% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 38% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 38% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 39% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 40% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 40% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 41% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 41% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 42% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 43% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 43% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 44% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 44% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 45% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 46% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 46% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 47% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 48% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 48% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 49% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 49% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 50% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 51% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 51% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 52% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 52% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 53% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 54% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 54% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 55% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 56% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 56% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 57% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 57% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 58% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 59% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 59% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 60% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 60% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 61% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 62% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 62% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 63% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 64% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 64% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 65% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 65% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 66% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 67% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 67% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 68% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 69% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 70% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 70% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 71% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 72% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 72% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 73% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 73% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 74% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 75% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 75% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 76% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 77% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 77% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 78% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 78% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 79% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 80% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 80% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 81% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 82% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 83% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 83% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 84% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 85% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 85% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 86% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 86% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 87% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 88% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 88% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 89% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 90% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 90% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 91% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 91% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 92% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 93% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 93% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 94% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 94% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 95% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 96% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 96% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 97% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 98% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 98% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; | 99% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;| 99% | |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;| 100% ## ## **************** DIRECT AND INDIRECT EFFECTS OF X ON Y **************** ## ## Direct effect of X on Y: ## effect se t p LLCI ULCI ## 0.0396 0.1096 0.3612 0.7187 -0.1780 0.2572 ## ## Indirect effect(s) of X on Y: ## Effect BootSE BootLLCI BootULCI ## self_esteem 0.3565 0.0794 0.2237 0.5378 ## ## ******************** ANALYSIS NOTES AND ERRORS ************************ ## ## Level of confidence for all confidence intervals in output: 95 ## ## Number of bootstraps for bias-corrected bootstrap confidence intervals: 1000 Note that what changes here is the SE and confidence interval around each effect’s estimate. We can see that the 95% confidence interval [.224, .538] is different to what we observed in the Sobel test version of this analysis. These are likely to be more robust, and thus we generally will want to prefer bootstrapping when running mediations. "],["moderation-introduction.html", "13.4 Moderation: Introduction", " 13.4 Moderation: Introduction We start the second half of this module with an overview of what moderation is, and how it can be used. On the next page we will go through a worked example. 13.4.1 Refresher on interactions If you have completed the module on factorial ANOVAs, you’ll be familiar with the concept of an interaction: where the effect of one IV depends on the effect of another IV. At the time we dealt with interactions between categorical predictors, i.e. variables with discrete, defined and mutually exclusive groupings in the data - e.g. sex (male, female) vs treatment (drug, placebo). We essentially estimate mean scores for each combination of categories in our data, and compare the variouos combinations on our continuous outcome measure, leading to graphs like these: However, one thing we haven’t looked at in great detail are interaction effects between continuous predictors. 13.4.2 Moderation Moderation occurs when one variable influences the relationship between a predictor (X) and an outcome (Y). In contrast to a mediator, which we denote as M, we typically denote a moderator using W. Put simply, the existence of a moderator implies that the effect of X on Y depends on the value of W. That might sound familiar… and for good reason! If you thought that just sounds like an interaction effect, you’d be absolutely right. At a statistical/mathematical level, a moderation is simply an interaction between two continuous predictors, and thus you can think of moderation and interaction as the same thing. That means that we can define a moderation using a regression formula as follows: \\[ y = \\beta_0 + \\beta_1x + \\beta_2w + \\beta_3(xw) + \\epsilon_i \\] Where \\(\\beta_1\\) and \\(\\beta_2\\) and are regression coefficients (slopes) for X and W respectively, and \\(\\beta_3\\) is the interaction effect. However, you may have already clocked that this may not be as easy as it may be for a factorial ANOVA, where we deal with categorical predictors. After all, at least in the categorical instance we could group observations based on the combinations of the two predictors. For example, in a factorial ANOVA with sex as one of the variables, we could plot/test an effect between a predictor and an outcome for men and women separately. But how do we do this for continuous variables, where there are no ‘clear’ cutpoints? To unpack a moderation, there are two techniques we can apply. 13.4.3 Simple slopes The first approach is the simple slopes approach, which is similar in principle to simple effects tests after factorial ANOVAs. The basic idea of a simple slopes test is to calculate the predicted values between X and Y, for multiple levels of the moderator W. The standard approach is to take the mean value of the moderator W, as well as 1 SD above and below the mean, and calculate the regression slopes for each level of the moderator. We can then plot this as follows: From this graph, we can infer the nature of the interaction. In this example, for participants high on the moderator (red line), the relationship between X and Y is stronger; consequently, for participants low on the moderator (blue line), participants show a weaker relationship. Of course, we can also choose more points, e.g. if we also wanted to look at +/- 2SD, or at percentile-based cutpoints we could do so. 13.4.4 The Johnson-Neyman technique A critique of the simple slopes, or “pick-a-point” approach is that the selected points are relatively arbitrary. While +/- 1SD make sense as cutpoints, we tend to choose those in the absence of anything especially meaningful or precise. A second technique for probing a continuous interaction/moderation is called the Johnson-Neyman (JN) technique. The basic idea of the Johnson-Neyman technique is that it identifies the point at which the moderator no longer significantly interacts with the predictor. The exact maths behind this is complex, but in short it aims to identify the values of W that are equal to or below a critical t value for a significant interaction effect. We visualise the results of the Johnson-Neyman technique with a plot, which plots values of the moderator (W) on the x-axis against the slope of the predictor on the y-axis. Here’s an example (from the help docs for the interactions package in R: The red shaded region indicates where the moderation is non-significant. So, in this instance, values of the moderator between ~ 3.8 and 5.9 (as an eyeball guess) do not significantly moderate the relationship between the predictor and outcome. Values above or below this, however, do indicate significant moderation. Using the PROCESS macro in SPSS/R, it’s possible to get the exact values for where this range starts and ends. "],["moderation-example.html", "13.5 Moderation: Example", " 13.5 Moderation: Example To round this module off, here is a worked example of a moderation. 13.5.1 Example scenario We will use the dataset from Powell et al. (2022) in the Week 10 seminar, which looks at harmonious and obsessive passion in the context of heavy metal music listeners. We’ll test a specific question for this example: Do positive experiences predict harmonious passion, and does satisfaction with life moderate this relationship? passion_data &lt;- read_csv(here(&quot;data&quot;, &quot;week_10&quot;, &quot;W10_Powell_2022.csv&quot;)) 13.5.2 Setting up in R To set up a moderation using PROCESS, we turn to the process() function once again. This time, we want to set the following options: w sets our moderator. model = 1 indicates that we want a simple moderation. plot = 1 gives us values for plotting the moderation. jn = 1 calculates the Johnson-Neyman values for a significant moderation. These are also used for plotting. save = 2 is used to save the output to a variable. passion_mod &lt;- process( data = passion_data, y = &quot;HP_TOT&quot;, x = &quot;SPANE_pos&quot;, w = &quot;SWLS_TOT&quot;, model = 1, plot = 1, jn = 1, save = 2 ) 13.5.3 Output Let’s now take a look at our output. The first output is our overall model fit, which is identical to what we get for any other multiple regression. Our model in this instance is significant, and explains 19.6% of the variance in harmonious passion. The second output gives us our standard regression table. This tells us whether the moderation is significant, and we can interpret this as we would for any other regression we’ve seen. So, here we can see that positive experiences are a significant predictor of harmonious passion (B = 0.796, t = 6.290, p &lt; .001), but satisfaction with life is not (p = .279). However, the interaction - or moderation - between the two is significant (B = 0.036, t = 2.576, p = .011). ## ## ********************* PROCESS for R Version 4.3.1 ********************* ## ## Written by Andrew F. Hayes, Ph.D. www.afhayes.com ## Documentation available in Hayes (2022). www.guilford.com/p/hayes3 ## ## *********************************************************************** ## ## Model : 1 ## Y : HP_TOT ## X : SPANE_pos ## W : SWLS_TOT ## ## Sample size: 177 ## ## ## *********************************************************************** ## Outcome Variable: HP_TOT ## ## Model Summary: ## R R-sq MSE F df1 df2 p ## 0.4425 0.1958 32.2539 14.0427 3.0000 173.0000 0.0000 ## ## Model: ## coeff se t p LLCI ULCI ## constant 26.4430 5.7267 4.6175 0.0000 15.1398 37.7462 ## SPANE_pos 0.0960 0.2529 0.3796 0.7047 -0.4031 0.5951 ## SWLS_TOT -0.8896 0.3329 -2.6723 0.0083 -1.5466 -0.2325 ## Int_1 0.0358 0.0139 2.5757 0.0108 0.0084 0.0632 ## ## Product terms key: ## Int_1 : SPANE_pos x SWLS_TOT ## ## Test(s) of highest order unconditional interaction(s): ## R2-chng F df1 df2 p ## X*W 0.0308 6.6341 1.0000 173.0000 0.0108 ## ---------- ## Focal predictor: SPANE_pos (X) ## Moderator: SWLS_TOT (W) ## ## Conditional effects of the focal predictor at values of the moderator(s): ## SWLS_TOT effect se t p LLCI ULCI ## 12.0000 0.5256 0.1305 4.0288 0.0001 0.2681 0.7831 ## 20.0000 0.8120 0.1290 6.2931 0.0000 0.5573 1.0666 ## 28.0000 1.0984 0.2025 5.4242 0.0000 0.6987 1.4980 ## ## Moderator value(s) defining Johnson-Neyman significance region(s): ## Value % below % above ## 6.8962 4.5198 95.4802 ## ## Conditional effect of focal predictor at values of the moderator: ## SWLS_TOT effect se t p LLCI ULCI ## 5.0000 0.2750 0.1940 1.4176 0.1581 -0.1079 0.6579 ## 6.5000 0.3287 0.1778 1.8485 0.0662 -0.0223 0.6796 ## 6.8962 0.3429 0.1737 1.9738 0.0500 -0.0000 0.6857 ## 8.0000 0.3824 0.1627 2.3500 0.0199 0.0612 0.7035 ## 9.5000 0.4361 0.1490 2.9264 0.0039 0.1420 0.7302 ## 11.0000 0.4898 0.1371 3.5717 0.0005 0.2191 0.7604 ## 12.5000 0.5435 0.1276 4.2606 0.0000 0.2917 0.7952 ## 14.0000 0.5972 0.1209 4.9410 0.0000 0.3586 0.8357 ## 15.5000 0.6509 0.1175 5.5379 0.0000 0.4189 0.8829 ## 17.0000 0.7046 0.1179 5.9785 0.0000 0.4720 0.9372 ## 18.5000 0.7583 0.1218 6.2259 0.0000 0.5179 0.9987 ## 20.0000 0.8120 0.1290 6.2931 0.0000 0.5573 1.0666 ## 21.5000 0.8657 0.1390 6.2263 0.0000 0.5912 1.1401 ## 23.0000 0.9194 0.1513 6.0776 0.0000 0.6208 1.2179 ## 24.5000 0.9731 0.1652 5.8887 0.0000 0.6469 1.2992 ## 26.0000 1.0268 0.1805 5.6871 0.0000 0.6704 1.3831 ## 27.5000 1.0805 0.1969 5.4883 0.0000 0.6919 1.4690 ## 29.0000 1.1342 0.2140 5.3004 0.0000 0.7118 1.5565 ## 30.5000 1.1879 0.2317 5.1267 0.0000 0.7305 1.6452 ## 32.0000 1.2416 0.2499 4.9681 0.0000 0.7483 1.7348 ## 33.5000 1.2953 0.2685 4.8241 0.0000 0.7653 1.8252 ## 35.0000 1.3489 0.2874 4.6937 0.0000 0.7817 1.9162 ## ## Data for visualizing the conditional effect of the focal predictor: ## SPANE_pos SWLS_TOT HP_TOT ## 19.0000 12.0000 25.7538 ## 23.0000 12.0000 27.8561 ## 27.0000 12.0000 29.9584 ## 19.0000 20.0000 24.0785 ## 23.0000 20.0000 27.3264 ## 27.0000 20.0000 30.5742 ## 19.0000 28.0000 22.4032 ## 23.0000 28.0000 26.7966 ## 27.0000 28.0000 31.1900 ## ## ******************** ANALYSIS NOTES AND ERRORS ************************ ## ## Level of confidence for all confidence intervals in output: 95 ## ## W values in conditional tables are the 16th, 50th, and 84th percentiles. This indicates that we should investigate further, first with our simple slope/interaction plot as below. Unfortunately, as you can tell, the plot argument in process() doesn’t actually generate any plots. Rather, it gives a series of values for the predictor, moderator and outcome to plot. The same applies for the output from the Johnson-Neyman technique - while the output does tell you where the moderation is/is not significant, it does not draw a plot. This is probably because PROCESS was designed for SPSS users first and foremost in mind, and the SAS and R versions were designed for parity for SPSS first and foremost (rather than designed to take advantage of their native capabilities). There are ways to use these values for plotting, but truthfully - at least for this subject - they are far more effort than they are worth. Rather, there is a great package called interactions that will draw these plots for us. To start, we use lm() to build a regular regression model with an interaction between our predictor and moderator: library(interactions) passion_mod_lm &lt;- lm(HP_TOT ~ SPANE_pos * SWLS_TOT, data = passion_data) The interact_plot() function will then draw us a standard plot with simple slopes, based on the model above. For this function to work, at a minimum you must give it a) the name of the lm() model, b) the name of the predictor to pred and c) the name of the moderator to modx. The colors argument has also been specified to change the colours for the lines (by default they are different shades of blue). Based on the below graph, we can see that in all instances, the relationship between positive experiences and harmonious passion is positive. However, for people who are high on satisfaction with like (Mean + 1SD), the relationship is stronger - indexed by a greater slope. This relationship is weaker for people who are low on satisfaction with life (Mean - 1SD). interact_plot(passion_mod_lm, pred = &quot;SPANE_pos&quot;, modx = &quot;SWLS_TOT&quot;, colors = &quot;Set1&quot;) Lastly, we can examine the Johnson-Neyman plots to see where this relationship is non-significant. The same basic set of arguments - model, predictor, and moderator - can also be used as is for the johnson_neyman() function, which will draw a Johnson-Neyman plot. Helpfully, the function will also give a brief summary of the regions of significance/non-significance. Based on the plot and the text output, the moderation is non-significant when the moderator (satifaction with life) is below 6.9. johnson_neyman(passion_mod_lm, pred = &quot;SPANE_pos&quot;, modx = &quot;SWLS_TOT&quot;) ## JOHNSON-NEYMAN INTERVAL ## ## When SWLS_TOT is OUTSIDE the interval [-65.76, 6.90], the slope of ## SPANE_pos is p &lt; .05. ## ## Note: The range of observed values of SWLS_TOT is [5.00, 35.00] "],["non-parametric-tests.html", "Chapter 14 Non-parametric tests ", " Chapter 14 Non-parametric tests "],["parametric-vs-non-parametric-tests.html", "14.1 Parametric vs non-parametric tests", " 14.1 Parametric vs non-parametric tests 14.1.1 Introduction Recall the aim of much of the statistics we do: When we estimate these parameters using statistical tests, we make certain assumptions about data in order for our tests to be valid. Many of those assumptions involve some degree of normality - whether the data/outcome needs to be normally distributed, or the residuals in the model need to be normally distributed. The tests that we cover in this subject - t-tests and ANOVAs especially - are called parametric tests, because they make an assumption about the distribution. But what happens if those assumptions aren’t met? 14.1.2 Non-parametric tests Non-parametric tests do not make assumptions about the underlying distributions of data (and hence are sometimes called distribution-free tests). Instead, they are more general tests that make the following (broad) hypotheses: \\(H_0\\): The underlying distributions are equal \\(H_1\\): The underlying distributions are not equal So when should they be used, and what are their pros and cons? In general, non-parametric tests should be considered when a) assumptions for parametric tests are not met and b) you are working with small samples. As noted below, with large samples a lot of the parametric assumptions in tests are fairly robust (unless deviations are particularly severe). Below are the non-parametric equivalents to the major tests that we cover, with their associated datasets. Note that chi-square tests are already non-parametric, while non-parametric regression is just a headache. "],["spearmans-rho.html", "14.2 Spearman’s rho", " 14.2 Spearman’s rho 14.2.1 Introduction Spearman’s rho (\\(\\rho\\)) is a non-parametric correlation coefficient, broadly equivalent in interpretation with Pearson’s correlation coefficient. It is used to calculate a correlation in instances where Pearson’s r would not be appropriate; namely, when data is not linear. Spearman’s rho can be used when data is monotonic. Monotonic data is data where as X changes, Y changes in one direction. Below is a visualisation of monotonic versus non-monotonic data: In the linear and monotonic examples, the line of best fit follows one direction - up. In the rightmost graph, however, there is a decrease and then an increase in the predicted values of y. This is an example of a non-monotonic function. 14.2.2 Understanding ranks Non-parametric statistics, including all of the ones that we talk about in this module, rely on establishing ranks in your data. Mathematically, this is how non-parametric tests are able to test the hypotheses they do without needing to rely on accurately estimating some parameter, or making assumptions about said parameters. Although each test has a different way of using these ranks, many of them often start by calculating ranks in your data. This is about as simple as it sounds. Consider the table below, with 5 measurements on a simple scale. The left column shows the raw data, while the middle column shows the ranked data - largest to smallest, reading top to bottom. The right column shows their ranks in order. The smallest value is given a rank of 1. The ranks are then used to calculate test statistics for non-parametric tests. 14.2.3 Example Below is a simple example of a correlation between two variables where non-linearity may be important. Singing accuracy is naturally skewed heavily, and develops non-linearly throughout life. singing &lt;- read_csv(here(&quot;data&quot;, &quot;nonpara&quot;, &quot;singing_data.csv&quot;)) ## Rows: 284 Columns: 2 ## ── Column specification ──────────────────── ## Delimiter: &quot;,&quot; ## dbl (2): accuracy, age ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. The relevant dataset contains just two variables: accuracy (measured in cents) and age (participant age). The scatterplot below shows the relationship between these two variables: singing %&gt;% ggplot( aes(x = age, y = accuracy) ) + ggpubr::theme_pubr() + geom_point() To calculate Spearman’s rho, the steps are much the same as the way they are for normal correlations - we use the same cor.test() function that we saw earlier. The main difference is that we now must specify the method argument to equal to \"spearman\", which will calculate Spearman’s rho instead.. Here, we see results consistent with a Pearson’s correlation; a significant, positive association between age and accuracy (\\(\\rho\\) = .53, p &lt; .001). cor.test(singing$accuracy, singing$age, method = &quot;spearman&quot;) ## Warning in cor.test.default(singing$accuracy, singing$age, method = ## &quot;spearman&quot;): Cannot compute exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: singing$accuracy and singing$age ## S = 1808559, p-value &lt; 2.2e-16 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.5262664 "],["mann-whitney-u-tests.html", "14.3 Mann-Whitney U tests", " 14.3 Mann-Whitney U tests 14.3.1 Introduction A Mann-Whitney U (also called a Wilcoxon rank-sum test) is a non-parametric form of the independent samples t-test. In other words, it applies to situations where you are comparing two independent groups, and for whatever reason the assumptions of an independent t-test are severely violated. Note that many statistics webpages erroneously call the Mann-Whitney U a test of medians; this is not necessarily true (and even the distribution point is a little strained). The test is simply on the ranks of the data. 14.3.2 Hypotheses \\(H_0\\): The probability distributions of the two groups is the same (i.e. they derive from the same distribution). \\(H_1\\): The probability distributions of the two groups are not the same (i.e. they derive from different distribution). The test statistic is the U statistic. The U statistic ranges from 0 (which implies complete separation between the two groups) and n1 * n2 (the sample sizes of both groups multiplied). 14.3.3 Example The dataset for this page and the next relate to young men’s wages in 1980 and 1987 across the United States. The original study was interested in the effects of union bargaining/membership on wages. The following variables are in the dataset: nr: Participant ID year: year of measurement (1980 or 1987) school: Years of schooling exper: Years of work experience, calculated as school - 6 union: Was their wage set by collective bargaining? (two levels: yes, no) ethn: Participant ethnicity (three levels: black, hisp, other) married; Marital status (two levels: yes, no) health: Does the participant have a health problem? (two levels: yes, no) wage: Hourly wage, log-transformed industry, occupation, residence: Demographic and descriptive variables wages &lt;- read_csv(here(&quot;data&quot;, &quot;nonpara&quot;, &quot;wages.csv&quot;)) ## Rows: 1090 Columns: 13 ## ── Column specification ──────────────────── ## Delimiter: &quot;,&quot; ## chr (7): union, ethn, married, health, industry, occupation, residence ## dbl (6): ...1, nr, year, school, exper, wage ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Consider the following question: In 1980, were wages higher for union members than non-union members? Let’s take a look at the data in R first. First, let’s filter our dataset so that we only have cases from 1980. wages_1980 &lt;- wages %&gt;% filter(year == 1980) Pretend that we run our assumption checks on the wage data and obtain the following: wages_1980 %&gt;% group_by(union) %&gt;% shapiro_test(wage) wages_1980 %&gt;% levene_test(wage ~ union, center = &quot;mean&quot;) ## Warning in leveneTest.default(y = y, group = group, ...): group coerced to ## factor. Both assumptions have been violated. Now, pretend that we think this violation is bad enough that even a Welch test wouldn’t be appropriate. In this instance, we may turn to a Mann-Whitney U test. 14.3.4 Output TO run a Mann-Whitney U test, we use the wilcox.test() function in R. The wilcox.test() function behaves just like the regular t.test() function for both independent and paired-samples t-tests, down to the same notation. So, we can use the same notation for a independent-samples t-test as we have done so in the past: wilcox.test(wage ~ union, data = wages_1980) ## ## Wilcoxon rank sum test with continuity correction ## ## data: wage by union ## W = 19767, p-value = 2.898e-07 ## alternative hypothesis: true location shift is not equal to 0 Here is our output above. We can see that the p-value is significant, and so therefore these two samples (union vs non-union) do not appear to come from the same underlying distribution (Mann-Whitney U = 19767, p &lt; .001). We can then use descriptives as per normal to figure out where the difference is (the median and mean wages for union members are higher than non-members). We also want to calculate our effect size for this test, called the rank biserial correlation. We won’t worry too much about the maths here, but we can broadly interpret this along similar lines to Pearson’s r (weak to medium in this instance). To do this, we can use the rank_biserial() function in the effectsize package, which works like its cohens_d() counterpart: rank_biserial(wage ~ union, data = wages_1980) Something to note, though, is that unlike the standard t-test, how exactly this result should be interpreted is a little more vague. With a regular t-test, we test differences between two group means, and thus we can directly make a comparison between means when interpreting a test. In this instance, however, we are testing differences in ranks; this doesn’t have a clean interpretation beyond there just being a difference (of sorts) between the groups. "],["wilcoxon-signed-rank-test.html", "14.4 Wilcoxon signed rank test", " 14.4 Wilcoxon signed rank test 14.4.1 Introduction The non-parametric equivalent to the paired-samples t-test is the Wilcoxon signed-rank test. The sign and rank part of the test’s name comes from how the test statistic is calculated. We won’t deal too much with the mechanics of doing this, but it involves three main steps: Calculate the difference between condition 1 and condition 2 Rank each difference based on its absolute value (i.e. disregard whether it is positive/negative) Add up each set of signed differences (i.e. add all the positive differences together, and add all the negative ones together). The test statistic is the minimum of the two. In essence, the maths is exactly the same as a regular paired-samples t-test (i.e. it is a one-sample test on the differences between groups), but just using ranks this time rather than means. The Wilcoxon signed-rank test can be used to test whether the medians differ between the two conditions (i.e. it’s appropriate to hypothesise this here). Like the other non-parametric tests, it is a test that is free from assumptions about distributions. 14.4.2 Example In the wages dataset, there are wages between 1980 and 1987. Did the median wage change between these two years? Here are our descriptives: wages_wide &lt;- wages %&gt;% select(nr, year, wage) %&gt;% pivot_wider( id_cols = nr, names_from = year, values_from = wage, names_prefix = &quot;wage_&quot; ) Recall that in a paired-samples t-test, the normality assumption refers to whether the differences between the two conditions are normally distributed. We can test this in the usual two ways: 1) with a normality significance test, and 2) by assessing a Q-Q plot. Here is the former, to show what a non-normal dataset might look like: shapiro.test(wages_wide$wage_1980 - wages_wide$wage_1987) ## ## Shapiro-Wilk normality test ## ## data: wages_wide$wage_1980 - wages_wide$wage_1987 ## W = 0.88454, p-value &lt; 2.2e-16 As we can see, the test is significant (Shapiro-Wilks’ W = .885, p &lt; .001) - naturally, a tell-tale sign that this data aren’t normally distributed. This would be a good example to use Wilcoxon signed-rank tests over a regular paired t-test. 14.4.3 Output The setup for a signed-rank test in R again uses the same syntax as the regular t.test() function for a paired test - meaning that we can either give it the two separate columns with paired = TRUE, or use Pairs(a, b) ~ 1 notation. For simplicity we’ll just do the former: wilcox.test(wages_wide$wage_1980, wages_wide$wage_1987, paired = TRUE) ## ## Wilcoxon signed rank test with continuity correction ## ## data: wages_wide$wage_1980 and wages_wide$wage_1987 ## V = 15096, p-value &lt; 2.2e-16 ## alternative hypothesis: true location shift is not equal to 0 As mentioned on the previous page, we can also use our rank_biserial() function the same way to calculate an effect size for this paired test: rank_biserial(wages_wide$wage_1980, wages_wide$wage_1987, paired = TRUE) Here is our output. Our test is clearly significant, so we can reject the null and say that wages in 1987 were higher than wages in 1980 (W = 15096, p &lt; .001). Our effect size is also large this time (and negative, indicating that wages were higher in 1987 than 1980). "],["kruskal-wallis-anova.html", "14.5 Kruskal-Wallis ANOVA", " 14.5 Kruskal-Wallis ANOVA 14.5.1 Introduction The Kruskal-Wallis ANOVA is the non-parametric equivalent of the basic one-way ANOVA. It is essentially an extension of the Mann-Whitney U test, which has a couple of important ramifications: namely, it doesn’t assume any underlying distributions. Like the Mann-Whitney U test, by default the Kruskal-Wallis ANOVA is purely a test of whether the data in each group come from the same underlying distributions. The KW ANOVA can only test for a difference in medians if you can assume that each group’s distribution is the same shape and spread (e.g. all groups are skewed in the same way). Otherwise, you are essentially testing for a difference in the underlying distributions. 14.5.2 Example scenario The example data for this page and the next come from one data source, looking at language abilities in young children. These datasets contain the same participants, but the file labelled “autism_kw” takes data at one cross-section while the “autism_friedman” file contains four timepoints. The variables in this dataset include: childid: Participant ID sicdegp: Assessment of expressive language development. Three groups: high, medium, low. age2: Participant’s age, centered around 2 years old. The numeric values indicate how many years have passed since the child was 2 years old. In the &quot;autism_friedman&quot; dataset, the columns are labelled age_0, age_1, age_3 and age_7. These refer to the ages of 2 years old, 2yo, 5yo and 9yo respectively. vsae: Vineland Socialisation Age Equivalent gender, race: Participant’s gender and race bestest2 - Diagnosis at age 2. Two levels: autism and PDD (pervasive developmental disorder). We will take a look at the first autism dataset (“autism_kw”). In this dataset, we want to conduct an ANOVA comparing socialisation age equivalents (VSAE) between children with varying levels of expressive language development. autism &lt;- read_csv(here(&quot;data&quot;, &quot;nonpara&quot;, &quot;autism_long.csv&quot;)) %&gt;% mutate( sicdegp = factor(sicdegp) ) ## Rows: 63 Columns: 7 ## ── Column specification ──────────────────── ## Delimiter: &quot;,&quot; ## chr (4): sicdegp, gender, race, bestest2 ## dbl (3): childid, age2, vsae ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. 14.5.3 Checking assumptions (normal ANOVA) Here’s what a regular ANOVA would look like on this data - specifically, the assumption checks. We can see that both assumptions are violated; Levene’s test is significant (F(2, 60) = 4.57, p =. 014), and the Shaprio-Wilks test is too (backed up by a funky looking Q-Q plot; W = .86, p &lt; .001). autism_aov &lt;- aov(vsae ~ sicdegp, data = autism) # levene&#39;s test autism %&gt;% levene_test(vsae ~ sicdegp, center = &quot;mean&quot;) # Shapiro-wilks shapiro.test(autism_aov$residuals) ## ## Shapiro-Wilk normality test ## ## data: autism_aov$residuals ## W = 0.86118, p-value = 4.232e-06 autism_aov %&gt;% broom::augment() %&gt;% ggplot( aes(sample = .std.resid) ) + geom_qq() + geom_qq_line() + ggpubr::theme_pubr() Now, these aren’t too bad in general, but let’s assume for the sake of practice that these violations are severe enough that we would consider using non-parametrics instead. 14.5.4 Output To conduct a Kruskal-Wallis ANOVA in R, we can use the kruskal.test() function. This function works very similarly to aov(), meaning that we can provide the same notation as we normally would: autism_kw &lt;- kruskal.test(vsae ~ sicdegp, data = autism) autism_kw ## ## Kruskal-Wallis rank sum test ## ## data: vsae by sicdegp ## Kruskal-Wallis chi-squared = 28.474, df = 2, p-value = 6.562e-07 Note here that the test statistic in a Kruskal-Wallis is a chi-square distribution, with a df of g - 1 (where g = number of groups). This is sometimes called H, but is mathematically equivalent to the chi-square we are familiar with. Our overall result is significant (\\(\\chi^2\\)(2) = 28.47, p &lt; .001). The same notation is used for calculating effect sizes, using the rank_epsilon_squared() function from effectsize: rank_epsilon_squared(vsae ~ sicdegp, data = autism) Here, our non-parametric effect size is epsilon squared, \\(\\epsilon^2\\). It is not commonly seen so can be hard to interpret, but people have made various guidelines of their own. For posthocs, Jamovi uses Dwass-Steel-Critchlow-Fligner tests (phew!), or simply DCSF tests, for ‘post hoc’ pairwise comparisons. We’ll use the same here for compatibility with Jamovi. The only thing you really need to know about these comparisons is that they have an in-built correction for the family-wise error rate, so do not need adjusting after the analyses have been run. To run these tests, we need a new package called PMCMRplus. Within this package is a function called dscfAllPairsTest(), which will give us the p-values for each pairwise comparison. The required code is exactly the same as we have used above: PMCMRplus::dscfAllPairsTest(vsae ~ sicdegp, data = autism) ## ## Pairwise comparisons using Dwass-Steele-Critchlow-Fligner all-pairs test ## data: vsae by sicdegp ## high low ## low 5.4e-06 - ## med 0.0002 0.0635 ## ## P value adjustment method: single-step We can see that there is a significant difference in socialisation age between children with high versus low expressive language (p &lt; .001), as well as between children with high versus medium expressive language (p &lt; .001). However, there is no significant difference in socialisation age between children with low and medium expressive language (p = .064). "],["friedman-anovas.html", "14.6 Friedman ANOVAs", " 14.6 Friedman ANOVAs 14.6.1 Introduction The Friedman ANOVA (or Friedman test) is the non-parametric equivalent of a one-way repeated measures ANOVA. The idea behind the Friedman’s ANOVA is the same as its parametric counterpart, namely to test whether there are differences in treatments across multiple time points. 14.6.2 Example scenario We will take a look at the autism dataset again, albeit with a new version called autism_wide.csv. In this scenario, we now want to see how expressive language changes over time. Note that the variables relating to age are referenced/centered to 2 years old. That is, a value of age2 of 0 indicates the child is 2 years old; a value of 1 refers to 3 years old and a value of 3 refers to 5 years old. autism_wide &lt;- read_csv(here(&quot;data&quot;, &quot;nonpara&quot;, &quot;autism_wide.csv&quot;)) ## Rows: 63 Columns: 9 ## ── Column specification ──────────────────── ## Delimiter: &quot;,&quot; ## chr (4): sicdegp, gender, race, bestest2 ## dbl (5): childid, age_0, age_1, age_3, age_7 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Reshape into long format autism &lt;- autism_wide %&gt;% pivot_longer( cols = age_0:age_7, names_to = &quot;age2&quot;, values_to = &quot;vsae&quot;, names_prefix = &quot;age_&quot; ) %&gt;% mutate( age2 = factor(age2) ) 14.6.3 Checking assumptions (normal ANOVA) Like last time, let’s examine our assumptions using a normal repeated-measures ANOVA. We can see that our sphericity assumption is violated (p &lt; .001), and very severely so; recall that the W statistic in Mauchly’s test is a deviation from 1, so our test statistic of W = .047 is very low! This might be one instance where we would legitimately consider running a Friedman ANOVA, if we weren’t keen on applying such a strong Greenhouse-Geisser correction to our ANOVA. autism %&gt;% anova_test(dv = vsae, within = age2, wid = childid) ## ANOVA Table (type III tests) ## ## $ANOVA ## Effect DFn DFd F p p&lt;.05 ges ## 1 age2 3 186 48.401 3.62e-23 * 0.286 ## ## $`Mauchly&#39;s Test for Sphericity` ## Effect W p p&lt;.05 ## 1 age2 0.047 4.88e-38 * ## ## $`Sphericity Corrections` ## Effect GGe DF[GG] p[GG] p[GG]&lt;.05 HFe DF[HF] p[HF] ## 1 age2 0.434 1.3, 80.71 2.02e-11 * 0.439 1.32, 81.73 1.55e-11 ## p[HF]&lt;.05 ## 1 * 14.6.4 Running a Friedman ANOVA Friedman ANOVAs are available in R with the friedman.test() function. This function requires long data, and by and large even uses the same formula notation that we are used to. The only difference is that because this is repeated measures data, we must tweak the formula slightly to account for this. The formula needs to take outcome ~ predictor | id notation, where id needs to point to a column in the dataset that simply indicates each participant’s unique ID. Otherwise, the formula is much as you would expect: friedman.test(vsae ~ age2 | childid, data = autism) ## ## Friedman rank sum test ## ## data: vsae and age2 and childid ## Friedman chi-squared = 133.51, df = 3, p-value &lt; 2.2e-16 Also like the Kruskal-Wallis ANOVA, R will report a chi-square as the test statistic. This is for the same reason as before; the actual test statistic Q approximates a chi-square distribution with large enough samples (e.g. n &gt; 15). We can see that our omnibus result is significant (\\(\\chi^2\\)(3) = 133.51, p &lt; .001). Although Jamovi does not give an effect size for a Friedman ANOVA, there actually is one called Kendall’s W. The effectsize package provides a function called - you guessed it - kendalls_w() to calculate this. The notation is the same as friedman.test(). kendalls_w(vsae ~ age2 | childid, data = autism) ## Warning: 9 block(s) contain ties. Of course, we still need to do post-hoc pairwise comparisons. The post-hocs that Jamovi provides are called Durbin-Conover pairwise comparisons, which are simply called Durbin tests elsewhere. The PMCMRplus package we mentioned earlier provides a function called durbinAllPairsTest() to conduct these posthocs. Note that although the function can be run without assignment, this will only give p-values; to get the proper output, we will want to assign the function’s output to a variable and then run summary() on this. This function will also allow you to use p-value adjustment methods. Holm adjusted p values are the default, which we will run with. autism_posthoc &lt;- PMCMRplus::durbinAllPairsTest(y = autism$vsae, groups = autism$age2, blocks = autism$childid) summary(autism_posthoc) ## ## Pairwise comparisons using Durbin&#39;s all-pairs test for a two-way balanced incomplete block design ## data: autism$vsae, autism$age2 and autism$childid ## P value adjustment method: holm ## H0 ## t value Pr(&gt;|t|) ## 1 - 0 == 0 7.317 2.2075e-11 *** ## 3 - 0 == 0 14.189 &lt; 2.22e-16 *** ## 7 - 0 == 0 19.979 &lt; 2.22e-16 *** ## 3 - 1 == 0 6.872 1.8542e-10 *** ## 7 - 1 == 0 12.662 &lt; 2.22e-16 *** ## 7 - 3 == 0 5.790 2.9470e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The left column of this output is denoting a specific hypothesis being tested. For example, “1 - 0 == 0” means that it is testing whether the difference between age2 = 1 and age2 = 0 is equal to 0. The corresponding columns to the right give the test statistic and the p-value. Based on our results, we can see that all comparisons are significant (p &lt; .001). To interpret this, the most useful way would be to draw a plot and go back to the main descriptives, to infer that there is a significant increase or decrease in expressive language with age. "],["colours.html", "A R colour palettes", " A R colour palettes "],["broom.html", "B The broom() package", " B The broom() package The broom() package provides a couple of helper functions for tidying up output from numerous R functions for running tests. This can be useful in a couple of instances, particularly for either a) accessing certain aspects of models that are not immediately accessible or b) simply for neater manipulation for multiple models. While broom is a part of the tidyverse, it is not one of the tidyverse’s default packages. Therefore, to use the package you need to manually call it: library(broom) For the examples on this page, we will use datasets from the datarium package (which we have seen before), just because it provides a nice set of datasets for demonstrating how these functions work. So let’s go ahead and load that too: library(datarium) The main functions in broom are generic functions with what R calls several methods - i.e. ways that the function handles different types of data or objects. Every method comes with a different set of arguments that can change the output, depending on what object the function is run on. For example, if you use tidy() on an lm object, you get different optional arguments than for an aov() object, and so on. The next page gives examples using the datarium datasets. "],["broom-tidy.html", "B.1 The tidy() function", " B.1 The tidy() function You may have noticed that many of the outputs from common R functions, like lm() and aov(), print their results in a certain format - namely, it essentially prints as text. The tidy() function will simply turn the core output into a data frame. This is useful when you are running multiple models at once, or if for some reason you want to work with the values in model outputs directly. This function will work with just about every standard test function you get in R. Here an example with the marketing dataset, which contains continuous variables. Let’s fit a multiple regression using lm(), and call summary() on the results: data(marketing) marketing_lm &lt;- lm(sales ~ youtube + facebook + newspaper, data = marketing) summary(marketing_lm) ## ## Call: ## lm(formula = sales ~ youtube + facebook + newspaper, data = marketing) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.5932 -1.0690 0.2902 1.4272 3.3951 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.526667 0.374290 9.422 &lt;2e-16 *** ## youtube 0.045765 0.001395 32.809 &lt;2e-16 *** ## facebook 0.188530 0.008611 21.893 &lt;2e-16 *** ## newspaper -0.001037 0.005871 -0.177 0.86 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.023 on 196 degrees of freedom ## Multiple R-squared: 0.8972, Adjusted R-squared: 0.8956 ## F-statistic: 570.3 on 3 and 196 DF, p-value: &lt; 2.2e-16 tidy() works directly on model objects, not raw data, so we use the tidy() function on our regression model. As you can see, the data is now in data frame format: tidy(marketing_lm) For lm() objects, you can return a confidence interval on the regression coefficients: tidy(marketing_lm, conf.int = TRUE, conf.level = 0.95) B.1.1 Correlations For a correlation: marketing_cor &lt;- cor.test(marketing$youtube, marketing$facebook) marketing_cor ## ## Pearson&#39;s product-moment correlation ## ## data: marketing$youtube and marketing$facebook ## t = 0.77239, df = 198, p-value = 0.4408 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.08457548 0.19208899 ## sample estimates: ## cor ## 0.05480866 tidy(marketing_cor) B.1.2 Chi-squares A chi-square test object: data(&quot;properties&quot;) properties_table &lt;- table(properties$property_type, properties$buyer_type) properties_chisq &lt;- chisq.test(properties_table, correct = FALSE) properties_chisq ## ## Pearson&#39;s Chi-squared test ## ## data: properties_table ## X-squared = 82.504, df = 9, p-value = 5.134e-14 tidy(properties_chisq) B.1.3 t-tests For a t-test object (applies to all t-tests): data(&quot;genderweight&quot;) weight_t &lt;- t.test(weight ~ group, data = genderweight) weight_t ## ## Welch Two Sample t-test ## ## data: weight by group ## t = -20.791, df = 26.872, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means between group F and group M is not equal to 0 ## 95 percent confidence interval: ## -24.53135 -20.12353 ## sample estimates: ## mean in group F mean in group M ## 63.49867 85.82612 tidy(weight_t) B.1.4 ANOVA objects For a regular aov() object, you can optionally ask for the intercept term using intercept = TRUE: data(&quot;stress&quot;) stress_aov &lt;- aov(score ~ treatment * exercise, data = stress) summary(stress_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treatment 1 351.4 351.4 12.295 0.000923 *** ## exercise 2 1776.3 888.1 31.076 1.04e-09 *** ## treatment:exercise 2 217.3 108.7 3.802 0.028522 * ## Residuals 54 1543.3 28.6 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 tidy(stress_aov) Objects fitted by the Anova package also work, but repeated measures designs do not work. It’s best to stick to rstatix if you want a repeated measures ANOVA in dataframe format. TukeyHSD() can also be used: TukeyHSD(stress_aov) %&gt;% tidy() emmeans objects can also be tidied, e.g. for simple effects tests: # Simple effects of exercise for every treatment emmeans(stress_aov, ~ exercise, by = &quot;treatment&quot;) %&gt;% pairs() %&gt;% tidy() "],["broom-glance.html", "B.2 The glance() function", " B.2 The glance() function The glance() function will generate model fit summaries from models. It works primarily with lm() and other models, and returns several indices of model fit (depending on the original object). For lm() objects, importantly, it returns \\(R^2\\) and adjusted \\(R^2\\). IT also returns estimates for the AIC and BIC, as well as some other fit statistics. glance(marketing_lm) glance() also works for aov() objects, but is perhaps less useful. glance(stress_aov) "],["broom-augment.html", "B.3 The augment() function", " B.3 The augment() function The a B.3.1 Chi-square tests Using augment() on a chi-square object will print out a dataframe containing both the expected and the observed proportions for each cell. augment(properties_chisq) B.3.2 Regressions augment() is probably most useful for regression models. It will print out the following: .fitted is the predicted score for each participant .resid is the residual for each participant (i.e. actual - fitted) .cooksd is Cook’s distance, which is useful for outlier detection in some contexts augment(marketing_lm) "],["technical-details-of-anova.html", "C Technical details of anova()", " C Technical details of anova() ## Rows: 811 Columns: 6 ## ── Column specification ──────────────────── ## Delimiter: &quot;,&quot; ## dbl (6): id, age, GoldMSI, DFS_Total, trait_anxiety, openness ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Technical note: this test works not too unlike a regular ANOVA, except the F-test is being conducted on the residual sums of scores. From a mathematical point of view, we are essentially conducting an F-test on the change in the residual sum of squares with the following formula: \\[ F(df_{df_b - df_a}, df_a) = \\frac{MS_{comp}}{MS_{a}} = \\frac{(SS_b - SS_a)/(p_a - p_b)}{SS_a/df_a} \\] Consider two models, Model A and Model B. Imagine Model B is a nested version of Model A - i.e. it it the same model as Model A but with less predictors. In our case, imagine Model B is flow_block1 (which only had one predictor) and Model A is flow_block2 (which had two). \\(p_a\\) is the number of coefficients in Model A including the intercept, and same with \\(p_b\\). The exact process is: Calculate the difference between residual SS in the two models - this is the \\((SS_b - SS_a)\\) part of the formula above. This is just the difference in RSS between model 1 (model B) and 2 (model A), i.e. 10124.2 - 9982.9 = 141.3. Calculate the difference in df \\((p_a - p_b)\\). In flow_block1 we have one predictor and one intercept, so we have 2 terms - this is \\(p_b\\). In flow_block2 we have two predictors and one intercept, which makes \\(p_a = 3\\). Therefore, \\((p_a - p_b) = 3 - 2 = 1\\). Calculate a mean square ratio for the comparison, which is \\(MS_{comp}\\). Essentially, we divide the result in step 1 (143.1) by the result in step 2 (1). This follows the same formula for mean squarews as we have seen before: \\(MS = \\frac{SS_{comp}}{df_{comp}}\\), so \\(MS_{comp} = \\frac{141.3}{1} = 141.3.\\) While this is identical to the sum of squares value in the table above, note that this is not the same value. Calculate a mean square ratio between RSS and df for the new model. This is the \\(SS_a/df_a\\) part of the equation. \\(df_a\\) is calculated as \\(n - p_a\\), where n is the original sample size. So \\(df_a = 811 - 3 = 808\\). Note that the value for row 2 (which corresponds to Model A/flow_block2) under Res.df is 808. Note that \\(df_b\\) is the same; \\(n - p_b = 811 - 2 = 809\\). Same deal as above after that, except this time we use the values from the new model only, i.e. residual SS for model A (flow_block2) and the residual df. \\(MS_a = \\frac{SS_{a}}{df_{a}}\\) \\(MS_a = \\frac{9982.9}{808} = 12.33507\\) Calculate an F ratio between the MS of the comparison and the MS of the new model to calculate a value for F. This is exactly the same formula as it would be for a regular ANOVA, just that now we are doing: \\[ F = \\frac{MS_{comp}}{MS_{a}} \\] \\(F = \\frac{141.3}{12.33507} = 11.45514\\) Calculate a p-value for this F-statistic by comparing the p against an F distribution. The two dfs in the original formula are a) \\(df_b - df_a\\) and b) \\(df_a\\). Which means: \\(df_b\\) is 809 - \\(df_a\\) is 808 = 1 \\(df_a\\) is 808 So we end up with a test statistic of \\(F(1, 808) = 11.455\\). We can use this to calculate a p-value by calculating the probability of getting a value of at least 11.455, on an F distribution with degrees of freedom parameters described above. We can visualise this below. Note that because the values for F are so infinitesimally small with these parameters and at this F-value, I’ve zoomed in the plot to visualise the highlighted area: tibble( x = seq(0, 15, by = .5), y = df(x, df1 = 1, df2 = 809) ) %&gt;% ggplot( aes(x = x, y = y) ) + geom_line(linewidth = 1) + theme_pubr() + geom_vline(xintercept = 11.455, linewidth = 1, colour = &quot;royalblue&quot;) + annotate(&quot;text&quot;, x = 12, y = 0.0024, label = &quot;F = 11.455&quot;) + stat_function(fun = df, args = list(df1 = 1, df2 = 809), geom = &quot;area&quot;, xlim = c(11.455, 15), fill = &quot;royalblue&quot;, alpha = 0.5) + scale_x_continuous(expand = c(0, 0), limits = c(10, NA)) + scale_y_continuous(expand = c(0, 0), limits = c(0, 0.003)) + labs(x = &quot;F-value&quot;, y = &quot;Density&quot;) ## Warning: Computation failed in `stat_function()`. ## Caused by error in `fun()`: ## ! could not find function &quot;fun&quot; ## Warning: Removed 20 rows containing missing values ## or values outside the scale range ## (`geom_line()`). R can manually calculate a p-value with the pf() function. pf() will calculate the probability of a value on the F distribution, given the two degrees of freedom parameters to characterise the distribution. lower.tail = FALSE is used to indicate that we want to calculate the probability of getting something above our critical F-value; lower.tail = TRUE would calculuate the probability below it. pf(11.45514, df1 = 1, df2 = 808, lower.tail = FALSE) ## [1] 0.0007473355 Note that our p-value isn’t exactly the same as the value in the table - this is because we’ve used rounded values. The code below extracts the unrounded values and uses them in the calculations. As you can see we get the exact p-value in the table. x &lt;- anova(flow_block1, flow_block2) x # Using the output from anova() to manually calculate p-value SSb &lt;- x$RSS[1] SSa &lt;- x$RSS[2] pa &lt;- length(coef(flow_block2)) pb &lt;- length(coef(flow_block1)) dfa &lt;- nrow(w10_flow) - pa dfb &lt;- nrow(w10_flow) - pb f_val &lt;- ((SSb-SSa)/(pa-pb))/(SSa/dfa) pf(f_val, df1 = dfb-dfa, df2 = dfa, lower.tail = FALSE) ## [1] 0.0007546911 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
