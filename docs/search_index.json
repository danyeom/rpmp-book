[["exploratory-factor-analysis.html", "Chapter 12 Exploratory factor analysis ", " Chapter 12 Exploratory factor analysis "],["introduction-to-efa.html", "12.1 Introduction to EFA", " 12.1 Introduction to EFA We generally conduct a survey because we’re interested in how different people hold different attitudes, ideas or beliefs about things. All of the standard statistical tools that you learned in Modules 5 and 6 are useful in this regard, and can absolutely be used with survey data. But given how highly dimensional survey data is, it opens up a new way of analysing data - exploratory factor analysis. 12.1.1 Dimension reduction Sometimes it’s easy to forget that designing a questionnaire and administering it gives rise to quite complex data. After all, a single question item may simply ask participants to rate themselves on a statement using a Likert scale, as we have seen earlier in this module. However, consider how many questions we might collect across a scale and it quickly becomes evident that this kind of data is highly dimensional - that is, with lots of individual questions we end up with a lot of data to sift through. This kind of scenario is where dimension reduction techniques become extremely useful. Dimension reduction techniques allow us to essentially collapse data into ‘supervariables’ that can simplify the analyses that we do by capturing the commonalities across questionnaire items. Principal components analysis (PCA) is the most common form of dimension reduction. Principal components analysis lets us take highly dimensional data, such as a questionnaire/scale with multiple items, and collapse that down into a smaller number of components. 12.1.2 Factor analysis Factor analysis is another analytical technique like dimension reduction. However, the key conceptual difference is that while PCA lets us collapse multiple variables into a smaller number of components, factor analysis lets us identify latent factors in our data. Latent factors are the factors underlying the behaviours and responses that we observe in our questionnaire items. We might find, for example, that performance on a series of tests is actually underlaid by multiple distinct, theoretically meaningful factors. Therefore, factor analysis lets us build and test theories about latent psychological constructs. Factor analysis allows us to indirectly measure these latent factors - essentially, are our questions tapping into the same ‘thing’? Factor analysis can be split into exploratory factor analysis (EFA) and confirmatory factor analysis (CFA). We will focus specifically on EFA in this module. A terminology note You’ll note that we’ve described PCA as generating components, while FA generates factors. It’s worth remembering that this is intentional, and the two terms should not be used interchangeably. We will talk more about this on the pages that follow, but components are simply linear combinations of multiple variables. Factors are estimates of latent variables that drive behaviour. The latter specifically is what we use to test theories about psychological constructs. 12.1.3 The steps of exploratory factor analysis EFA is quite an involved analysis, and there are several considerations that must be taken into account: Prepare data and assess for suitability Decide on the extraction method Decide on how many factors to retain Decide on the rotation method Interpret the results 12.1.4 Example dataset The example we’ll be using to work through this data is from a brilliant statistician and educator, Professor Andy Field, who is very highly regarded for his Discovering Statistics series - including Discovering Statistics with SPSS and Discovering Statistics with R. I highly recommend checking them out if you plan on using them! As part of his book, Prof. Field came up with a questionnaire called the SPSS Anxiety Questionnaire (SAQ). For the purposes of the next few pages we’ll be using a reduced version with just 9 questions, which we’ll call the SAQ-9. The questions in this survey are: Q1: Statistics makes me cry Q2: My friends will think I’m stupid for not being able to cope with SPSS Q4: I dream that Pearson is attacking me with correlation coefficients Q5: I don’t understand statistics Q6: I have little experience of computers Q14: Computers have minds of their own and deliberately go wrong whenever I use them Q15: Computers are out to get me Q19: Everybody looks at me when I use SPSS Q22: My friends are better at SPSS than I am saq &lt;- read_csv(here(&quot;data&quot;, &quot;efa&quot;, &quot;SAQ-9.csv&quot;)) ## Rows: 2571 Columns: 9 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (9): q01, q02, q04, q05, q06, q14, q15, q19, q22 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. As we walk through the content, we will use this dataset to illustrate how to conduct an exploratory factor analysis. (Note the questions were specifically chosen for demonstration purposes.) To actually conduct the EFA, we will primarily rely on two packages: psych and EFA.dimensions. The psych package is a fairly big package designed to run many common analyses in psychological science, specifically analyses that relate to psychometrics. It’s an incredibly useful package to be aware of in general. EFA.dimensions is another great package that provides functions to help with certain parts of the EFA process. "],["theory-of-efa.html", "12.2 Theory of EFA", " 12.2 Theory of EFA On this page, we (briefly) touch on a bit of the statistical theory underlying factor analysis and PCA. We won’t dive too deeply into the maths underlying EFA, but will focus on the high-level conceptual stuff. 12.2.1 PCA vs EFA Here is a good point to formally differentiate PCA vs EFA, following on from the brief disclaimer on the previous page. Both PCA and EFA will extract up to k factors that attempt to explain the observed variables. k, in this instance, is capped at the number of observed variables; so, if you input 24 variables into a PCA, the maximum number of components that you can estimate is 24. However, what defines these components/factors differs: In PCA, where we are just interested in collapsing variables, we assume that all variance in the observed variables is explained by the factors. Ultimately, a PCA will extract k components that ultimately explain 100% of the variance in all of the observed variables. In EFA, the goal is only to explain common variance between the observed variables. EFA explicitly models the variance in the items to be comprised of common variance, which is variance due to shared underlying factors, and unique variance. Unique variance can be further broken down into specific variance, which is variance that is specific to each item, and error variance. By partitioning variance in this way, EFA is able to test models of factors as it allows for disconfirmation - a vital part of any model testing. In short, EFA allows us to generate theoretical entities that we can test in subsequent analyses. PCA only provides us with a measure that combines the effects of multiple variables, but does not test latent factors. The common factor model The basis of factor analysis is the common factor model. Broadly speaking, the common factor model suggests that variables that relate to each other are likely driven by underlying latent variables. For example, if five questions in a survey all ask about a specific aspect of motivation, and these items correlate with each other, we would expect the same thing - or latent factor - to be driving responses on these five items. Exploratory factor analysis operates under this common factor model. Below is a basic diagram of the common factor model. The squares represent observed variables, which are the variables that we measure. The big circle denotes the latent factor that we want to estimate. The circles leading to the observed variables are error terms. In an exploratory factor analysis, the primary thing we want to investigate is the factor loadings, denoted by the various lambdas (\\(\\lambda\\)). We will see precisely what the factor loadings are later, but generally they are how strongly the latent factor predicts each observed variable. Now let’s take a look at the common factor model with two factors. As you can see, we allow every observed variable to load onto every factor - thus, we estimate factor loadings for every possible path going from latent factors to observed variables. This is what we estimate in exploratory factor analysis (and PCA - sort of). For brevity’s sake, only three lambdas have been shown, but hopefully they are illustrative enough to get the general gist across. Every path leading from a latent variable to an observed variable is a parameter to be estimated in an exploratory factor analysis. 12.2.2 Partial correlations In Module 10, we talked about the concept of correlation - i.e. how related two variables are. Recall that correlation coefficients are scaled from -1 to 1. Now we move to the concept of partial correlation - the relationship between two variables while controlling for a third. Let’s come back to the flow data in Week 10. Imagine we want to test the correlation between Gold MSI scores and flow proneness. However, we might suspect that openness to experience may play a role in the relationship between these variables. One thing we could do is to calculate a partial correlation between MSI scores and flow proneness while controlling for openness. If we know the correlation between: MSI scores and flow proneness, MSI scores and openness, Flow proneness and openness, We could calculate the partial correlation between Gold-MSI and flow proneness, controlling for openness, using the below formula: Both PCA and EFA rely on estimating partial correlations. Specifically, factor analysis aims to estimate latent factors that minimise the partial correlations among observed variables. If a latent factor perfectly explains the relationship between two variables, the partial correlations between the observed variables should be zero. A lot of the ‘under the hood’ maths, which we won’t touch on, essentially relates to identifying the latent factors that maximise the amount of variance explained in each variable by the factor solutions. "],["initial-considerations-for-efa.html", "12.3 Initial considerations for EFA", " 12.3 Initial considerations for EFA We’ll start with some basic considerations for EFA/PCA. These are generally things that should be thought about/considered before an EFA, or at least before you interpret the results. 12.3.1 Sample size For adequate power, EFA typically needs a fairly big sample size. There is no clear agreement about what constitutes a ‘good’ sample size, and it’s difficult to give concrete recommendations. Many guides and sources will often mention a n:p rule of thumb, where n is sample size and p is number of variables (which is the number of parameters that needs to be estimated). The idea is that an ideal EFA sample size will have n participants for every variable you are analysing. These can range from as low as 3:1 to 20:1, with a typical ‘ok’ range being from 10-20:1. However, there is no clear support for these rules, and no minimum is truly sufficient (Hogarty et al. 2005). In general - the bigger the better, and the more variables you have the more participants you need. Aim for at least 300+ no matter the circumstance (this is a very blunt rule of thumb!). Below are our descriptives. While we can use a bit of tidyverse to get descriptives, the describe() function from the psych package is also convenient for getting basic descriptives for every column in a dataset. We can see that we have n = 2571, which should be more than adequate. describe(saq) 12.3.2 Assumptions EFA can be conducted with one of multiple algorithms that determines the final factor structure to be extracted. Different methods rely on different assumptions, so basic assumption checks are useful: Data must be interval/ratio data. Ordinal data is problematic unless you generally have at least 5 scale points; in which case, you can broadly approximate this to be continuous. Normality is important, depending on the method. The usual QQ-plot or Shapiro-Wilks tests on individual items can be useful here.a Multicollinearity: observed variables should not be collinear with each other. Note though that this only tests the normality of one variable, i.e. univariate normality. EFA is ideal with multivariate normality, i.e. the joint dimensions of the entire dataset are normally distributed. Univariate normality is necessary but not sufficient for multivariate normality. Sadly, however, Jamovi doesn’t give an easy way to test multivariate normality so let’s run with univariate normality for now. 12.3.3 Factorability Factorability broadly describes whether the data are likely to be amenable to factor analysis. If data are factorable, it suggests that there is likely to be at least one latent factor underlying the observations. We can test factorability in three ways: Correlations A simple matrix of correlations can give us a first-pass indication of factorability. If most items correlate with each other, this can indicate that there are underlying latent factors. There is no hard and fast rule for what counts as ‘acceptable’, but if most variables are not significantly correlated that indicates that the data may not be factorable. In our SAQ-9 data, we can see that all correlations between variables are significant, which is generally a good sign. cor(saq) ## q01 q02 q04 q05 q06 q14 ## q01 1.00000000 -0.09872403 0.43586018 0.4024399 0.21673399 0.3378797 ## q02 -0.09872403 1.00000000 -0.11185965 -0.1193466 -0.07420968 -0.1646999 ## q04 0.43586018 -0.11185965 1.00000000 0.4006722 0.27820154 0.3508096 ## q05 0.40243992 -0.11934658 0.40067225 1.0000000 0.25746014 0.3153381 ## q06 0.21673399 -0.07420968 0.27820154 0.2574601 1.00000000 0.4022441 ## q14 0.33787966 -0.16469991 0.35080964 0.3153381 0.40224407 1.0000000 ## q15 0.24575263 -0.16499581 0.33423089 0.2613719 0.35989309 0.3801148 ## q19 -0.18901103 0.20329748 -0.18597751 -0.1653221 -0.16675017 -0.2540581 ## q22 -0.10440866 0.23087487 -0.09838349 -0.1325359 -0.16513541 -0.1698375 ## q15 q19 q22 ## q01 0.2457526 -0.1890110 -0.10440866 ## q02 -0.1649958 0.2032975 0.23087487 ## q04 0.3342309 -0.1859775 -0.09838349 ## q05 0.2613719 -0.1653221 -0.13253593 ## q06 0.3598931 -0.1667502 -0.16513541 ## q14 0.3801148 -0.2540581 -0.16983754 ## q15 1.0000000 -0.2098023 -0.16790617 ## q19 -0.2098023 1.0000000 0.23392259 ## q22 -0.1679062 0.2339226 1.00000000 # Alternatively, the lowerCor() fucntion from psych prints this more nicely lowerCor(saq) ## q01 q02 q04 q05 q06 q14 q15 q19 q22 ## q01 1.00 ## q02 -0.10 1.00 ## q04 0.44 -0.11 1.00 ## q05 0.40 -0.12 0.40 1.00 ## q06 0.22 -0.07 0.28 0.26 1.00 ## q14 0.34 -0.16 0.35 0.32 0.40 1.00 ## q15 0.25 -0.16 0.33 0.26 0.36 0.38 1.00 ## q19 -0.19 0.20 -0.19 -0.17 -0.17 -0.25 -0.21 1.00 ## q22 -0.10 0.23 -0.10 -0.13 -0.17 -0.17 -0.17 0.23 1.00 Bartlett’s test of sphericity Bartlett’s test of sphericity tests the null hypothesis that all correlations between variables are zero at the population level. In other words, if Bartlett’s test is non-significant it suggests that all of the indicator variables are not correlated. As a result, Bartlett’s test is pretty much always significant as it is very sensitive to sample size. Unsurprisingly, then, our Bartlett’s test result is significant. To run this, we use the cortest.bartlett() function from psych. Note that base R does include a function called bartlett.test(), but this is not the same test! (same Bartlett, though.) cortest.bartlett(saq) ## R was not square, finding R from data ## $chisq ## [1] 3674.737 ## ## $p.value ## [1] 0 ## ## $df ## [1] 36 Kaiser-Meyer-Olkin (KMO) Test/Kaiser’s Measure of Sampling Adequacy This test is often referred to as the KMO Test or Kaiser’s MSA, but both respectively mean the same thing. It is a measure of how much variance among all variables might be due to common variance. Higher KMO/MSA values indicate that more variance is likely due to common factors, thus indicating suitability for factor analysis. Kaiser (1974) provided the following (hilarious) interpretations of MSA values: MSA values are typically calculated for each variable, and for overall. It helps to report both. The KMO() function in psych will calculate both sets of measures of sampling adequacy. Here are our variables below. Overall they are generally in the meritorious range (except for one): KMO(saq) ## Kaiser-Meyer-Olkin factor adequacy ## Call: KMO(r = saq) ## Overall MSA = 0.82 ## MSA for each item = ## q01 q02 q04 q05 q06 q14 q15 q19 q22 ## 0.81 0.76 0.82 0.84 0.82 0.84 0.85 0.84 0.77 "],["how-many-factorscomponents.html", "12.4 How many factors/components?", " 12.4 How many factors/components? A crucial element of doing an EFA is deciding on the number of factors that should be extracted for the final solution. This is not a trivial decision, and essentially determines the final factor structure you derive and interpret in your factor analysis. Note that while we mainly talk about factors on this page, the same considerations apply when thinking of components in PCA. 12.4.1 Deciding on the number of factors Recall that an EFA/PCA will extract up to k factors/components, where k is the number of observed variables. At k factors/components, this will have explained all of the possible variance there is to explain in the observed variables. The basic idea behind how these factors are calculated is by essentially drawing straight lines through our data, much like a regression line. The idea is that each straight line (factor/component) should explain as much variance as possible, and each successive factor/component that is drawn explains the remaining variance. The first factor/component will always attempt to explain the most variance possible. The second factor/component will then be drawn through what remains after the first factor/component is calculated, in a way that both maximises the variance captured and is uncorrelated with the first factor/component. This lets us capture as much variance as possible in a clean way, where we can identify the relative contributions of each successive factor/component. The amount of variance that is captured by each factor/component is represented by a number called the eigenvalue. Naturally, the first factor/component will have the highest eigenvalue, and the eigenvalue of each factor/component afterwards will decrease. This means that at some point, we reach a stage where an additional factor doesn’t add much in terms of the variance explained. This indicates that there isn’t much utility in retaining factors after a certain point - i.e. we get diminishing returns on increasing the number of factors we have to interpret. We must strike a balance between having a relatively straightforward factor structure to interpret and how much variance is explained. Too few factors means we may not accurately capture enough variance to be meaningful or miss very crucial relationships, but too many factors means we lose parsimony and interpretability. There are several ways in which we can identify where the most optimal number of factors to retain is. 12.4.2 The Kaiser-Guttman rule The Kaiser-Guttman, Kaiser or simply the “eigenvalue &gt; 1” rule states that we should simply keep any factor with an eigenvalue above 1. To do this, we first need a correlation matrix from our data. We then feed this to the eigen() function in base R, which will calculate eigenvalues. I’ve piped it here, but you can also go straight to eigen(cor(saq)). Our data suggests that we retain 2 factors using this rule. saq_eigen &lt;- cor(saq) %&gt;% eigen() saq_eigen$values ## [1] 2.9515869 1.2029159 0.9339932 0.7897083 0.7671243 0.6453702 0.6078335 ## [8] 0.5663971 0.5350707 12.4.3 The scree plot The scree plot is a plot of each factor’s eigenvalue. This method relies on visual inspection - namely, you want to identify the ‘elbow’ of the line, or the point where the graph levels off. This is the point where the amount of variance explained by additional factors reaches that diminishing returns phase. psych will plot two sets of eigenvalues - one ‘component’-based set (which is what we calculated above), and one ‘factor’-based set (which is what Jamovi gives you). This is inherently a bit subjective, and sometimes isn’t very clear. On our scree plot below, it looks like four factors is the point where the diminishing returns begin, so we would go with retaining four factors. However, a more conservative interpreter could reasonably argue that we should only retain 2 factors. scree(saq) 12.4.4 Parallel analysis Parallel analysis (Horn, 1965) is a sophisticated technique that involves simulating random datasets of the same size as our actual dataset, and comparing our dataset’s eigenvalues against the random dataset’s eigenvalues. Parallel analysis is generally demonstrated using a scree plot with an additional scree line for the simulated datasets. The fa.parallel() function will run this for us. The number of factors to retain is determined by the number of factors where our actual data’s eigenvalue exceeds the simulated dataset’s eigenvalue. In this instance, we would choose to retain three factors, as the actual data eigenvalues clearly drop below the simulated data at four factors. fa.parallel(saq) ## Parallel analysis suggests that the number of factors = 3 and the number of components = 2 12.4.5 How to decide? Let’s summarise our interim decisions so far: Kaiser’s rule suggests 1 factor Visual scree plot inspection suggests either 2 or 4 factors Parallel analysis suggests 3 factors How do we decide what to use? Decades of empirical and simulation literature have shown a couple of things: Parallel analysis is one of the best methods of identifying how many factors should be retained. While it is sensitive to various things like sample size, simulation studies have shown that parallel analysis consistently outperforms other methods in terms of how many factors should be retained. In contrast, do not use the Kaiser rule! The Kaiser rule will consistently misestimate the number of factors - often, the misestimation will be quite severe. It is an extremely popular rule because a) it is simple to interpret and b) SPSS, which was the dominant statistical program of choice for a very long time, defaults to only using the Kaiser rule for PCA/EFA. Theoretical and practical considerations should also inform your decision making. If parallel analysis suggests 7 factors, for example, but those 7 factors are hard to interpret then you should probably not run with that by default. Instead, the next thing to do would be to step through solutions that remove one factor at a time until an acceptable, interpretable model has been reached. There are other methods of identifying factors, such as Vellicer’s Minimum Average Partials test, but Jamovi does not provide these options. In the absence of any strong justification for anything else, it is best to fall back on parallel analysis. Thus, it’s best if we go with three factors. "],["interpreting-output.html", "12.5 Interpreting output", " 12.5 Interpreting output Let’s now look at how to actually interpret the output of a factor analysis, including how to make sense of the main numbers that you get out of a basic EFA/PCA output. 12.5.1 Extraction methods PCA only has one method of deriving the eigenvalues of the components, and so it will give the same answer every time you run it on the same dataset. EFA, however, has multiple possible means of estimating factors, which are typically termed extraction methods. We won’t go into the details of how exactly they work, but the key thing to know is that the extraction method essentially changes There are three extraction methods available in Jamovi: Maximum likelihood. One of the most common options, and provides the most generalisable and robust estimates. ML methods assume normality and generally require large datasets, however. Principal axis factoring. Principal axis factoring does not make particular assumptions about the normality or distribution of the data, meaning that it is good at handling more complex datasets. Minimum residuals. The minimum residual method is sort of a middle-ground option between the two, but isn’t as commonly used in psychological research. Under ideal conditions, maximum likelihood (ML) and PA (principal axis) methods will generally give very similar estimates of the factors. When data are severely non-normal (or you want to anticipate that it will be), it is better to go with PA in the first instance. Otherwise, ML estimates are generally the way to go. To run a factor analysis in r, we use the fa function from psych. At minimum, we must specify the following: The dataset as the first argument The number of factors we want to extract The type of rotation - for now set this to “none”, as we’ll talk about this later The factoring method, fm - as above. \"ml\" stands for maximum likelihood, \"pa\" stands for principal axis factoring and \"minres\" stands for minimum residuals. saq_efa &lt;- fa( saq, nfactors = 3, rotate = &quot;none&quot;, fm = &quot;ml&quot; ) 12.5.2 Interpreting output Below is the main output of our EFA. This is what we call a factor matrix: saq_efa ## Factor Analysis using method = ml ## Call: fa(r = saq, nfactors = 3, rotate = &quot;none&quot;, fm = &quot;ml&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## ML1 ML2 ML3 h2 u2 com ## q01 0.59 0.31 -0.12 0.46 0.54 1.6 ## q02 -0.25 0.25 0.32 0.23 0.77 2.8 ## q04 0.62 0.23 -0.02 0.44 0.56 1.3 ## q05 0.56 0.19 -0.06 0.36 0.64 1.3 ## q06 0.55 -0.21 0.32 0.45 0.55 2.0 ## q14 0.63 -0.12 0.10 0.42 0.58 1.1 ## q15 0.55 -0.15 0.10 0.34 0.66 1.2 ## q19 -0.37 0.21 0.22 0.23 0.77 2.3 ## q22 -0.28 0.31 0.24 0.24 0.76 2.9 ## ## ML1 ML2 ML3 ## SS loadings 2.33 0.47 0.35 ## Proportion Var 0.26 0.05 0.04 ## Cumulative Var 0.26 0.31 0.35 ## Proportion Explained 0.74 0.15 0.11 ## Cumulative Proportion 0.74 0.89 1.00 ## ## Mean item complexity = 1.8 ## Test of the hypothesis that 3 factors are sufficient. ## ## df null model = 36 with the objective function = 1.43 with Chi Square = 3674.74 ## df of the model are 12 and the objective function was 0.01 ## ## The root mean square of the residuals (RMSR) is 0.01 ## The df corrected root mean square of the residuals is 0.02 ## ## The harmonic n.obs is 2571 with the empirical chi square 28.51 with prob &lt; 0.0046 ## The total n.obs was 2571 with Likelihood Chi Square = 33.87 with prob &lt; 0.00071 ## ## Tucker Lewis Index of factoring reliability = 0.982 ## RMSEA index = 0.027 and the 90 % confidence intervals are 0.016 0.037 ## BIC = -60.35 ## Fit based upon off diagonal values = 1 ## Measures of factor score adequacy ## ML1 ML2 ML3 ## Correlation of (regression) scores with factors 0.89 0.65 0.59 ## Multiple R square of scores with factors 0.79 0.43 0.35 ## Minimum correlation of possible factor scores 0.59 -0.15 -0.31 What do these numbers mean? Firstly, each value in each factor column (labelled ML1, ML2 etc) gives us our loadings. Note that as per the common factor model we described before, we estimate a loading for every variable on every factor. However, this can be a bit gross to interpret, so we generally choose to suppress (not remove) loadings below a certain threshold. By default, Jamovi will hide loadings below 0.3. We can also sort the items based on their loadings. To do this, we use the fa.sort() function and feed in our EFA object. We can then use print() to clean up this output with two arguments: digits to show rounding, and cut to suppress values below a certain size. This gives us a nicer output: saq_efa &lt;- fa.sort(saq_efa) print(saq_efa, digits = 3, cut = .30) ## Factor Analysis using method = ml ## Call: fa(r = saq, nfactors = 3, rotate = &quot;none&quot;, fm = &quot;ml&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## ML1 ML2 ML3 h2 u2 com ## q14 0.631 0.420 0.580 1.12 ## q04 0.622 0.441 0.559 1.27 ## q01 0.589 0.311 0.458 0.542 1.61 ## q05 0.562 0.358 0.642 1.26 ## q15 0.551 0.337 0.663 1.23 ## q06 0.546 0.324 0.448 0.552 1.97 ## q19 -0.371 0.229 0.771 2.27 ## q22 0.314 0.238 0.762 2.88 ## q02 0.319 0.229 0.771 2.84 ## ## ML1 ML2 ML3 ## SS loadings 2.333 0.474 0.352 ## Proportion Var 0.259 0.053 0.039 ## Cumulative Var 0.259 0.312 0.351 ## Proportion Explained 0.738 0.150 0.111 ## Cumulative Proportion 0.738 0.889 1.000 ## ## Mean item complexity = 1.8 ## Test of the hypothesis that 3 factors are sufficient. ## ## df null model = 36 with the objective function = 1.432 with Chi Square = 3674.737 ## df of the model are 12 and the objective function was 0.013 ## ## The root mean square of the residuals (RMSR) is 0.012 ## The df corrected root mean square of the residuals is 0.021 ## ## The harmonic n.obs is 2571 with the empirical chi square 28.515 with prob &lt; 0.00465 ## The total n.obs was 2571 with Likelihood Chi Square = 33.874 with prob &lt; 0.000706 ## ## Tucker Lewis Index of factoring reliability = 0.982 ## RMSEA index = 0.0266 and the 90 % confidence intervals are 0.0163 0.0374 ## BIC = -60.351 ## Fit based upon off diagonal values = 0.998 ## Measures of factor score adequacy ## ML1 ML2 ML3 ## Correlation of (regression) scores with factors 0.892 0.652 0.587 ## Multiple R square of scores with factors 0.795 0.425 0.345 ## Minimum correlation of possible factor scores 0.590 -0.150 -0.310 Statistically speaking, in a factor matrix each loading is a regression coefficient for the latent factor predicting the variable. We can interpret them as we would with normal regressions, except this is a regression coefficient for our latent variable predicting each observed variable. For example, the loading for q14 on factor 1 is 0.631. This means that for every 1 unit increase on latent factor 1, scores on Q14 increase by .631 units. Jamovi typically only gives you the uniqueness column, u2, which gives us the value of unique variance as a percentage. This is how much variance in each item is not explained by the factors we have chosen. In this instance, 58% of the variance in Q14 is not explained by the three factors. Generally though, it’s easier to think in terms of communalities, or the values in the h2 column, which is the amount of variance that is explained by the factors. Communalities are simply 1 - uniqueness; thus, the communality of Q14 is 0.420, which indicates that 42% of the variance in Q14 is explained by the three factors. Higher communalities indicate that the factors collectively explain more variance in the observed variable. How is this value calculated? Communalities are the sum of the squared factor loadings. Therefore, we can look at Q14’s factor loadings in the first (pre-sorted) output table, and calculate the communality as: \\[ h^2 = .631^2 + -.116^2 + .0963^2 \\] Which gives us an answer of approximately 0.420: .631^2 + (-.116)^2 + .0963^2 ## [1] 0.4208907 12.5.3 Total variance explained The output of psych will also give you a brief table of the total amount of variance explained by each factor. This is shown in the output of fa, but can also be accessed again below. It is generally useful to at least report the total cumulative variance explained by all factors. In this case, the three factors collectively explain 35.1% of the total variance in the data (shown by the row saying “Cumulative var”). saq_efa$Vaccounted ## ML1 ML2 ML3 ## SS loadings 2.3325379 0.47409809 0.35202983 ## Proportion Var 0.2591709 0.05267757 0.03911443 ## Cumulative Var 0.2591709 0.31184845 0.35096287 ## Proportion Explained 0.7384567 0.15009441 0.11144890 ## Cumulative Proportion 0.7384567 0.88855110 1.00000000 "],["rotation-and-interpreting-output-again.html", "12.6 Rotation, and interpreting output again", " 12.6 Rotation, and interpreting output again You may have noticed that on the previous page, we didn’t make much of an effort to actually talk about what the factors were or what they meant. That’s because the output that we got on the previous page isn’t actually terribly informative or easy to interpret. To help with this, in EFA we perform a technique called rotation. 12.6.1 Rotations Rotations are a technique in EFA that are done to help with the interpretability of the factor solution. The key aim of rotation is to achieve simple structure where possible. You may have noticed that the matrix from the previous page has quite a few variables with high enough loadings on multiple factors. This is called cross-loading, and indicates that two factors explain the variable. This is not easily interpretable! Ideally, in a robust simple structure we want: Only one loading per variable At least three loadings per factor print(saq_efa, digits = 3, cut = 0.3) ## Factor Analysis using method = ml ## Call: fa(r = saq, nfactors = 3, rotate = &quot;none&quot;, fm = &quot;ml&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## ML1 ML2 ML3 h2 u2 com ## q14 0.631 0.420 0.580 1.12 ## q04 0.622 0.441 0.559 1.27 ## q01 0.589 0.311 0.458 0.542 1.61 ## q05 0.562 0.358 0.642 1.26 ## q15 0.551 0.337 0.663 1.23 ## q06 0.546 0.324 0.448 0.552 1.97 ## q19 -0.371 0.229 0.771 2.27 ## q22 0.314 0.238 0.762 2.88 ## q02 0.319 0.229 0.771 2.84 ## ## ML1 ML2 ML3 ## SS loadings 2.333 0.474 0.352 ## Proportion Var 0.259 0.053 0.039 ## Cumulative Var 0.259 0.312 0.351 ## Proportion Explained 0.738 0.150 0.111 ## Cumulative Proportion 0.738 0.889 1.000 ## ## Mean item complexity = 1.8 ## Test of the hypothesis that 3 factors are sufficient. ## ## df null model = 36 with the objective function = 1.432 with Chi Square = 3674.737 ## df of the model are 12 and the objective function was 0.013 ## ## The root mean square of the residuals (RMSR) is 0.012 ## The df corrected root mean square of the residuals is 0.021 ## ## The harmonic n.obs is 2571 with the empirical chi square 28.515 with prob &lt; 0.00465 ## The total n.obs was 2571 with Likelihood Chi Square = 33.874 with prob &lt; 0.000706 ## ## Tucker Lewis Index of factoring reliability = 0.982 ## RMSEA index = 0.0266 and the 90 % confidence intervals are 0.0163 0.0374 ## BIC = -60.351 ## Fit based upon off diagonal values = 0.998 ## Measures of factor score adequacy ## ML1 ML2 ML3 ## Correlation of (regression) scores with factors 0.892 0.652 0.587 ## Multiple R square of scores with factors 0.795 0.425 0.345 ## Minimum correlation of possible factor scores 0.590 -0.150 -0.310 Rotation can help us achieve this. What rotations essentially do is change how variance is distributed within each factor. That wording is extremely important to note. It does not change our data in any way - the actual amount of variance explained by each factor does not change. What does change is how variance is distributed across the factors, which has the effect of then changing the loadings. But the actual data does not change!! There are two families of rotations that we can employ. Orthogonal rotations force factors to be uncorrelated. Oblique rotations allow factors to be correlated. The below diagram visualises what rotations do: Which rotation to choose? In psychology, everything tends to be correlated with everything else, and it’s extremely rare that we would get an instance where two factors do not correlate at all. For that reason, oblique rotations are generally the way to go. Orthogonal rotations are extremely hard to justify without strong a-priori evidence - and even if two factors are uncorrelated, oblique rotations will give the same solution as orthogonal ones. In short, there’s generally no reason to prefer an orthogonal rotation by default. 12.6.2 Rotated factor solution Let’s apply an oblique rotation to our factor analysis. The default oblique rotation is called oblimin. To do this, we need to re-run our fa() function with a rotation specified. saq_efa_rot &lt;- fa( saq, nfactors = 3, rotate = &quot;oblimin&quot;, fm = &quot;ml&quot; ) This produces the following output, which we now term a pattern matrix: saq_efa_rot %&gt;% fa.sort() %&gt;% print(digits = 3, cut = 0.3) ## Factor Analysis using method = ml ## Call: fa(r = saq, nfactors = 3, rotate = &quot;oblimin&quot;, fm = &quot;ml&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## ML1 ML3 ML2 h2 u2 com ## q01 0.714 0.458 0.542 1.02 ## q04 0.609 0.441 0.559 1.06 ## q05 0.553 0.358 0.642 1.02 ## q06 0.704 0.448 0.552 1.01 ## q14 0.440 0.420 0.580 1.57 ## q15 0.434 0.337 0.663 1.32 ## q02 0.506 0.229 0.771 1.04 ## q22 0.488 0.238 0.762 1.08 ## q19 0.412 0.229 0.771 1.11 ## ## ML1 ML3 ML2 ## SS loadings 1.360 1.043 0.756 ## Proportion Var 0.151 0.116 0.084 ## Cumulative Var 0.151 0.267 0.351 ## Proportion Explained 0.431 0.330 0.239 ## Cumulative Proportion 0.431 0.761 1.000 ## ## With factor correlations of ## ML1 ML3 ML2 ## ML1 1.000 0.591 -0.411 ## ML3 0.591 1.000 -0.448 ## ML2 -0.411 -0.448 1.000 ## ## Mean item complexity = 1.1 ## Test of the hypothesis that 3 factors are sufficient. ## ## df null model = 36 with the objective function = 1.432 with Chi Square = 3674.737 ## df of the model are 12 and the objective function was 0.013 ## ## The root mean square of the residuals (RMSR) is 0.012 ## The df corrected root mean square of the residuals is 0.021 ## ## The harmonic n.obs is 2571 with the empirical chi square 28.515 with prob &lt; 0.00465 ## The total n.obs was 2571 with Likelihood Chi Square = 33.874 with prob &lt; 0.000706 ## ## Tucker Lewis Index of factoring reliability = 0.982 ## RMSEA index = 0.0266 and the 90 % confidence intervals are 0.0163 0.0374 ## BIC = -60.351 ## Fit based upon off diagonal values = 0.998 ## Measures of factor score adequacy ## ML1 ML3 ML2 ## Correlation of (regression) scores with factors 0.852 0.823 0.736 ## Multiple R square of scores with factors 0.726 0.678 0.541 ## Minimum correlation of possible factor scores 0.453 0.356 0.082 Now we can see our simple structure taking effect, and this output is much more interpretable from before. These values are still regression coefficients between each latent factor and each variable, but now we can group these variables into their underlying latent factors much more easily. We can see that questions 1, 4 and 5 are best captured by factor 1, questions 6, 14 and 15 by factor 2 and questions 2, 19 and 22 by factor 3. One warning here. On the previous page, where we had an unrotated solution, the communalities were calculated by summing the squared factor loadings for each variable. That rule no longer applies here because by allowing the factors to correlate, the regression loadings now capture non-specific variance. Summing the squared factor loadings will lead to greater communality values than what they actually are. However, as you can hopefully see in the uniqueness column, the actual communalities have not changed. Rotation does not change how much variance is explained in total - only how that variance is distributed! Oblique rotations will also generate a factor correlation matrix. This calculates the correlations between the factors - remembering that by specifying an oblique rotation, we allowed them to correlate: saq_efa_rot$Phi ## ML1 ML3 ML2 ## ML1 1.0000000 0.5909483 -0.4107233 ## ML3 0.5909483 1.0000000 -0.4475027 ## ML2 -0.4107233 -0.4475027 1.0000000 Finally, we get the table of variance explained. This is now not as useful because of the same reason we cannot sum the squared factor loadings - each factor on its own now captures shared variance across the other factors as they are allowed to correlate. However, the total amount of variance explained is still 35.1%; once again, this does not change. saq_efa_rot$Vaccounted ## ML1 ML3 ML2 ## SS loadings 1.3601670 1.0425634 0.75593543 ## Proportion Var 0.1511297 0.1158404 0.08399283 ## Cumulative Var 0.1511297 0.2669700 0.35096287 ## Proportion Explained 0.4306144 0.3300645 0.23932111 ## Cumulative Proportion 0.4306144 0.7606789 1.00000000 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
