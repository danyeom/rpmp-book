[["logistic-regression.html", "Chapter 11 Logistic regression", " Chapter 11 Logistic regression The final section of the book (at least for the time being) deals with logistic regression. In some ways, we have come full circle with the inclusion of this chapter: the first statistical test we looked at were to do with categorical data, and now we make a partial return to categorical data. "],["probability-and-odds.html", "11.1 Probability and odds", " 11.1 Probability and odds 11.1.1 Reminder of probabilities Consider the following table: Burnout No burnout Total Musician 20 4 24 Non-musician 10 8 18 Total 30 12 42 You may recall that this is a 2x2 contingency table, which we have seen before in a chi-square context. Using this table, we can work out the probability of certain events or outcomes. If we selected someone randomly from this table, for example, what is the probability that they would be a musician? Well, we can see that there are 24 musicians from the sample of 42, so we could simply say: \\[ P(Musician) = \\frac{24}{42} = 0.57 \\] Likewise, what is the probability that someone is burnt out? That would simply be: \\[ P(Burnout) = \\frac{30}{42} = 0.71 \\] What about the probability that someone is a musician and and burnt out? We could denote this as follows: \\[ P(Musician \\cap Burnout) = \\frac{20}{42} = 0.47 \\] What about the probability that someone is burnt out, given they are a musician? This would be a conditional probability, where we are finding a probability of something on the condition that the person is burnt out. There are 24 participants who reported burnout, so our calculation would be as follows: \\[ P(Burnout | Musician) = \\frac{20}{24} = 0.83 \\] 11.1.2 Odds Now, let’s talk about odds. Odds are simply the likelihood of a particular outcome occuring, and is calculated as the probability that an event will occur, divided by the probability that the event will not occur. In other words, if the probability of an event is denoted as \\(A\\), the probability of event \\(A\\) not occuring is \\(1-A\\). We can then calculate the odds as: \\[ Odds = \\frac{A}{1-A} \\] Let’s return to our example above, and print out the table again for ease of reference. Burnout No burnout Total Musician 20 4 24 Non-musician 10 8 18 Total 30 12 42 What are the odds of burnout in the musician group? To do this, we need to find the probability of burnout given they are musicians, and divide that by the probability of no burnout given they are musicians. The odds of burnout given that someone is a musician is as we saw above: \\[ P(Burnout | Musician) = \\frac{20}{24} \\] And the probability of someone not burning out given that they are a musician must therefore be: \\[ P(No \\ burnout | Musician) = \\frac{4}{24} \\] Now we can divide these two probabilities as follows: \\[ Odds = \\frac{20/24}{4/24} = \\frac{20}{4} = 5 \\] What this means is that musicians are 5 times as more likely to experience burnout than not experience it. One more example. What are the odds of burnout in the non-musician group? Using the same principles as above, we can calculate this as follows. \\[ P(Burnout | Nonmusician) = \\frac{10}{18} \\] \\[ P(No \\ burnout | Nonmusician) = \\frac{8}{18} \\] \\[ Odds = \\frac{10/18}{8/18} = \\frac{10}{8} = 1.25 \\] So even non-musicians are 1.25 times more likely - or, in other words, 25% more likely - to increase burnout than not experience it. 11.1.3 Odds ratios Now we can take a look at the odds ratio. The odds ratio describes how likely one outcome is given an exposure/group, compared to another exposure/group. The odds ratio is calculated by dividing the odds of event A by the odds of event B. The resulting value gives an indication of how much more likely event A is compared to event B, given differences in exposure. We have already calculated two sets of odds ratios: The odds that a musician experiences burnout; \\(Odds = 5\\) The odds that a non-musician experiences burnout; \\(Odds = 1.25\\) We can now calculate an odds ratio for how likely a musician is to experience burnout compared to a non-musician. We simply divide the two sets of odds: \\[ OR = \\frac{Odds(A)}{Odds(B)} \\] \\[ OR = \\frac{5}{1.25} = 4 \\] An odds ratio of 4 indicates that a musician is 4 times as likely to experience burnout compared to a non-musician. Heavens! "],["logistic-regression-introduction.html", "11.2 Logistic regression: Introduction", " 11.2 Logistic regression: Introduction 11.2.1 Introduction All of the concepts on the previous page bring us to the main technique of this model, which is logistic regression. Logistic regression is used when we want to predict a binary outcome - for example, dead/alive status, affected/unaffected status and other scenarios where we have two primary outcomes. In this sense, we are essentially making a prediction about how likely outcome 1 is over outcome 0. Keep this in mind! 11.2.2 Modelling probabilities (sort of) Consider the following example data. We can see that we have two columns of interest: age and outcome. Notice how outcome only takes the values of 0 and 1. This is because this is a binary variable, where 0 = one outcome and 1 = another outcome. Often, we run into situations where we are interested in predicting a binary outcome using a series of predictors, including continuous ones. age_data Your first thought may be to just use a simple linear regression in this instance, and sure, R isn’t going to stop you from doing so: summary( lm(outcome ~ age, data = age_data) ) ## ## Call: ## lm(formula = outcome ~ age, data = age_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.36788 -0.12490 0.01528 0.13212 0.32370 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.55739 0.16515 -3.375 0.00819 ** ## age 0.10281 0.01421 7.235 4.89e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2108 on 9 degrees of freedom ## Multiple R-squared: 0.8533, Adjusted R-squared: 0.837 ## F-statistic: 52.35 on 1 and 9 DF, p-value: 4.893e-05 You might conclude that you have a significant model, with age being a significant predictor of the binary outcome. Nice! … right? Well, the moment you plot your data you may quickly see the problem with this approach: age_data %&gt;% ggplot( aes(x = age, y = outcome) ) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; There are two huge problems here! For starters, the line currently implies that there are values that exist between 0 and 1, but what is that meant to mean? In this instance, how can we have any intermediate values between our binary outcome? The second problem is that a simple linear regression also implies that there are values that exist beyond 0 and 1, as you can hopefully see in the graph above. This also makes no sense! 11.2.3 The logistic model In short, if we want to use our standard regression techniques, we need to model our data in a way where we can have outcomes beyond 0 or 1. Given that probabilities don’t let us do this, that’s a no-go (unless we use probit regression, but that’s a different kettle of fish). Maybe we could use the odds because they let us go past 1 - but as we saw previously, odds are bounded at a minimum of 0. However… log odds are not bounded in this way, as the log of 0 is \\(- \\infty\\). It also turns out that with a large enough sample size, the relationship between a predictor and the log odds is linear. Therefore, we can model a regression against the log odds as follows: \\[ log(Odds) = \\beta_0 + \\beta_1 x_1 + \\epsilon_i \\] This is essentially the equation for logistic regression. We use the linear regression formula to predict the log odds of an outcome. This makes logistic regression a form of the generalised linear model (GLM). We won’t go too into GLMs beyond here, but essentially the GLM uses a linear equation to model an outcome Y using a link function. The link function describes how the predictors relate to the outcome in the model, or in other words it allows us to use a linear regression on a transformed outcome: \\[ f(Y) = \\beta_0 + \\beta_1 x_1 + \\epsilon_i \\] In the formula above, \\(f(Y)\\) is used to describe the link function. In our instance, we are looking at a logit link to do a logistic regression, where our outcome is log odds (as opposed to Y, the dependent variable directly). There are many others out there that are suited for different types of data (e.g. Poisson regression). As another example of a link function, the identity function is \\(f(Y) = Y\\); this gives us linear regression, so really our usual regression models are just an example of the GLM. The logistic function is characterised by a very obvious S-shaped curve: (Technical note: the regression line that is fit is no longer least squares regression. Rather, it uses a procedure called maximum likelihood. Think of it as a different engine under the hood.) In practice, then, this means that we can use the same kinds of thinking as we have in previous regression models to interpret logistic regression outcomes. Namely, given the formula above, a 1-unit increase in \\(x_1\\) will correspond to a \\(\\beta_1\\) increase in the log odds. However, even though mathematically that makes sense, it’s hard to interpret what this actually means. What does an increase in log odds correspond to?? To solve this dilemma, we often will want to convert our outcome back into odds. To do so is simple: we simply exponentiate both sides: \\[ Odds = e^{\\beta_0 + \\beta_1 x_1 + \\epsilon_i} \\] To obtain the probability of an event occuring, we need to convert the odds back into a probability: \\[ P(Y = 1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\epsilon_i)}} \\] "],["example-8.html", "11.3 Example", " 11.3 Example Below is a dataset relating to the presence of sleep disorders (credit to Laksika Tharmalingam on Kaggle for this dataset!), and various metrics relating to lifestyle and physical health. The dataset contains the following variables: id: Participant id gender: Participant gender age: Participant age sleep_duration: The number of hours the participant sleeps per day sleep_quality: The participant’s subjective rating (1-10) of the quality of their sleep physical_activity: How many minutes per day the participant does physical activity stress: Subjective rating of stress level (1-10) bmi: BMI category (Underweight, normal, overweight) blood_pressure: systolic/diastolic blood pressure heart_rate: Resting heart rate of the participant, in bpm sleep_disorder: Whether the participant has a sleep disorder or not (0 = No, 1 = Yes) sleep &lt;- read_csv(here(&quot;data&quot;, &quot;logistic&quot;, &quot;sleep_data.csv&quot;)) ## Rows: 374 Columns: 11 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): gender, bmi, blood_pressure ## dbl (8): id, age, sleep_duration, sleep_quality, physical_activity, stress, ... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. In this example, we’re interested in seeing whether specific factors predict whether or not the participant has a sleep disorder. We’ll work with an example with one predictor to start, and then move to multiple predictors afterwards. 11.3.1 Assumptions The refreshing thing about the logistic regression is that there are actually very few assumptions that need to be made. For starters, we do not assume the following: Linearity between the IV and the DV Normality Homoscedasticity None of these assumptions apply! What we do assume instead is: Linearity between the IV and the logit (i.e. the log odds) The outcome is a binary variable that is mutually exclusive (someone cannot be Y = 0 and Y = 1 at the same time) Absence of multicollinearity (in a multiple regression context) 11.3.2 Building the model Here is where things start to change a little from what we’re used to. Because we are not building a linear model but a generalised linear model, we now need to use the glm() function in R. glm(), by and large, works exactly the same way as you have seen with lm(); you need to give it arguments in the form of outcome ~ predictor, and once you have run the function you need to call the results using summary(). The first thing that changes is that by virtue of the fact that we are using the GLM, we must specify what link function we are working with. This can be set using the family argument. In the instance of logistic regression, the relevant family is binomial, and specifically binomial(link = \"logit\"): model &lt;- glm(outcome ~ predictor, data = data, family = binomial(link = &quot;logit&quot;)) The formula is a little strange, but binomial() is essentially a function that takes the argument link, for which we set the value as \"logit\". The logit argument is the default for this function though, so we can simply shorten this down to family = binomial specifically for logistic regressions. (For other binomial-based GLMs, you must specify the link.) With that in mind, we can build our logistic regression models. In the first instance, let’s see if age predicts whether someone has a sleep disorder. sleep_glm &lt;- glm(sleep_disorder ~ age, data = sleep, family = binomial) summary(sleep_glm) ## ## Call: ## glm(formula = sleep_disorder ~ age, family = binomial, data = sleep) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.28024 0.65343 -8.081 6.44e-16 *** ## age 0.11549 0.01493 7.738 1.01e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 507.47 on 373 degrees of freedom ## Residual deviance: 433.36 on 372 degrees of freedom ## AIC: 437.36 ## ## Number of Fisher Scoring iterations: 4 Note that our output table is a little bit different than what we’re used to; this is because of the change in procedure mentioned on the previous page (from least squares to maximum likelihood). However, we can still largely read this output the same way as we have. We can see that age is a significant predictor (p &lt; .001). The coefficient for age is .115. We can get the coefficients seperately by using the coef() function on our model: coef(sleep_glm) ## (Intercept) age ## -5.2802414 0.1154897 What does this mean? It means that for every 1 unit increase in age, the log odds increase by .115. This is important because remember that we’re modelling against log odds, not odds or probability! In essence, we have estimated the following: \\[ log(Odds) = -5.280 + (0.115 \\times x_1) \\] We need to make sense of this another way somehow. Recall that log odds and odds relate to each other in the following way: \\[ Odds = e^{\\beta_0 + \\beta_1 x_1 + \\epsilon_i} \\] To obtain the odds, as discussed on the previous page, we need to exponentiate our coefficients: exp(coef(sleep_glm)) ## (Intercept) age ## 0.005091201 1.122423009 The exponentiated coefficient gives us our odds ratio. This describes the multiplied change in odds for every 1 unit increase of our predictor. In this instance, for every 1 year increase in age, the predicted odds of having a sleep disorder are multiplied by 1.12. Another way to describe this is that the predicted odds of having a sleep disorder increase by a factor of 1.12. This coefficient does not mean the following: The odds increase by 1.12 for every unit of x - remember, only the log odds are linearly related to the predictor. The odds are non-linearly related. The probability increases by 1.12 - same deal as above, the probability isn’t linearly related to the predictors. 11.3.3 Predictions Now recall that we can convert from odds to probabilities in the following manner: \\[ P(Y = 1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\epsilon_i)}} \\] Using this, we can make predictions about the probability of our outcome for a given value of our predictor. For example, what is the probability that a 50 year old will have a sleep disorder? That would be given as the following: \\[ P(Y = 1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\epsilon_i)}} \\] \\[ P(Y = 1) = \\frac{1}{1 + e^{-(-5.280 + (0.115 \\times 50)}} \\] \\[ P(Y = 1) = \\frac{1}{1 + e^{-0.47}} \\] We can use R to do the calculation for us: 1/(1 + exp(-0.47)) ## [1] 0.6153838 Thus, a 50 year old person has a 61.5% chance of having a sleep disorder (note that this has been rounded). We can actually plot the expected probabilities across a range of values for age by first asking R to predict the probabilities across a range of ages. This will draw the characteristic S-shaped curve of the logistic model. We use the predict() function to calculate the predicted probabilities of each value of our predictor. The type = response argument is used here to tell R to predict the probabilities (and not the log odds). age_range &lt;- data.frame( age = 20:70 ) age_range$predicted &lt;- predict(sleep_glm, age_range, type = &quot;response&quot;) age_range %&gt;% ggplot( aes(x = age, y = predicted) ) + geom_smooth(method = &quot;glm&quot;, method.args = list(family = binomial), se = FALSE) + theme_pubr() ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## Warning in eval(family$initialize): non-integer #successes in a binomial glm! We can get confidence intervals around our estimated coefficients using confint(), just like we have previously. We can do this either on the original coefficients, or the exponentiated ones. The confidence interval around the exponentiated coefficients gives us a 95% CI for our odds ratio. confint(sleep_glm) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) -6.60646431 -4.0395789 ## age 0.08712071 0.1457569 exp(confint(sleep_glm)) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 0.001351603 0.01760488 ## age 1.091028365 1.15691494 Thus, we can say that the OR of a sleep disorder is 1.12 (95% CI = [1.09, 1.16]). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
