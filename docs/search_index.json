[["index.html", "Research Process for Music Psychologists R version Chapter 1 Preface", " Research Process for Music Psychologists R version Daniel Yeom 2024-04-13 Chapter 1 Preface This is an R adaptation of Modules 5 - 11 of Research Process for Music Psychologists (MUSI90252). By and large the content is exactly the same as the Jamovi-focused content in the Canvas shell, except: - Some commentary has either been added or removed for R-specific material (e.g. information on functions) - Embedded content, such as that in Module 7, is not available here (but is Jamovi-specific anyway) - Some content has been reorganised in a minor way because of how R outputs things compared to Jamovi. This mainly affects the content around effect sizes, because often they rely on separate functions in R - but also occurs more often during the ANOVA and linear regression content (due to the fact that the models need to be built first before doing some steps, such as assumptions) "],["usage.html", "1.1 Usage", " 1.1 Usage Each bookdown chapter is an .Rmd file, and each .Rmd file can contain one (and only one) chapter. A chapter must start with a first-level heading: # A good chapter, and can contain one (and only one) first-level heading. Use second-level and higher headings within chapters like: ## A short section or ### An even shorter section. The index.Rmd file is required, and is also your first book chapter. It will be the homepage when you render the book. "],["render-book.html", "1.2 Render book", " 1.2 Render book You can render the HTML version of this example book without changing anything: Find the Build pane in the RStudio IDE, and Click on Build Book, then select your output format, or select “All formats” if you’d like to use multiple formats from the same book source files. Or build the book from the R console: # cd C:/Users/yeom7/OneDrive\\ -\\ The\\ University\\ of\\ Melbourne/Tutoring/RPMP/rpmp_bookdown bookdown::render_book() To render this example to PDF as a bookdown::pdf_book, you’ll need to install XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.org/tinytex/. "],["preview-book.html", "1.3 Preview book", " 1.3 Preview book As you work, you may start a local server to live preview this HTML book. This preview will update as you edit the book when you save individual .Rmd files. You can start the server in a work session by using the RStudio add-in “Preview book”, or from the R console: bookdown::serve_book() "],["intro.html", "Chapter 2 A very brief introduction to R", " Chapter 2 A very brief introduction to R The R version of the RPMP content is very different to the regular Jamovi version, by virture of the fact that even though Jamovi is built on R, it isn’t necessarily built with R users first in mind. Rather, it’s built for users of SPSS and other platforms that may be used to a point and click approach. To clarify, there’s nothing wrong with this at all - one of Jamovi’s greatest strengths is how easy it is to use - but it means that the same procedures in R work a little differently. This chapter is not meant to be an exhaustive be-all end-all to how to use R. It will only go through enough to complete RPMP’s content. It will however introduce a couple of things that I think make for good starting habits using R, which is never a bad thing! For a far more comprehensive and rigorous overview of how to program in R, there is no better guide than R 4 Data Science (R4DS) by Hadley Wickham: https://r4ds.had.co.nz/. In general, I am a huge proponent of the ‘tidy’ workflow of data analysis. While it was originally designed for data science in mind, I think it’s a valuable model for psychological science as well: "],["assignment-and-syntax.html", "2.1 Assignment and syntax", " 2.1 Assignment and syntax 2.1.1 Basic syntax In R, assigning values/strings etc. to variables is slightly different to other languages. Instead of using equals signs, we use a left arrow &lt;- for assignment. For example: # Use this x &lt;- 5 # This works but isn&#39;t preferred x = 5 When it comes to naming variables and the like, the easiest/most readable way for most people is to separate words using an underscore. e.g. # this is preferred variable_name # this is also very common variable.name # sentence case is also sometimes used VariableName # no spaces are allowed variable name # This will give you an error # some heathens use camel case variableName To view what has been assigned to a variable, you can simply write the variable name: # This is the variable we named earlier x ## [1] 5 2.1.2 Variable types Like many programming languages, R works by manipulating different types of variables. Knowing how to work with these different variables is fairly essential to using R, so here is a brief overview. First, we have our most basic classes of variables. The first is numeric: var_a &lt;- 5.25 var_a ## [1] 5.25 A special form of a numeric variable is an integer variable, which is used for whole numbers. var_b &lt;- 6 var_b ## [1] 6 The second is character, which is used for text (or strings in programming language). Characters must be enclosed with speech marks: var_c &lt;- &quot;This is a string&quot; var_c ## [1] &quot;This is a string&quot; Finally, we have logical variables, which can take on the form of TRUE or FALSE. TRUE and FALSE (or alternatively T or F) are special values in R that, as their names suggest, are used to indicate when a certain value is true or false. var_d &lt;- TRUE var_d ## [1] TRUE You can check the class of any given variable using the class() function in base R. class(var_a) ## [1] &quot;numeric&quot; class(var_b) ## [1] &quot;numeric&quot; class(var_c) ## [1] &quot;character&quot; class(var_d) ## [1] &quot;logical&quot; 2.1.3 Data structures Of course, in R we don’t work with single values (often). We instead work with larger data structures. While there are a number of data structures in R, by and large the main one we will work with are data frames. Data frames are flexible, row-column structures that contain data. R works best when data frames are in a tidy format. In a tidy format: Each variable is its own column Each observation (participant, object) is its own row Each value is in its own cell. Figure 2.1: Adapted from R4DS. Here is an example of a data frame in tidy format. Note how each column corresponds to a different variable, or piece of data that we’re interested in. Each column is also clearly labelled, so it is clear what it represents. Each row corresponds to an observation (a single penguin, in this case). So, the first row represnts an Adelie penguin on Torgersen Island, with a bill length of 39.1mm etc etc. library(palmerpenguins) penguins For the purposes of RPMP you won’t need to create any data frames, but you will need to know how to read files in. More on this on the next page! "],["packages-and-functions.html", "2.2 Packages and functions", " 2.2 Packages and functions The beauty of R is that its functionality is essentially limitless; its open-source nature and strong community mean that new functions and capabilities are regularly made for R. These new functions augment/extend what R is capable of doing, and are generally designed in the form of packages. To provide a very simple explanation of what packages are, packages are a collection of code that provide new functions in R. Some packages occassionally come with data too, such as the palmerpenguins package. 2.2.1 Loading packages When you start a new R session, the first thing that you’ll want to do is load the packages that you need to use. For now, we’ll load two packages for functions: tidyverse and rstatix. tidyverse is a huge package that contains a group of other packages designed for data manipulation, visualisation and cleaning. rstatix allows for simple statistical tests to be performed in an easy way. palmerpenguins comes with a dataset for practicing on. palmerpenguins loads a dataset named penguins that contains basic info on 3 species of penguins across 3 different islands. To load a package, call the library() function and enter the name of the package in brackets: library(tidyverse) ## Warning: package &#39;ggplot2&#39; was built under R version 4.3.3 ## Warning: package &#39;tidyr&#39; was built under R version 4.3.3 ## Warning: package &#39;readr&#39; was built under R version 4.3.3 ## Warning: package &#39;dplyr&#39; was built under R version 4.3.3 ## Warning: package &#39;stringr&#39; was built under R version 4.3.3 ## ── Attaching core tidyverse packages ────────────────────────────────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.5 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ ggplot2 3.5.0 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.3 ✔ tidyr 1.3.1 ## ✔ purrr 1.0.2 ## ── Conflicts ──────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors library(rstatix) ## ## Attaching package: &#39;rstatix&#39; ## ## The following object is masked from &#39;package:stats&#39;: ## ## filter library(palmerpenguins) 2.2.2 Tidyverse The tidyverse is a mega-collection of phenomenal packages that fundamentally change how to interface with R. The tidyverse provides packages for things like: Wrangling data Reading and writing data Making graphs Tidying and reshaping data Scraping the web The tidyverse is probably the single most popular suite of packages for R because of the functionality it provides. All of the tidyverse packages are written in consistent syntax, generally use very easy language that has an emphasis on verbs (i.e. you’re telling R to do something) and integrate seamlessly with each other and R. The tidyverse is a philosophy of R just as much as it is a suite of functions, and is part of what makes R so powerful today. Many aspects of the tidyverse are reliant on the pipe operator, %&gt;%. This basically tells R to take a dataframe or output and pass it onto a function that comes directly afterwards. Any function that takes a data frame as its first argument can (theoretically) be piped, meaning that we can chain strings of functions together in one run in a readable way. See the example below: data %&gt;% function_1() %&gt;% function_2(do = &quot;this&quot;) %&gt;% function_3(avoid = &quot;that&quot;) in the hypothetical example above, we first take data, and pass it to function 1. We then take the output of function 1 and pass that to function 2, which has the argument do = \"this\". Afterwards, we take the output of function 2 and pass it to function 3. There are a number of clear benefits to the tidyverse way of doing things. These include: Functions are generally stated as verbs, which means that you’re always doing something with a function (and it’s clear what that something is) Piping avoids the cyclical hell of creating intermediate variables. Consider a non-tidyverse version of the code example above, written down below. This code is not only a bit of a pain to read, but is also clunky in that it generates several intermediate variables that aren’t all that useful (a lot of the time). output_1 &lt;- function_1(data) output_2 &lt;- function_2(output_1, do = &quot;this&quot;) output_3 &lt;- function_3(output_2, avoid = &quot;that&quot;) Piped code is generally quite easy to read. tidyverse also provides a consistent syntax for other packages! It provides a quasi-philosophy and style guide for developers to write their own packages to write ‘tidy’ packages. rstatix is a great example of this. Throughout this book you will see a lot of tidyverse! 2.2.3 The here() package Another staple bit of code you will see throughout RPMP is the here package. The official vignette for here summarises what this package does: The here package enables easy file referencing by using the top-level directory of a file project to easily build file paths. This is in contrast to using setwd(), which is fragile and dependent on the way you order your files on your computer. Alternatively, read the below quote from R goddess Jenny Bryan: If the first line of your #rstats script is setwd(\"C:\\Users\\jenny\\path\\that\\only\\I\\have\"), I will come into your lab and SET YOUR COMPUTER ON FIRE. In short, here() allows us to locate files in a relative manner as opposed to an absolute one. This is super super useful for sharing your code and data with other people, and ensuring that your scripts will run no matter where they are. We won’t get into project-oriented workflows for RPMP. However, imagine you have a folder structure like this: |-code |-----rpmp_week1.Rmd |-data |-----w1_dataset.csv |-output |-RPMP.rproj Normally, to locate a file on a disk you would generally have to give the entire pathway to that file. That could be something like \"C:\\Users\\Dan\\Documents\\Subjects\\RPMP\\data\\w1_dataset.csv\" - which is immensely unwieldy if we want to read in data - and won’t work the moment I give my script to someone else, as their folder structure could be completely different! The alternative with here could be as simple as: here(&quot;data&quot;, &quot;w1_dataset.csv&quot;) This tells R that I’m looking for something here in the data folder, specifically a file named w1_dataset.csv. As long as the relative positions are correct - i.e. the .csv file is in the data folder - R will know where to locate the file. You will see a lot of here() in this version of the RPMP guide because as you may appreciate, there are a lot of data files stored in all manner of folders. "],["wrangling-data-with-dplyr-and-others.html", "2.3 Wrangling data with dplyr and others", " 2.3 Wrangling data with dplyr and others dplyr is a package within the tidyverse for manipulating and wrangling data. dplyr is one of the most popular packages on R because it provides a suite of functions that are fairly essential to manipulating and working with data. Below is a brief overview of some of these functions, applied to the penguins dataset. 2.3.1 Selecting columns with select() select() lets you select the columns you want from a dataset. Simply specify the columns that you want by name. Below we take the species and island columns from the penguins dataset: penguins %&gt;% select(species, island) You can select columns via a number of ways: Simply by name, e.g. select(species, island) By their index or column number, e.g. select(1, 2) select(1:4) will select columns 1 to 4 select(-1) will select the last column By certain operator functions, such as starts_with() and ends_with(), e.g. ends_with(\"mm\") will select all columns that end with “mm” Combinations of the above also work. Removing columns is simply done by adding a minus sign - in front of the arguments for select, and are compatible with all of the options above. 2.3.2 Filtering rows with filter() filter() selects the rows that you want based on a certain condition. Here, we specify the column that want to filter by and state the condition (== means equals to). We can filter on multiple conditions; for example, filtering Adelie penguins by the year 2007: penguins %&gt;% filter(year == 2007, species == &quot;Adelie&quot;) You can also choose to filter out rows based on a condition by adding an exclamation mark in front of the column name. penguins %&gt;% filter(!year == 2007) drop_na() is another useful starting function that simply removes all rows with NA/empty cells. If you enter it as is then it will clean the entire dataset; if you specify a column then it will remove all rows with NAs in that column. Below is an example of a pipe using these functions - going from selecting columns to filtering rows and finally cleaning up the empty cells. penguins %&gt;% select(species, body_mass_g, year) %&gt;% filter(year == 2009) %&gt;% drop_na() 2.3.3 Creating new columns with mutate() mutate() is a function that lets you create new columns. This can be extremely useful for operations like recoding variables and transforming them. The nice thing about mutate() is that you can do all manner of operations without touching your original data. The basic workflow of mutate() looks like this: data %&gt;% mutate( new_column_1 = a_function(...), new_column_2 = another_function(...) ) This example code would create two new columns, named new_column_1 and new_column_2, with their respective values being whatever the functions were. For example, in the penguins dataset we have a variable called body_mass_g, which is the body mass of each penguin in grams. If we wanted to convert this to kilograms, we would need to divide each penguin’s value on this variable by 1000. mutate() makes this a piece of cake. Let’s also chain a select() command to only show the following variables: species, island, body_mass_g and sex. penguins %&gt;% select(species, island, body_mass_g, sex) %&gt;% mutate(body_mass_kg = body_mass_g/1000) You can see now that we have a new column called body_mass_kg that has our new transformed variable. mutate() can also take functions (and likely will make up the majority of your use of it). For example, let’s say that we want to make a variable that takes the natural logarithm of bill length (for whatever reason). We could do this as follows using the log() function within our mutate() call: penguins %&gt;% select(species, island, bill_length_mm, bill_depth_mm) %&gt;% mutate(bill_length_log = log(bill_length_mm)) 2.3.4 Summarising data with summarise() and group_by() Finally, sometimes we will want to summarise data - for example, to calculate basic features such as descriptives or for plotting. To do that, we can use the function summarise() (or summarize() for American users). summarise() works very similarly to mutate(). data %&gt;% summarise( summary_1 = a_function(...), summary_2 = another_function(...) ) The difference is that while mutate() retains the features of your data, summarise() will instead collapse it. To illustrate, let’s say we want to calculate a) how many penguins there are (with the function n()) and b) the mean body mass (with the mean() function). penguins %&gt;% summarise( n_penguins = n(), mean_mass = mean(body_mass_g, na.rm = TRUE) ) This is… good and all, but consider what we’ve just done. We’ve just calculated the number of penguins and the mean body mass across the entire dataset. However, that may not necessarily be meaningful, particularly in this instance where we have meaningful groups within the data. For example, the above mean collapses across years, which may not be appropriate. Enter in another function called group_by(). As the name implies, group_by() will perform operations per a grouping variable that you specify. group_by() works especially well with summarise, because the idea is something like this: data %&gt;% group_by(variable) %&gt;% # Tell R to group the subsequent output by this variable summarise( summary_1 = a_function(...), summary_2 = another_function(...) ) %&gt;% ungroup() # Tell R grouping is no longer needed Let’s put this into practice by calculating the n and mean per year. Notice how the output now calculates n and the mean body mass per year, which is much more informative! penguins %&gt;% group_by(year) %&gt;% summarise( n_penguins = n(), mean_mass = mean(body_mass_g, na.rm = TRUE) ) %&gt;% ungroup() Naturally, group_by() can group using multiple variables. This is easy to do so as well penguins %&gt;% group_by(year, island) %&gt;% summarise( n_penguins = n(), mean_mass = mean(body_mass_g, na.rm = TRUE) ) ## `summarise()` has grouped output by &#39;year&#39;. You can override using the `.groups` argument. Suddenly this is much more informative - we can now do calculations/operations per year and island, which provides a lot more nuance. 2.3.5 Some other handy tidyverse functions As stated at the start of this chapter, this book will only cover enough R functions to provide you with an understanding of what goes on in this book and how. Nonetheless, there are so many tidyverse functions out there that are worth exploring and knowing about. Below is a brief list of some other functions from dplyr you may wish to keep in mind. To find out what a given function does in more detail, you just need to type ?function into the R console to search for its documentation (or ??x to do a broad search). Note that all of these are dplyr functions, so need to be piped from a dataset like usual. arrange() will sort your rows by a variable you specify. For example, if you wanted to sort the penguins dataset by island, you could use arrange(island) (or arrange(desc(island)) for descending order). distinct() will give you all unique values in a given column. distinct(island), for instance, will give you each unique island name. rename() will let you rename columns. `relocate() will let you rearrange the column order. The slice() set of functions is another way of subsetting rows based on their position. "],["making-graphs-with-ggplot2.html", "2.4 Making graphs with ggplot2", " 2.4 Making graphs with ggplot2 ggplot2 is the tidyverse way of visualising data. The ‘gg’ in the name ggplot2 stands for grammar of graphics. The idea is that a graph is built by the same few components - a dataset, geoms (visual marks that represent data) and a coordinate system. A cheatsheet for ggplot2 can be found here. To start a plot, we first call the ggplot function. Here, we specify three of the main features of our plot - the dataset that we want to use, the x axis and the y axis. the x and y axes are wrapped within the aes() function, which defines our aesthetics. Here, we simply say which columns of our dataset should go on the x and y axes. Using the penguin dataset, we can start to visualise a scatter plot between bill length and bill depth like this: ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm)) However, notice that the plot is empty. This is because we haven’t defined any geoms, or actual plotting methods, in our plot. Refer to the cheatsheet for all of the possible geoms. For now, we will stick to basics. A scatter plot can be specified by adding geom_point(): ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm)) + geom_point() ## Warning: Removed 2 rows containing missing values or values outside the scale range (`geom_point()`). From here, we can change many things by adding or modifying our existing layers. If we want to colour the dots by sex (which is a column in the penguins data), we can do so by specifiyng colour = sex in our aes() function. ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, colour = sex)) + geom_point() ## Warning: Removed 2 rows containing missing values or values outside the scale range (`geom_point()`). The axis labels aren’t very informative as they are. We can change this by adding labs(), which is a simple way of specifying x and y labels: ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, colour = sex)) + geom_point() + labs(x = &quot;Bill length (mm)&quot;, y = &quot;Bill depth (mm)&quot;) ## Warning: Removed 2 rows containing missing values or values outside the scale range (`geom_point()`). To add a line of best fit (e.g. in the instance of regressions), add geom_smooth(method = \"lm\"). Specifying the method is important because by default, geom_smooth() will probably fit LOESS curves (local polynomial regressions). Note that if you’ve specified a colour/grouping variable, separate regression lines will be fitted for each group. By default, the line will also have standard error bands around it. You can turn this off by also specifying se = FALSE in the geom_smooth() call. ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, colour = sex)) + geom_point() + labs(x = &quot;Bill length (mm)&quot;, y = &quot;Bill depth (mm)&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## Warning: Removed 2 rows containing non-finite outside the scale range (`stat_smooth()`). ## Warning: Removed 2 rows containing missing values or values outside the scale range (`geom_point()`). geom_boxplot() will create boxplots. An example is below, with species on the x axis and body mass on the y: ggplot(data = penguins, aes(x = species, y = body_mass_g)) + geom_boxplot() ## Warning: Removed 2 rows containing non-finite outside the scale range (`stat_boxplot()`). Finally, if you want to split plots by a certain variable, add facet_wrap() to our call. You need to specify what variable/column you want to split by, with a tilde in front. For example, if we wanted to split the scatter plot by year: ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, colour = sex)) + geom_point() + labs(x = &quot;Bill length (mm)&quot;, y = &quot;Bill depth (mm)&quot;) + facet_wrap(~year) ## Warning: Removed 2 rows containing missing values or values outside the scale range (`geom_point()`). From here, there are so many things you can do with ggplot - and it helps to get creative! Almost all of the graphs in RPMP (in fact, I think it actually is 100%) were made using ggplot2. "],["descriptive-statistics-module-5.html", "Chapter 3 Descriptive statistics (Module 5)", " Chapter 3 Descriptive statistics (Module 5) Descriptive statistics, as the name implies, are used to describe data. A key part of the quantitative research process is understanding the various ins and outs of your data. You’ll probably have a sense of why this is important if you have the qualitative presentation still fresh in your mind - namely, knowing your data is also really important for knowing what to do with it. In this first module, we will start with the first steps of understanding quantitative data. This involves visualising our data to see what it looks like, and describing key features of the data. If you’re familiar with statistics then some of this may seem a bit trivial, but it’s important that we get the basics right before we go on to doing fancy statistical tests. Also, I know that the idea of doing statistics and maths freaks a lot of us out - and that’s totally normal. Yes, there will be some number-crunching and maths in the next series of modules, but the focus of these modules is not to force you to calculate things by hand. You will encounter a whole bunch of mathematical formulae, but the point of doing so is to illustrate the concepts that underpin them. These concepts are crucial to understanding the ‘magic’ that happens with quantitative analysis, and a really solid foundation in statistical concepts will go a long way. That being said, throughout these statistics modules there will be a number of activities that ask you to actively work with sample datasets and analyse them. We promise that you will get so much more out of these modules if you complete these activities, because readings and webpages aren’t the best substitute for actually doing it and getting your hands dirty with data. Also note that didactic seminars begin this week! Each seminar will go through the key concepts that are covered here in the LMS. From here to Week 11, treat the LMS pages as an online textbook rather than the primary subject delivery (you may have noticed that the module requirements now list only this page and the quiz). Note below that the majority of these ‘textbook’ style pages don’t have estimated time allocations, as they are covered in the seminar anyway. By the end of this module you should be able to: Create both appropriate and meaningful graphs from data Calculate various forms of descriptive statistics Interpret both graphs and descriptive statistics, and explain what they tell you "],["visualising-data.html", "3.1 Visualising data", " 3.1 Visualising data When you have data in your hand, it is often tempting to dive straight into plugging it into an analysis and seeing what the results are. However, in general this is unwise. The first step in working with quantitative data is to see what your data looks like. Looking at the data can give us a first glance into many aspects of the results, which can be informative for analyses. 3.1.1 Why visualise data? “The greatest value of a picture is when it forces us to notice what we never expected to see.” -John W. Tukey One of the first steps in working with quantitative data is data visualisation, which is the process of graphing it and looking at it. If you work with quantitative data then it should become standard practice for you to graph your data before analysing it. It’s common for many students learning research methods and statistics to simply take a ‘cookie cutter’ approach - that is, collect data, run basic tests on it and call it a day. Sadly this is common even at our level, and you will almost certainly see this happen at music psychology conferences that you go to in future. People will present complex analyses that sound impressive - until you pick up on small cues that suggest they don’t really understand their data at all. Below is an example of why data visualisation should be a crucial part of the quantitative research process: This series of datasets is called the Datasaurus Dozen Links to an external site., which are 12 datasets that look entirely different (including the dinosaur!) but share almost identical summary statistics, as shown by the bold numbers on the right. Data visualisation is important because: It lets you observe patterns in your data. It can reveal unexpected structures in your data that would normally be missed otherwise. It is an effective way of communicating information. The best graphs tell a reader everything they need to know in one image. "],["counts-and-central-tendencies.html", "3.2 Counts and central tendencies", " 3.2 Counts and central tendencies Once we understand what our data looks like, we can then move to describing the general properties of the data. Such general properties are called descriptive statistics. Reporting descriptive statistics is crucial for many aspects of quantitative research. "],["variability.html", "3.3 Variability", " 3.3 Variability The other important part of describing data is in how spread out it is. Is our data tightly bunched together, or is it very spread out? This helps us understand where most of our data falls, as well as how it looks. 3.3.1 The variability of data The other key way of describing data is in its spread, or distribution. The way data is distributed can give key insights into how that data should be treated. Consider the following graphs below. You can see that all three graphs peak at around the same point, but look very different outside of that. The orange line is narrow, while the red line is considerably more spread out. All of these graphs peak at the same point but still look very different. Therefore, they have very different spreads, or distributions. We saw on the last page that we can quantify how far values are spread apart by finding the range. However, this isn’t always a good idea - two datasets with the exact same range can look wildly different. Therefore, we need ways of quantifying how data is spread out as well. 3.3.2 Standard deviation Standard deviation (\\(\\sigma\\) or SD) describes how spread out our data is within our sample, in standard (i.e. comparable) units. Data that is spread out widely (like the red curve above) will have a large standard deviation; likewise, data that has a narrow spread will have a small standard deviation. We’ll touch on this a bit more in the following pages, but for now just remember what a standard deviation is for. To calculate standard deviation, we first calculate variance, which is another measure of spread: \\[ Variance = \\frac{\\Sigma (x_i - \\bar{x} )^2}{n - 1} \\] Or, in human terms: Take each data point (xi) Subtract the mean from each data point (x with the bar) and square that difference Add them all up together Divide by n - 1 And then to calculate standard deviation, we simply take the square root of the variance. \\[ SD = \\sqrt{Variance} \\] Or, in full formula form: \\[ SD = \\sqrt{\\frac{\\Sigma (x_i - \\bar{x} )^2}{n - 1}} \\] Standard deviations (SD) should reported alongside means when results are written up (consult an APA guide). 3.3.3 Standard error, and the SDoTM Imagine that I have a population of 100 regular people (shown on the left). I take a sample of 10 people, measure their heights and then calculate the mean height of that one sample. I then repeat this process over and over again, and plot where each sample’s mean falls. Of course, because every sample is slightly different the mean of each sample will be slightly different too due to sampling error. Some sample means will be lower than the true population mean, while some will be higher. Eventually, we might end up with something like this: The spread of these sample means is called the sampling distribution of the mean (SDoTM), shown on the right. This gives us a sense of where the population mean (the parameter that we are interested in) might lie. With enough samples, the peak of this sampling distribution of the mean will converge around the population mean. As you can see in our hypothetical example, the peak of the sampling distribution of the mean sits pretty close to the original population mean, meaning our estimate is pretty good. The standard error of the mean (standard error; SE) is another measure of variability - this time, it is the spread of sample means across the sampling distribution of the mean. This represents how close our sample mean is to the likely population mean. If our sampling distribution is wide, our standard error will be large - and that means that we won’t have a very precise estimate of the population mean. However, if we have a small standard error that will mean that our sample mean is likely to be close to the population mean. Standard error is calculated using the below formula: \\[ SE = \\frac{SD}{\\sqrt n} \\] Where SD = standard deviation, and n = sample size. Practice: You have a dataset of 400 people. You know that the mean of the DV is 760, with a standard deviation of 40. Calculate the standard error for this sample. "],["distributions.html", "3.4 Distributions", " 3.4 Distributions The ‘shape’ of our data is equally important. What does our data actually look like? Does it even matter what it looks like? The topic of distributions in statistics and probability can make up its own subject (in fact it does), but here we discuss the basics below. 3.4.1 The normal distribution Earlier, we saw a series of graphs overlaid on top of each other. These graphs, while having different variability, were essentially all the same shape - they were symmetrical bell curves. These were all examples of the normal distribution (also called the Gaussian distribution). The classic normal distribution takes on a neat bell-shaped curve: In the normal distribution, the majority of data points cluster in the middle, while all other values are symmetrically distributed from either side from the middle. This is what gives the normal distribution its recognisable bell shape. The normal distribution is defined by two parameters: the mean and the standard deviation of the data. These two parameters define the overall shape of the bell curve - the mean defines where the peak is, while the standard deviation defines how spread out the tails are. An important feature of the normal distribution is where all of the data is spread, regardless of its shape: 95% of the data within the curve falls within 1.96 standard deviations, either side of the mean. This applies to any normal distribution no matter what the scale of the data is. 99.7% of data falls within just below 3 standard deviations. ## Warning in geom_segment(aes(x = 0, y = 0, xend = 0, yend = max(y)), colour = &quot;#556F44&quot;): All aesthetics have length 1, but the data has 1001 rows. ## ℹ Did you mean to use `annotate()`? 3.4.2 Skew Skewness, as the name implies, describes whether or not a distribution is symmetrical or skewed. If a distribution is skewed, we would expect numbers to be bunched up at one end of the distribution. Have a look at the three graphs below: The purple graph in the middle is symmetrically distributed, so we say that it has no skew. The red graph has values that are weighted towards the right-hand side of the x-axis, and so we say that it is either skewed left or negatively skewed. The blue graph, on the other hand is skewed right or positively skewed. The left-right refers to which end the tail of the distribution is on. Skewness can also be quantified numerically: A skewness of 0 means that a distribution is normal A positive skew value means that the data is skewed right A negative skew value means that the data is skewed left As a general rule, if a distribution has a skew greater than +1 or lower than -1, it is skewed. If your data is skewed then this is a bit of an issue; there’s a nice overview of this point and one way of dealing with skew in this blog post. 3.4.3 Kurtosis Kurtosis refers to the shape of the tails specifically. Are all of the data bunched very tightly around one value, or are the data evenly spread out? The three graphs you saw up above all have different kurtoses. The orange graph has most values very close to the peak at 50; therefore, the tails themselves are very small. The red line, on the other hand, is spread out and flatter so the tails are larger. The blue curve again approximates a normal distribution. We can quantify kurtosis through the idea of excess kurtosis - in other words, how far does it deviate from what we see in a normal distribution. This is shown below: The different types of excess kurtoses are: Leptokurtic (heavy-tailed) - tails are smaller. Kurtosis &gt; 1 Mesokurtic - normally distributed. Kurtosis is close to 0 Platykurtic (short-tailed) - tails are larger, and the peak is flatter. Kurtosis &lt; -1 Therefore, in the example above the orange curve would be considered leptokurtic, while the red one would be platykurtic. "],["z-scores.html", "3.5 z-scores", " 3.5 z-scores "],["inferential-statistics.html", "Chapter 4 Inferential statistics", " Chapter 4 Inferential statistics From a conceptual point of view, this week’s module might just be the most important that we cover in this subject. I don’t say that to freak you out, but in many ways it just is the genuine truth. The reason that this might be the most important one is because the topics that we cover in this module are ones that many people frequently misunderstand or misappropriate - to the point of scientific fraud. Inferential statistics are the subject of very heated debate in statistics, primarily about whether p-values and the like are really the right way to do science and statistics. We won’t really go there because that’s neither here nor there in terms of what the aim of this module really is: to not only show you one way of hypothesis testing, but to also equip you with the knowledge needed to understand how other people test hypotheses - or, sometimes, fail to do so. Before you go into this module - make sure that you have 5.4 Variability, 5.5 Distributions and 5.6 Central Limit Theorem firmly under your belt as they will be relevant here. By the end of this module you should be able to: Describe the steps taken to test a statistical null and alternative hypothesis Correctly define a p-value Explain the difference between Type I and Type II error, and how these relate to statistical power Construct a basic confidence interval for a point estimate, and interpret it "],["the-logic-of-hypothesis-testing.html", "4.1 The logic of hypothesis testing", " 4.1 The logic of hypothesis testing Central to all statistical testing is the underlying logic of hypothesis testing. All of the statistical tests that we cover in this module are built on this logic - and so it marks a great place for us to start our venture into inferential statistics. 4.1.1 Revisiting the hypotheses In Module 4, you will have had the chance to write your own hypothesis. This hypothesis guides the overall research and methodological design of our studies. However, you may have noticed that the hypotheses discussed in the video are not quite in the same format. This is because ‘hypothesis’ in this context refers to statistical hypotheses. Statistical hypotheses are formal statements that we use when testing for an effect. As mentioned in the video above, we propose two contrasting hypotheses when we want to do statistical testing: The null hypothesis (H0) - that there is no effect or difference The alternative hypothesis (H1) - that there is an effect or difference We can never know for sure whether one hypothesis is correct over the other. Instead, we choose to either reject or not reject the null hypothesis. 4.1.2 The issue of tails There are two main types of alternative hypotheses: Two-tailed: we hypothesise that something is happening regardless of whether it’s greater, smaller, increasing, decreasing etc… One-tailed: we hypothesise that the effect has a direction In a two-tailed hypothesis, we predict that an effect is occuring but we don’t predict anything beyond that. For example, if we are comparing whether two groups are different our alternative hypothesis would be that they are different - but we don’t predict whether group A will be bigger than group B or vice versa. These are sometimes called non-directional hypotheses. The blue areas on the curve on the right represent a 0.05 level of signifiance for a two-tailed hypothesis test. This is essentially how likely it is that we would observe our effect/difference if nothing was happening. If a difference is large enough that it falls within these blue tails, it is statistically significant. This is because if nothing really was happening, it would be unlikely that we see an effect/difference this big. Therefore, we would choose to reject the null hypothesis at this would fall below our chosen significance level. In a one-tailed hypothesis, we predict that the effect exists in a specific direction (hence, they are directional hypotheses). For example, we might predict that Group A is bigger than Group B. Or, we might predict that as X increases, Y increases too. The two curves below show the significance levels for one-tailed hypotheses. If we predicted that Group A was smaller than Group B, we would only reject the null hypothesis if Group A really was smaller than Group B, and this difference was large enough to be statistically significant (i.e. within one of the green tails). "],["p-values.html", "4.2 p-values", " 4.2 p-values At one point, the video on the previous page talks about ‘levels of significance’ - what does this actually mean? Here, we’ll talk about the p-value - one of the most commonly used, and possibly abused, concepts in research and statistics. We’ll talk about what the p-value actually means, and tie it to two related concepts later in this module: error and statistical power. 4.2.1 The definition of the p-value The definition of the p-value is the following (APA Dictionary, 2018): “in statistical significance testing, the likelihood that the observed result would have been obtained if the null hypothesis of no real effect were true.” What does this actually mean? We actually already touched on this in the previous page, but let’s dig a bit deeper. 4.2.2 A brief probability primer, and how this relates to p-values To start with, a p-value is a probability. A probability, of course, ranges from 0 to 1; 0 (0%) representing an impossible result, and 1 (100%) representing a certain result. Probabilities can be conditional, meaning that they are dependent on a certain condition being true. A p-value is a conditional probability. Specifically, a p-value refers to the probability of getting a particular result, assuming the null hypothesis. To illustrate what I mean, consider the two graphs below, showing some example data as points. On the right is an example of what one possible alternative hypothesis might look like - that this data is meaningfully represented by two underlying distributions or groups, and that there is a difference between these two groups (shown as the blue and red curves). Contrast that with the left graph, where we hypothesise that there is no difference - in other words, that the data can be captured by the one distribution. This graph on the left is a representation of our null hypothesis, and as per the definition of the p-value, this is the distribution we focus on. The area shaded blue represents the probability of getting that specific result. In the left example, the area shaded represents the probability of getting a value lower than x = 1. You can see that the blue area is quite big, so the probability of getting a value lower than 1 is quite high. On the right hand side is another example, which represents the probability of getting a value of x higher than 2. Here, the shaded area is small, so the associated probability is low. Now, recall the following figure from last week: norm_data &lt;- data.frame( x = seq(-3, 3, length = 1001) ) %&gt;% mutate( y = dnorm(x) ) ggplot(norm_data, aes(x, y)) + geom_line(linewidth = 1, colour = &quot;transparent&quot;) + stat_function(fun = dnorm, xlim = c(1.96, 3), geom = &quot;area&quot;, fill = &quot;#556F44&quot;, alpha = 0.8, colour = &quot;#556F44&quot;) + stat_function(fun = dnorm, xlim = c(-1.96, 1.96), geom = &quot;area&quot;, fill = &quot;#95BF74&quot;, alpha = 0.8, colour = &quot;#95BF74&quot;) + stat_function(fun = dnorm, xlim = c(-3, -1.96), geom = &quot;area&quot;, fill = &quot;#556F44&quot;, alpha = 0.8, colour = &quot;#556F44&quot;) + annotate(&quot;text&quot;, x = 0, y = 0.2, label = &quot;95%&quot;) + annotate(&quot;text&quot;, x = 2.5, y = 0.05, label = &quot;2.5%&quot;) + annotate(&quot;text&quot;, x = -2.5, y = 0.05, label = &quot;2.5%&quot;) + theme_pubr() + labs(x = &quot;Standard deviations&quot;, y = &quot;&quot;) + theme( panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_rect(fill = &quot;transparent&quot;,colour = NA), plot.background = element_rect(fill = &quot;transparent&quot;,colour = NA), axis.line.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank() ) + scale_x_continuous(breaks = c(-1.96, 0, 1.96)) This is basically the thinking we use when we calculate a p-value. Let’s break down the steps: We first assume the null hypothesis is true, and so we use the distribution of the null hypothesis to calculate the probability of getting our result or greater. To figure out where our result(s) sits on this null distribution, we calculate a test statistic. This test statistic is essentially a value that represents where our data sits on the test (null) distribution. Each test statistic is calculated in a different way because every distribution looks different, so we’ll come back to this over the next couple of weeks. Afterwards, we calculate the probability of getting our test statistic (or greater) using a similar logic to the example above. This probability is our p-value. This marks the probability of observing this result or greater. Once we have a p-value we then compare this against alpha, which is our chosen significance level (usually p = .05). If the probability of getting our result is smaller than this (pre-defined) cutoff, it means that it is unlikely assuming the null hypothesis is true, and therefore our result is statistically significant and we reject the null hypothesis. So in essence, if a p-value is p = .05 we’re saying that assuming the null is true, there is a 5% chance of observing this result or greater. Likewise, an extremely small p-value (e.g. p = .0000000001) means that assuming the null is true, the probability of getting the data we have is extremely small. The logic, therefore, is that there must be an alternative explanation. 4.2.3 The debate around p-values Throughout history, p-values have been so misunderstood and misappropriated that some journals, such as Basic and Applied Social Psychology, actually either discourage or outright ban the reporting of p-values. This ties into a wider debate about the usefulness or meaningfulness of the hypothesis testing approach outlined on the previous page, with a number of academics and scientists arguing that it is time to do away with the system as a whole. The debate is something that goes beyond the scope of the subject, so we won’t be taking a strong stand on it either way. What we do think though is that it’s really important that you understand how hypothesis testing works and what p-values can or can’t tell you. "],["types-of-error.html", "4.3 Types of error", " 4.3 Types of error Nothing in life is perfect, and that applies to the inferential statistics that we do. Every sample will differ slightly from one to another due to inherent sampling error; in a similar way, whenever we do an inferential test about a population from a sample, we always run the risk of making an error in the decisions that we make. 4.3.1 Statistical error Imagine an old man is seeing his local GP complaining of a headache. Upon examination, the doctor concludes that the old man is pregnant. A pregnant woman in her last trimester then comes in to the same GP. Despite her numerous pregnancy-related complaints, the doctor concludes that she is not pregnant. Both of these scenarios (while hopefully very unlikely!) are obviously forms of errors on the doctor’s part. In the first scenario, the doctor has accepted a diagnosis that is very clearly wrong. In the second scenario, the doctor has rejected the correct diagnosis. The same kind of logic applies directly to quantitative research. We want to be sure that when we observe a result, that result is actually likely. We therefore want to minimise the possibility of errors like above. 4.3.2 Type I error and alpha A moment ago we talked about alpha, or the significance level, from the previous sections about hypothesis testing and the p-value. Alpha is the same here as it is there - it is the probability of making a Type I error - that we incorrectly reject the null hypothesis when the null hypothesis is actually true. Essentially, alpha is the rate of Type I error we’re willing to accept whenever we do a hypothesis test. We generally set alpha as p &lt; .05 out of convention - i.e. most of the time, we’re willing to accept a 5% chance of a Type I error rate. However, we can set alpha to anything we want. Sometimes, we may set it lower (more on this in a few weeks’ time). "],["statistical-power.html", "4.4 Statistical power", " 4.4 Statistical power Another related but crucial consideration for inferential statistics is the concept of statistical power. Without spoiling too much about what it means here, below is an overview of what this concept is and why it is important. 4.4.1 Statistical power Power in a statistical context essentially describes how likely we are to actually detect an effect given our sample size. Mathematically, power is defined as \\(1 - \\beta\\), which in English terms means that it is the probability of not making a Type II error. Power is expressed as a percentage. For example, if your study has 50% power, it means it has an 50% chance of actually detecting an effect. The most common guideline is to aim for a study with 80% power. 4.4.2 Factors that affect power The primary factor (within your control) that affects how much statistical power you have to detect an effect is sample size. Think back to the formula for standard error, as a proxy explanation as to why this is the case. Larger samples tighten the sampling distribution of the mean, and so two distributions will overlap less and less the greater the sample sizes are. Therefore, if there is less overlap there is greater space to detect an effect. Some other factors that can affect power are: The effect size - how large is the difference between your groups, etc? If you’re trying to detect very small effects, you need much more power to detect it compared to larger ones. Performing a one-tailed test - because effects are only being tested in one direction, this alters the p-value (it actually halves it; a two-tailed p = .10 is a one-tailed p = .05). Don’t do this though, because there are very few instances in which you can justify using a one-tailed test without reviewers and other clued-in readers suspecting that you’re intentionally fudging your power. Increase alpha - for a semi-detailed explanation of why, see here. In addition, there is a nifty tool here that lets you see what happens to error rates when you change specific parameters: Link to the tool The consequence of being underpowered means that you can miss effects that exist. A good proportion of studies in psychology are underpowered, meaning that effects are being missed where they exist. Power is therefore an integral consideration of good study design, particularly for experimental contexts. Consider two hypotheses - a null and an alternative hypothesis. As long as we sample a population, the two will always overlap (even if that overlap is really small). The area shaded in bright green below represents where the two distributions overlap with each other. This is where the null hypothesis might be rejected when it shouldn’t be, or vice versa. Let’s assume an alpha of .05, and divide this region accordingly. There is a certain probability of alpha (where the null is incorrectly rejected), as well as a probability of beta (where the null is not rejected when it should be). What happens if we were to decrease alpha? (i.e. reduce our Type I error rate)? All else being equal (sample size, effect size etc), we can see that alpha now takes up less space in the overlapping area - and so, beta (Type II error) will increase. Subsequently, if we were to increase alpha we can see the opposite; beta will decrease, because now the overlap is predominantly now covered by the rejection region covered by alpha. 4.4.3 Power analyses Identifying an appropriate level of statistical power is an important part of planning quantitative research. Before conducting a study, it is wise to run a power analysis. Doing a power analysis allows you to identify how many participants you may need in order to reliably detect an effect of a given size. Most modern statistics software will allow you to conduct power analyses: SPSS (from version 27) Jamovi (with the jpower module) R (with the pwr package, among many others) We won’t get too into the maths here of how this is done (it heavily depends on your research design, e.g. how many groups you have, what test you plan on doing…). These programs will let you select the appropriate design you want to test, and choose the size of the effect you want and your alpha level. The power analysis will then give you a minimum sample size per group for you to achieve that level of statistical power. 4.4.4 Post-hoc power analyses You might see authors in some papers where you present a power analysis after collecting your sample and doing your analyses. Supposedly, this is to show that your sample had enough power to detect an effect. However, this is conceptually flawed. The primary flaw is that post-hocs are essentially just restatements of your p-values and so do little to show the true power of a design/test. "],["confidence-intervals.html", "4.5 Confidence intervals", " 4.5 Confidence intervals As we saw on the page about p-values, some people are vocal about their distaste for relying solely on p-values for decision making. One way of augmenting our estimates is to calculate a confidence interval for each estimate we make. Confidence intervals provide an estimate of the precision of our estimate, and so are a crucial concept to know about. 4.5.1 A reminder from the previous module It’s time to force you to remember what the graph below means once again (I promise this is the last time you will see this figure! I think). By now you should be very familiar with what this graph shows; namely, that 95% of the data in a normal distribution lies within 1.96 standard deviations either side of the mean, yada yada. ## Warning in geom_segment(aes(x = 0, y = 0, xend = 0, yend = max(y)), colour = &quot;#556F44&quot;): All aesthetics have length 1, but the data has 1001 rows. ## ℹ Did you mean to use `annotate()`? This sounds all well and good, but this provides us with some useful information. If all that business about values within 1.96 standard deviations is still fresh in your mind, this next statement should be a no-brainer: if 95% of our data on a normal distribution lies within 1.96 SD either side of the mean, that means that there is a certain range of values that 95% of our data falls in. For example, say we have a sample of scores on a test, with a mean of 70 and a standard deviation of 4. If we want to know where 95% of the scores lie in this sample, we would do the following calculation: Using this formula, we can calculate the values where 95% of the data lie: In other words, 95% of our data in this sample lie between 62.16 and 77.84. The remaining 5% of the sample lie above or below these values. 4.5.2 Confidence intervals We can use the same principle to make inferences about the true population parameter. When we take a sample, each one will have its own standard error (remember this reflects an estimate of the distance between the sample mean and the true population mean). If we were to repeatedly take samples, in the long run we would expect the true population mean to fall within 95% of all sample means. And, just like the normal distribution, when we look at the sampling distribution of the means below, 95% of all sample means will fall within 1.96 standard errors (thanks to the Central Limit Theorem). norm_data &lt;- data.frame( x = seq(-3, 3, length = 1001) ) %&gt;% mutate( y = dnorm(x) ) ci_figure &lt;- ggplot(norm_data, aes(x, y)) + geom_line(size = 1, colour = &quot;transparent&quot;) + stat_function(fun = dnorm, xlim = c(1.96, 3), geom = &quot;area&quot;, fill = &quot;#3d7ab8&quot;, alpha = 0.8, colour = &quot;#3d7ab8&quot;) + stat_function(fun = dnorm, xlim = c(-1.96, 1.96), geom = &quot;area&quot;, fill = &quot;#7fbaf5&quot;, alpha = 0.8, colour = &quot;#7fbaf5&quot;) + stat_function(fun = dnorm, xlim = c(-3, -1.96), geom = &quot;area&quot;, fill = &quot;#3d7ab8&quot;, alpha = 0.8, colour = &quot;#3d7ab8&quot;) + annotate(&quot;text&quot;, x = 0, y = 0.2, label = &quot;95% of sample means\\nwill fall within this range&quot;) + # annotate(&quot;text&quot;, x = 2.5, y = 0.05, label = &quot;2.5%&quot;) + # annotate(&quot;text&quot;, x = -2.5, y = 0.05, label = &quot;2.5%&quot;) + theme_pubr() + labs(x = &quot;Standard errors&quot;, y = &quot;&quot;) + theme( panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_rect(fill = &quot;transparent&quot;,colour = NA), plot.background = element_rect(fill = &quot;transparent&quot;,colour = NA), axis.line.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank() ) + scale_x_continuous(breaks = c(-1.96, 0, 1.96)) ci_figure With this important property in mind, we can calculate a 95% confidence interval. This is an estimate of the range of values for our estimate of the parameter. In other words, it is a measure of precision. The formula for a 95% confidence interval, as it turns out, is exactly the same as above with one change: \\[ 95\\% CI = M \\pm (1.96 \\times SE) \\] Therefore, if we have an estimate of a population parameter (e.g. what the population mean is), we can use a 95% CI around that estimate to quantify the precision/uncertainty of that estimate. If a confidence interval is narrow, it suggests our estimate is quite precise. This can be extremely informative at a glance. For example, pretend that we have a null hypothesis that a parameter is equal to 0 (as is usually the case). If we calculate a confidence interval and that happens to include the value of 0 (e.g. 95% CI: [-0.5, 1.5]), we can immediately infer that 0 is a likely value for this parameter - and thus, the null hypothesis is plausible. On the other hand, if the CI did not include 0 then we could infer that there likely is a significant effect. Not all confidence intervals though will contain the true parameter purely because of how samples work (i.e. the inherent error between a sample and a population). In addition, it does not mean that there is a 95% chance a single interval will contain the true parameter. So what does the 95% part refer to? 4.5.3 Confidence The confidence level is a long-run probability that a group of confidence intervals will contain the true population parameter. A 95% confidence level means that if we were to take samples repeatedly and calculate a CI for each one, 95% of those CIs will contain the true population parameter in the long-run. Or, say if you were to take 100 samples and calculate a CI for each one, 95 of them would include the true population parameter. The choice of 95% is conventional, like alpha (our significance criterion); we can (but often don’t) change our level of confidence. This changes the relevant formula for calculating the interval, as in the examples below: \\[ 90\\% CI = M \\pm (1.645 \\times SE) \\] \\[ 99\\% CI = M \\pm (2.576 \\times SE) \\] Notice that the value we multiply the SE by has changed. If you have been especially observant so far, you may have figured out what these values are: they’re z-scores! 4.5.4 Confidence intervals in literature On the page about p-values, I left a brief note around some of the current discourse around the usefulness (or uselessness) of p-values. Proponents of getting rid of p-values/moving away from them advocate strongly for two alternatives: a) effect sizes (self-explanatory; we will come to this) and b) confidence intervals, to show the range of long-run plausible values for the estimate. Again, we won’t be taking an especially strong stance either way. That being said, it is now fairly common practice to report 95% confidence intervals alongside the results of significance tests for transparency. Programs like Jamovi will usually calculate these intervals automatically for you. "],["chi-squares.html", "Chapter 5 Chi-squares", " Chapter 5 Chi-squares .math { font-size: x-large; } While many of the things we are interested in when it comes to psychological research are continuously distributed (height, weight, reaction time, personality), there are many instances where we will need to work with data that is categorical. This can involve categorical independent variables (e.g. assigning participants to one or two groups) or categorical outcomes (e.g. responding yes or no). We’ll start with looking at relationships between these categorical variables, and testing for significant associations. This module will see you diving deep into Jamovi again - so be prepared to get hands on with a bunch of data! Between the seminar, the worked examples and exercises there are at least 6 different datasets to play around with for this week! By the end of this module you should be able to: Describe how a chi-square statistic is calculated Conduct two forms of chi-square tests: goodness-of-fit and tests of independence Calculate and interpret an appropriate effect size for tests of independence "],["calculating-a-chi-square.html", "5.1 Calculating a chi-square", " 5.1 Calculating a chi-square We’ll start this module off by briefly going through the basics of what research designs suit chi-square tests, as well as the basic maths underlying the first part of the statistical test. 5.1.1 Categorical data As mentioned at the start of this module, chi-square tests are used when we work with categorical data - i.e., when we are dealing with counts of items or people, rather than continuous variables. Research questions focused on relationships or associations among categorical variables are suited to chi-square tests. Every categorical variable will have levels (categories) within them. These are the different values that categorical variable can be. For example, biological sex is often coded with two levels: male and female. Or perhaps you might categorise socioeconomic status into three levels: low, medium and high bands. This is something that can form a core part of your research design. The most basic example is asking participants what their biological sex is - participants will respond with one of the two categories. Alternatively, you can create categories from existing data. For example, many scales designed to assess psychological disorders such as depression and anxiety often have ‘cutoff’ points, where a certain score on the scale is indicative of a possible disorder. If you have everyone’s raw scores, you can convert these scores into categories depending on whether they are above or below these cutoff points (though this needs to be strongly justified). All of this kind of data are amenable to chi-square tests, if you are interested in relationships between categorical variables. The family of chi-square tests basically work by comparing the observed values to the expected values. As the names imply, observed value simply means the number of observations we have in each category or level (i.e. what our data actually is). Expected values, on the other hand, are the number of things we would expect to see under the null hypothesis. We can visualise categorical variables in two basic ways: a) a contingency table or b) a bar graph of counts. Below is the same set of data, shown in both forms: 5.1.2 The chi-square formula To test whether a result is significant, remember that we need to calculate a test statistic, and see where that fits on its underlying distribution. Here, our test statistic is handily named the chi-square statistic. To calculate the chi-squared test statistic we use the following formula: \\[ \\chi^2 = \\Sigma \\frac{(O-E)^2}{E} \\] Where: O = observed value E = expected value The English translation of that above formula can be described in four steps: Calculate observed count - expected count Square that difference Divide it by the expected count Do this for each cell, and add them all up We’ll look at this in more detail when we look at the actual tests. For the time being, here’s the key takeaway: have a look at the two graphs below, representing the observed and expected values of two different datasets. Have a think about the following: What is each graph telling you? The left graph appears to show noticeable differences between the observed and expected values. Based on the mathematical formula above, what will happen to the chi-square value? Each graph represents one set of data. Based on your answers to a) and b) above, which one of the two would you expect to demonstrate a significant effect? "],["the-chi-square-distribution.html", "5.2 The chi-square distribution", " 5.2 The chi-square distribution On the previous page we introduced the formula for a chi-square test statistic. But what do we do with it? We’ll go through this below in detail, given that this is the first time we’re coming across an actual test. While a computer will do all of this stuff automatically, it’s useful to know the actual mechanisms underlying the test. 5.2.1 The chi-square distribution Recall from Week 6 about how hypothesis tests work - we calculate a test statistic that conforms to a particular distribution, and we assess how likely our observed test statistic is (or greater) on this distribution, assuming the null hypothesis is true. This gives us the p-value for that test. With that in mind, the chi-square distribution that underlies the chi-square test looks something like this: The shape of the chi-square distribution is only dependent on the degrees of freedom (df). We’ll look at how to calculate degrees of freedom for various tests, including the family of chi-square tests, as we move along the subject. Here is the same set of lines above, but shown in their own plot this time: As you can hopefully see, the chi-square distribution is very heavily skewed at lower degrees of freedom (and therefore in smaller sample sizes). As degrees of freedom increase though, it approximates a normal distribution. For instance, here’s what the chi-square distribution looks like when df = 100: 5.2.2 Hypothesis testing in chi-squares At this point, also recall that p-values are the probability that we would get our observed result (or greater), assuming the null hypothesis is true. Here is our first application of this concept. When we use a chi-square test, we are performing the following basic steps: Establish null and alternative hypotheses Determine alpha level (in this case, alpha = .05 as always) Calculate our test statistic - here, this is the chi-square statistic Compare our chi-square test statistic against the chi-square distribution Calculate how likely we would have seen our chi-square value or greater on this distribution a. Or, alternatively, establish a critical chi-square value - the value that must be crossed for a result to be significant We’ll expand on this more in the next couple of pages. 5.2.3 Calculating significance Let’s say that we have a df of 5. How do we know where the critical chi-square value is? For that, we consult a chi-square table. This table gives the critical chi-square value at a set degrees of freedom and alpha level. These are freely available online, but here’s a short excerpt. To read this table: The far-left column lists different degrees of freedom. We need to find the row that corresponds to df = 5. Each column provides critical chi-squares at different alpha levels. We want to find the column that says .05. The number at both of these points tells us the critical chi-square value. So, for a df = 5 and an alpha = .05, the critical chi-square value is 11.07. If our own chi-square value is greater than this, the probability of that value or greater occurring (assuming the null) will be less than 5%; i.e. the boundary for statistical significance. In picture form: Hopefully that makes sense - this kind of logic is pretty much identical to the other tests that we will cover in the coming weeks! "],["goodness-of-fit.html", "5.3 Goodness of fit", " 5.3 Goodness of fit Now that we’ve covered the conceptual groundwork for chi-square tests in general, we can now start looking at actual tests that can help us answer research questions. The most basic chi-square test is the goodness of fit test. 5.3.1 Goodness of fit tests Goodness of fit tests are used when we want to compare a set of categorical data against a hypothetical distribution. Goodness of fit tests require one categorical variable - as the name implies, a goodness of fit test looks at whether the proportions of categories/levels in this variable fits an expected distribution. In other words, do our counts for each category match what we would expect under the null? “Distribution” in this context means probability distributions, and can apply to a wide range of scenarios. For the purposes of what we’re learning here, we’ll stick to a question along the lines of: “do the categories of variable X align with their expected probabilities”? 5.3.2 Example An example question that we look at in the seminar is Do Skittle bags have an even number of each colour? In the seminar, we go through whether or not a random bag has an even split of colours. Here on Canvas, we’ll now tackle their equally delicious rivals, M&amp;Ms. The data and analysis come courtesy of Rick Wicklin, a data analyst at SAS (who also make statistics software). You can read his full blog here: The distribution of colors for plain M&amp;M candies - The DO Loop (sas.com) Links to an external site.. We’ll be recreating Rick’s first analysis here. We’re sticking with the candy theme because a) they’re delicious and b) the M&amp;Ms are a great way to introduce what to do when expected proportions are not equal. M&amp;Ms come in six colours: red, blue, green, brown, orange and yellow. Unlike Skittles, these colours are not distributed equally within each bag of M&amp;Ms. In 2008, Mars (the parent company) published the following percentage breakdown of colours: 13% red 20% orange 14% yellow 16% green 24% blue 13% brown Rick collected his data in 2017, and so was interested in seeing if the proportions observed in his 2017 sample of M&amp;Ms aligned with the distribution of colours listed in 2008. A goodness of fit is the perfect test for this scenario because: We are making a claim about a distribution Our variable (colour) is categorical A chi-square goodness of fit will allow us to test whether the distribution of colours in a sample of M&amp;Ms aligns with the published proportions. Here’s our dataset: mnm_data &lt;- read_csv(here(&quot;data&quot;, &quot;week_7&quot;, &quot;W7_M&amp;M.csv&quot;)) head(mnm_data) 5.3.3 Calculating \\(\\\\\\chi^2\\) In goodness of fit tests, we first calculated the expected frequencies. In this example though, the expected proportions aren’t equal - and so we have to be mindful of this when calculating expected values. See the expected count cell for Red M&amp;Ms for how expected value is worked out in this instance. We can draw this up in table form alongside our own data: colour n expected_prop expected Blue 133 0.24 170.88 Brown 96 0.13 92.56 Green 139 0.16 113.92 Orange 133 0.20 142.4 Red 108 0.13 \\(712 \\times 0.13 = 92.56\\) Yellow 103 0.14 99.68 Then we would use the formula we saw on the previous page to calculate a chi-square statistic. However, since we’re doing this in R we’ll skip the manual maths. \\[ \\chi^2 = \\Sigma \\frac{(O-E)^2}{E} \\] If we have a look at the observed vs expected values, we might have a good idea of what’s going on already: 5.3.4 Using R Using Jamovi By default, the chisq.test() function in R will assume that your categories have an equal chance of happening. However, in this instance we know that the colours are not evenly distributed. To ensure the proper probabilities are set beforehand, this needs to be specified by giving the p argument within chisq.test(). Note that the order of the expected probabilities needs to match the order they appear in the dataset (R will generally order these alphabetically unless told otherwise): # This pulls the relevant variable directly w7_mnm_table &lt;- table(mnm_data$colour) w7_mnm_chisq &lt;- chisq.test(w7_mnm_table, p = c(0.24, 0.13, 0.16, 0.20, 0.13, 0.14)) 5.3.5 Output Here’s what our output looks like. First is our table of proportions. This can be really useful in laying out the data and seeing where the differences between observed and expected proportions might lie. w7_mnm_chisq$observed ## ## Blue Brown Green Orange Red Yellow ## 133 96 139 133 108 103 w7_mnm_chisq$expected ## Blue Brown Green Orange Red Yellow ## 170.88 92.56 113.92 142.40 92.56 99.68 Next, here is our test output. The result is significant (p = .004), suggesting that the 2017 bag of M&amp;Ms does not follow the same distribution of colours as the 2008 values. (If you read the rest of Rick’s blog post, it turns out that somewhere between 2008 and 2017 they changed where M&amp;Ms are made, and actually split production across two factories that produce different distributions of colours. Rick eventually found out that his bag most likely came from one of the plants, although Mars has not made these proportions public like they used to.) w7_mnm_chisq ## ## Chi-squared test for given probabilities ## ## data: w7_mnm_table ## X-squared = 17.353, df = 5, p-value = 0.003877 "],["tests-of-independence.html", "5.4 Tests of independence", " 5.4 Tests of independence Chi square tests of independence are used when we want to test whether two categorical variables are associated with each other (i.e. show a relationship). Some examples of this question might take on the following: Is smoking history (yes/no) associated with lung cancer diagnosis? (yes/no) Is there an association between gender and employment status? 5.4.1 Example scenario We’ll start off with a very basic example. In the below dataset, children from several schools were surveyed regarding what instrument they played. This dataset focuses on two instruments that have historically been seen as gendered (e.g. see Abeles 2009) - clarinet and drums. The sex of the child playing the instrument was also recorded. Our research question is: is there an association between sex and instrument choice? Dataset: instrument_data &lt;- read_csv(here(&quot;data&quot;, &quot;week_7&quot;, &quot;W7_instruments.csv&quot;)) ## Rows: 122 Columns: 2 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): instrument, sex ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(instrument_data) Contingency tables The primary way of ‘drawing up’ categorical data, particularly when two variables are involved, is to draw a contingency table. A contingency table is a two-way table that shows how many participants/items/objects fall under each combination of our two variables. Here is a contingency table of our data below: w7_instrument_table &lt;- table(instrument_data$instrument, instrument_data$sex) w7_instrument_table %&gt;% addmargins() ## ## F M Sum ## clarinet 51 48 99 ## drums 6 17 23 ## Sum 57 65 122 5.4.2 Expected frequencies To calculate expected frequencies in a two-way contingency table (i.e. a test of independence), we use the following formula: \\[ E = \\frac{R \\times C}{N} \\] Where R = row total and column = column total. Let’s put this into practice with girls who play the clarinet (highlighted above). The row total for this cell is 99 (i.e. total number of clarinet players). The column total is 57 (total number of girls). To calculate an expected value for this cell, we would therefore calculate the following: \\(E = \\frac{99 \\times 57}{122}\\) This works out to be roughly 46.25 - which means that we would expect roughly 46 female clarinet players. We then go through and calculate this for each cell, so that we have all of our expected values. Once we’ve done that, we can then calculate our chi-square test statistic using the same formula as always: In R 5.4.3 Output Here’s our output! Firstly, our contingency table: w7_inst_chisq &lt;- chisq.test(w7_instrument_table, correct = FALSE) w7_inst_chisq$observed ## ## F M ## clarinet 51 48 ## drums 6 17 w7_inst_chisq$expected ## ## F M ## clarinet 46.2541 52.7459 ## drums 10.7459 12.2541 Next is our chi-square test output. As you can see, our test of independence suggests a significant result (p = .03). In other words, we reject the null hypothesis that there is no association between instrument and sex. w7_inst_chisq ## ## Pearson&#39;s Chi-squared test ## ## data: w7_instrument_table ## X-squared = 4.848, df = 1, p-value = 0.02768 Lastly we have this output. Hold onto this for now - we’ll look at this in more detail over the next page. "],["effect-sizes-for-chi-squares.html", "5.5 Effect sizes for chi-squares", " 5.5 Effect sizes for chi-squares This is the first time we’re coming across effect sizes for any test - thankfully, we start with relatively easy ones to wrap your head around. We will cover two effect sizes: phi and Cramer’s V, both of which apply when conducting a test of independence. 5.5.1 Phi Phi is an effect size for chi-squares that applies only to 2x2 designs. The formula for phi is: \\[ \\phi = \\sqrt{\\frac{\\chi^2}{n}} \\] Essentially, it is the chi-square test statistic divided by the sample size, which is then square rooted. Again, it only works for 2x2 designs - i.e. each categorical variable can only have two categories within it. 5.5.2 Cramer’s V Cramer’s V is another effect size for chi-squares, but one that can be used for anything beyond a 2x2 design as well. The formula for Cramer’s V is similar: \\[ V = \\sqrt{\\frac{\\chi^2}{n(k-1)}} \\] Here, k refers to the number of groups in the variable with the lowest number of groups. So for example, in a 2x3 design, one variable has 2 levels and the other has 3; k = 2 in this instance. Phi and Cramer’s V can both be calculated in R with the following functions. Like chisq.test(), both functions will work if you give them a contingency table. (Note that for phi(), digits = 3 has been set to false just for display purposes. In rstatix::cramer_v(), correct = FALSE gives an equivalent result to Jamovi.) psych::phi(w7_instrument_table, digits = 3) ## [1] 0.199 rstatix::cramer_v(w7_instrument_table, correct = FALSE) ## [1] 0.1993439 An alternative is to use the effectsize package, which also helpfully calculates 95% CIs. effectsize::phi(w7_instrument_table, adjust = FALSE) effectsize::cramers_v(w7_instrument_table, adjust = FALSE) 5.5.3 Interpretation Phi and Cramer’s V are essentially both correlation coefficients (more on this in Week 10). Both phi and Cramer’s V can only be between 0 and 1. For this subject, the size of Cramer’s V and phi can be interpreted as follows: If the effect size = .10, the effect is small If effect size = .30, the effect is medium If effect size = .50 or above, the effect is large 5.5.4 Practice Given the following results from a 2x2 chi-square test of independence: \\(\\chi^2\\) = 5.45 N = 46 Calculate both phi and Cramer’s V. (You should get the same answer, but have a go at trying it both ways!) "],["t-tests.html", "Chapter 6 t-tests", " Chapter 6 t-tests t-tests are usually one of the first families of statistical tests that students learn when they take a research methods subject. I (Dan) pretty much learnt only t-tests until my third year of my psychology major (I learnt chi-squares and other tests through taking separate statistics subjects through my uni’s Department of Statistics before I learnt them in psychology). It’s not hard to see why this is the case - t-tests are really intuitive and simple to conduct, and so are an accessible way into learning statistical tests (even though chi-squares are even easier). The family of t-tests come into play when we have one categorical IV with two levels, and one continuous DV. As you can imagine, there are many instances where this kind of design comes into play, and you will see as much in the datasets and examples this week. There are nine datasets for you to play around this week (3 for each kind of test) - so hopefully that will give you plenty of practice! By the end of this module you should be able to: Describe how a t-test works in principle Conduct three forms of chi-square tests: one-sample, independent-samples and paired-samples Calculate and interpret an appropriate effect size for the above tests "],["t-tests-and-the-t-distribution.html", "6.1 t-tests and the t-distribution", " 6.1 t-tests and the t-distribution We begin this week’s module in much the same way we went through last week’s. We’ll look at the shape of the underlying distribution, and what determines the shape of that distribution. On this page we’ll also go through what the basic premise of the t-test is. 6.1.1 What is a t-test? The family of t-tests, broadly speaking, are used when we want to compare one mean against another. This can take on three major forms, which we will go into later in the module: Is this sample mean different from the population mean? Are these two group means different? Is the mean at point 1 different to the mean at point 2? All of these instances require a comparison between two means, which the t-tests will allow you to test for. So, in a general sense our hypotheses would be something like: \\(H_0\\): The means between the two groups are not significantly different. (i.e. \\(\\mu_1 = \\mu_2\\)) \\(H_1\\): The means between the two groups are significantly different. (i.e. \\(\\mu_1 \\neq \\mu_2\\)) The question of when t-tests should be used is hopefully somewhat obvious - in general, we use them we want to compare two means with each other. The important part is what kind of means they are: If we compare one sample mean to a hypothesised population mean, this is a one-sample t-test If we compare two group means, this is an independent samples t-test If we measure one group twice and compare the two means, this is a paired-samples t-test. In this module, we’ll go through all three (but will emphasise the latter two especially as they see the most use). 6.1.2 The t-statistic Last week we introduced the chi-square statistic, which is the value that we use when we want to assess whether a result is significant. When we want to assess whether a difference between two means is significant we calculate a different test statistic, which (as the name implies) is the t-statistic. While you won’t be required to calculate this by hand this week, it would be good to wrap your head around the below formula so you understand how it works in principle: \\[ t = \\frac{M_1 - M_2}{SE} \\] This is how t is calculated conceptually - it is the difference in the two means over the standard error of the mean. Note that the world conceptually is stressed here because the actual formula is slightly different for each t-test, but all work on this above principle. Again, we won’t go through the maths of this in detail - this is beyond the scope of the subject! 6.1.3 The t-distribution By now, hopefully you’re comfortable with the idea that we use our test statistic and find its position on its underlying probability distribution in order to calculate the p-value. The underlying distribution of this test is the t-distribution, which is depicted below. Note that like the chi-square distribution, degrees of freedom is the only parameter that determines its shape: t_data &lt;- tibble( x = seq(-5, 5, length = 100), df_1 = dt(x, 1), df_2 = dt(x, 2), df_5 = dt(x, 5) ) t_data %&gt;% pivot_longer( cols = df_1:df_5, names_to = &quot;df&quot;, values_to = &quot;t&quot; ) %&gt;% mutate( df = factor(df, labels = c(&quot;df = 1&quot;, &quot;df = 2&quot;, &quot;df = 5&quot;)) ) %&gt;% ggplot( aes(x = x, y = t, colour = factor(df)) ) + geom_line(size = 1) + labs( x = &quot;x&quot;, y = &quot;Density&quot;, colour = &quot;df&quot; ) + theme_pubr() The one key difference between the t-distribution and the chi-square distribution from a mathematical point of view is that the t-distribution is symmetrical, much like the normal distribution (although they are not the same). Therefore, it is possible to get a negative t test statistic; however, this simply reflects the order in which the groups are being compared. E.g. Say that Group 1 - Group 2 gives a test statistic of t = 1.5. If you were to enter the groups as Group 2 - Group 1 instead, the t would be -1.5. This simply reflects the ordering of the groups. 6.1.4 The t-table Once again, like the chi-square we have a beautiful little table for calculating a critical t-value. We won’t go into too much depth over how this works because it works exactly like how it does for chi-squares - find the row corresponding to your degrees of freedom, then find the column corresponding to your alpha level. Yes, there are a lot of cross-references to what we covered last week with chi-squares - and that’s a good thing! The point here is that conceptually, the process of testing hypotheses using t-tests is exactly the same as what we did with chi-squares, but the specific design and maths are different. "],["one-sample-t-test.html", "6.2 One-sample t-test", " 6.2 One-sample t-test The first test that we’ll look at is the one-sample t-test, which is the most simple of the three that we will look at this week. 6.2.1 One-sample t-test A one-sample t-test is used when we want to compare a sample against a hypothesised population value. It is useful when we already know the expected value of the parameter we’re interested in, such as a population mean or a target value. The basic hypotheses for a three-way interaction are: \\(H_0\\): The sample mean is not significantly different from the hypothesised mean. (i.e. \\(M = \\mu\\)) \\(H_1\\): The sample mean is significantly different from the hypothesised mean. (i.e. \\(M \\neq \\mu\\)) It’s worth noting that one-sample t-tests aren’t that commonly used because they require you to know the population value (or, if you hypothesise a value, you need to justify why). However, they’re included here because they’re still a part of the t-test family, and they serve as a nice introduction to how t-tests work. 6.2.2 Example data Historically, scores in a fictional research methods class average at 72. This year, you are the subject coordinator for the first time, and you notice that last year’s cohort appear to have really struggled. You want to see if there is a meaningful difference between the cohort’s average grade and what the target grade should be. Here’s the dataset below: w8_grades &lt;- read_csv(here(&quot;data&quot;, &quot;week_8&quot;, &quot;W8_grades.csv&quot;)) ## Rows: 128 Columns: 1 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (1): sample ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(w8_grades) 6.2.3 Assumption checks There is only one relevant assumption that we need to check for the one-sample t-test: whether our data is distributed normally or not. We can do this in two ways. The first and quickest is through a test called the Shapiro-Wilks test (often abbreviated as the SW test). The SW test is a significant test of departures from normality. The statistic in question, W, is an index of normality. If W is close to 1, then data is normally distributed; the smaller W becomes, the more non-normal the data is. A significant p-value on the SW test suggests that the data is non-normal. Thankfully, that isn’t an issue here. shapiro.test(w8_grades$sample) ## ## Shapiro-Wilk normality test ## ## data: w8_grades$sample ## W = 0.98652, p-value = 0.2395 The second way is through a QQ plot (Quantile-Quantile) plot. Essentially, these plot where data should be (if the data are normally distributed) against where the data actually is. If the normality assumption is intact, most of the data should lie on or close to the straight line, like the left plot. Data that looks like the right, where the data curves away from the central line, is more likely to be non-normally distributed. knitr::include_graphics(&quot;img/w8_qqplots.svg&quot;) 6.2.4 Output Here are our descriptive statistics. They alone might already tell us something is going on: w8_grades %&gt;% summarise( mean = mean(sample, na.rm = TRUE), sd = sd(sample, na.rm = TRUE), median = median(sample, na.rm = TRUE) ) %&gt;% knitr::kable(digits = 2) mean sd median 67.09 5.38 67.5 To run a one-sample t-test, the basic function is the t.test() function. For a one-sample t-test, you must provide the argument x (the data) and mu, which is the hypothesised population mean. Below is our output from the one-sample t-test. Our result tells us that there is a significant difference between the mean of the sample (M = 67.09) and the hypothesised mean (p &lt; .001). The mean difference here is calculated as Sample - Hypothesis; therefore, a difference of -4.914 means that the sample mean is lower than the population mean (which hopefully would have been evident from the descriptives anyway). This means that for some reason, last year’s cohort are performing worse than the expected average. t.test(w8_grades$sample, mu = 72) ## ## One Sample t-test ## ## data: w8_grades$sample ## t = -10.342, df = 127, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 72 ## 95 percent confidence interval: ## 66.14570 68.02617 ## sample estimates: ## mean of x ## 67.08594 Alternatively, you can use the t_test() function in R. This function requires formula notation. In this case, the formula must take the form of variable ~ 1 to indicate that it is a one-sample test: w8_grades %&gt;% t_test(sample ~ 1, mu = 72) "],["independent-samples-t-test.html", "6.3 Independent samples t-test", " 6.3 Independent samples t-test The independent-samples t-test is one of the most common tests that you will see in literature - it is one of the bread-and-butter tests of many music psychologists (for better or worse). 6.3.1 Independent samples Independent samples t-tests are used when we want to compare two separate groups on one continuous outcome. They’re therefore well-suited for data with one categorical IV with two levels, against one continuous outcome. 6.3.2 Example data For this example we’ll use a contrived but really simple example. A group of self-reported professional and amateur musicians were asked how many years of training they had on their primary instrument. w8_training &lt;- read_csv(here(&quot;data&quot;, &quot;week_8&quot;, &quot;W8_training.csv&quot;)) ## Rows: 60 Columns: 3 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): Group ## dbl (2): Participant, Years_training ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(w8_training) 6.3.3 Assumption checks There are three main assumptions for a basic independent samples t-test: Data must be independent of each other - in other words, one person’s response should not be influenced by another. This should come as a feature of good experimental design. The equality of variance (homoscedasticity) assumption. The classical t-test assumes that each group has the same variance (homoscedasticity). We can test this using a significant test called Levene’s test. If the test is significant (p &lt; .05), the assumption is violated. In our data, this assumption seems to be intact (p = .737). w8_training %&gt;% levene_test(Years_training ~ Group, center = &quot;mean&quot;) ## Warning in leveneTest.default(y = y, group = group, ...): group coerced to factor. The residuals should be normally distributed. This essentially has implications for how well the data behaves. We can test this in two ways. The first is using a normality test, like the Shapiro-Wilks (SW) test. Like Levene’s test, if the result of this test is significant it suggests that the normality assumption is violated. In our data, this appears to be the case (W=.946, p = .011). shapiro.test(w8_training$Years_training) ## ## Shapiro-Wilk normality test ## ## data: w8_training$Years_training ## W = 0.94685, p-value = 0.0111 6.3.4 Output In reality, it’s rare that any of these assumptions are fully met even when tests say they are (the tests we just mentioned can be biased). This is especially true for classical t-tests, which are very sensitive to violations. A consistently better alternative is to use the Welch t-test, which assumes the equality of variance assumption is not met. Welch t-tests are also fairly robust against the normality assumption, and so are more flexible without sacrificing accuracy. Here’s our output from R Note that this is from a Welch t-test - R will do this byu default. w8_training %&gt;% t_test(Years_training ~ Group) From this, we can see that the two groups do significantly differ on years of training (t(57.99) = 5.859, p &lt; .001). We can use the mean difference value to see, well… the difference in means between the two groups. In this case, professionals have 2.92 more years of training (on average) compared to amateurs. (n.b. the signs for t and the mean difference don’t overly matter so long as they are interpreted in the right way. The output above calculates amateurs - professionals, which is why the values for both are negative; but if you were to force the test to run the other way round, the values would be the same with the signs flipped. Hence why descriptives and graphs are super important too! "],["paired-samples-t-test.html", "6.4 Paired-samples t-test", " 6.4 Paired-samples t-test Here’s our last test for the module, and it is again another bread-and-butter statistical test in literature: the paired-samples t-test. 6.4.1 Paired-samples Paired samples t-tests, as the name sort of implies, are used when we have a sample and we take measurements twice. Often, paired-samples t-tests are interested in testing the effect of time on an outcome; for example, a before-after design lends itself quite nicely to paired-samples and other repeated-measures tests. The core hypotheses are very much the same here, aside from the caveat that the means are between conditions and not groups. Mathematically, the paired-samples t-test is actually just a variant of the one-sample t-test. If we did a one-sample t-test on the differences between the two timepoints/conditions, we would get the same results. 6.4.2 Example data For this example, we’ll take a look at a simple interventions study. Participants were asked to answer a short list of questions relating to how they were feeling, once before an intervention and once afterwards. Higher scores represent better emotional states. The intervention was a series of self-regulation classes and exercises that the participants took twice a week. We’re interested in seeing whether the intervention was effective. w8_symptoms &lt;- read_csv(here(&quot;data&quot;, &quot;week_8&quot;, &quot;W8_symptoms.csv&quot;)) ## Rows: 32 Columns: 2 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (2): before, after ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(w8_symptoms) R-note: For t-tests the data need to be in long format, so the below code will reformat this: # Create a wide version of the original dataset w8_symptoms_wide &lt;- w8_symptoms # Pivot to long format w8_symptoms &lt;- w8_symptoms %&gt;% pivot_longer( cols = everything(), names_to = &quot;time&quot;, values_to = &quot;symptom_score&quot; ) # Display start of new data head(w8_symptoms) 6.4.3 Assumption checks Similar to other tests, we need to check normality. Here, the assumption is whether the differences between time 1 and 2 are normally distributed (not necessarily time 1 and 2 themselves). Hence, when we run a Shapiro-Wilks test we’re running this on the values we get from Time 1 - Time 2. In our data, this assumption seems to be intact (p = .918). shapiro.test(w8_symptoms_wide$before - w8_symptoms_wide$after) ## ## Shapiro-Wilk normality test ## ## data: w8_symptoms_wide$before - w8_symptoms_wide$after ## W = 0.98465, p-value = 0.9177 6.4.4 Output In R, you can do pairwise t-tests using the default t.test() (or the rstatix equivalent t_test()) function. In both methods, you must set paired = TRUE in order to run a paired t-test. Your dataset needs to be in long format for both functions. Here are both methods. By now this should be familiar: t.test(symptom_score ~ time, data = w8_symptoms, paired = TRUE) ## ## Paired t-test ## ## data: symptom_score by time ## t = 2.9501, df = 31, p-value = 0.005999 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## 0.848815 4.651185 ## sample estimates: ## mean difference ## 2.75 w8_symptoms %&gt;% t_test(symptom_score ~ time, paired = TRUE) The mean symptom scores of the two timepoints are significantly different (t(31) = 2.95, p = .006). Based on the means and mean difference, participants reported having significantly better emotional states after the intervention compared to beforehand. "],["cohens-d.html", "6.5 Cohen’s d", " 6.5 Cohen’s d This might be starting to sound a little familiar by now, but here are some effect sizes for t-tests. Note that they’re different to the Cramer’s V we saw in the chi-square test of independence last week - this is because it is a) conceptually different and b) interpreted differently too. 6.5.1 What is Cohen’s d? Cohen’s d is a measure of effect size that is used when comparing between two means (i.e. in a t-test). It essentially is a measure of the distance between the two means. See below for three pairs of means: If two groups aren’t all that different (e.g. panel A), then any effect of group will be small or negligible. If the two groups are further apart, however (like panel C), there is a more obvious effect of group - and so the size of the effect itself will be larger. Cohen’s d essentially is a measure of this ‘distance’. The basic formula for calculating Cohen’s d is: \\[ d = \\frac{M_1 - M_2}{\\sigma_{pooled}} \\] In other words, Cohen’s d is calculated by taking the difference between the two group means and dividing that by the pooled standard deviation across both groups. Pooled SD is essentially an aggregate SD across both groups in the sample, and not something we’ll concern ourselves with this week (because like the maths for the t-statistic, the calculation of the pooled SD depends on the test and is hard). 6.5.2 Interpreting Cohen’s d Cohen provided some now-famous guidelines for interpreting the size of Cohen’s d values: Effect size Interpretation d = .20 Small d = .50 Medium d = .80 Large 6.5.3 Calculating Cohen’s d in R The cohens_d() function from rstatix can handle the calculation of effect sizes for all three variants of t-tests. One thing that’s quite nice about this is that the format for cohens_d() is exactly the same as it would be for the t_test() and t.test() family of functions: For a one-sample t-test, the formula once again needs to be in var ~ 1 format and mu must be specified: w8_grades %&gt;% rstatix::cohens_d(sample ~ 1, mu = 72) For an independent-samples t-test: w8_training %&gt;% cohens_d(Years_training ~ Group) For a paired-samples t-test, paired = TRUE must be selected: w8_symptoms %&gt;% cohens_d(symptom_score ~ time, paired = TRUE) The Canvas version asks you to interpret each of these effect sizes, but… rstatix will automatically label this for you! "],["anovas.html", "Chapter 7 ANOVAs", " Chapter 7 ANOVAs t-tests, as we saw last week, were a simple way of comparing a continuous outcome between two groups or conditions (i.e. one categorical variable with two levels). However, it’s also common to compare across three or more groups when doing real research. To test this kind of hypothesis where we have three or more groups, we need to turn to a different method - the ANOVA. ANOVAs are useful when you have one categorical IV with 3+ levels, and one continuous DV. The ANOVA is perhaps one of the most common statistical tests you will see in music psychology literature, in part because they generalise to a lot of research designs. In many respects, this week is fairly important in terms of knowing how to conduct an ANOVA and when. We’ll only be stepping through the basics this week, but there are some really important fundamentals to cover here. It’s also common to analyse two independent variables against one dependent variable. These too can be done using ANOVAs. While we won’t go into this in this module, there will be an Extension Module that deals with this separately - check it out sometime around Week 10/11 if you’re interested. By the end of this module you should be able to: Understand and describe the conceptual basis of an analysis of variance Describe how an ANOVA is calculated Conduct both one-way ANOVAs and one-way repeated ANOVAs, including their assumption tests Interpret the output of the ANOVAs and report their findings "],["anovas-and-the-f-distribution.html", "7.1 Anovas and the F-distribution", " 7.1 Anovas and the F-distribution Just as we’ve done so for chi-squares and t-tests, let’s begin with an overview of the mathematical and conceptual underpinnings of the ANOVA. 7.1.1 The basic logic of ANOVAs As we said in the introductory page, ANOVA stands for ANalysis Of VAriance. This essentially sums up how an ANOVA works in principle. ANOVAs can be used when analysing a categorical IV with two or more groups (typically 3+) and a continuous DV. As the setup suggests, ANOVAs are a good way of comparing different groups on a singular outcome. The most basic hypotheses for an ANOVA center around whether or not the means between groups are significantly different, i.e.: \\(H_0\\): The means between groups are not significantly different. (i.e. \\(\\mu_1 = \\mu_2 = ... \\mu_k\\)) \\(H_1\\): The means between groups are significantly different. (i.e.\\(\\mu_1 \\neq \\mu_2 \\neq ... \\mu_k\\)) How do we test for this? The basic logic of the ANOVA is this: Whenever we have data that we categorise into different groups, we end up with two key sources of variability, or variance: variance that exists between groups, and variance that exists within groups: ggplot(data = tibble(x = c(37, 90)), aes(x)) + stat_function(fun = dnorm, n = 101, args = list(mean = 52, sd = 5), colour = &quot;steelblue&quot;, linewidth = 2) + stat_function(fun = dnorm, n = 101, args = list(mean = 61, sd = 5), colour = &quot;darkseagreen&quot;, linewidth = 2) + stat_function(fun = dnorm, n = 101, args = list(mean = 73, sd = 5), colour = &quot;indianred&quot;, linewidth = 2) + labs(x = &quot;Scores&quot;, y = &quot;&quot;) + scale_y_continuous(breaks = NULL) + theme( panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_rect(fill = &quot;transparent&quot;,colour = NA), plot.background = element_rect(fill = &quot;transparent&quot;,colour = NA) ) Pretend that the three curves above represent data covering three different groups, and that we’ve hypothesised that there are three different means. Collectively, this data has a certain amount of variance. The first fundamental to recognise is that this total variance can be broken down into variance between groups and variance within groups: \\[ Variance_{total} = Variance_{between} + Variance_{within} \\] The variability between the groups would simply be the variance between the blue curve and the orange curve - in other words, how far apart the two sets of data are (in a simplistic sense). The within-group variance, on the other hand, is how much variance there is within each curve. If there is a lot of within-group variance within each curve, that would mean that (thinking back to Module 5) each group’s curve would be spread widely. If you’re following the logic of this so far, the consequences of high within-group variance might be obvious - the two curves would significantly overlap. In contrast, if the between-group variance is far higher than the within-group variance, the curves may not overlap much at all - suggesting that the means of the two groups really are different. This forms the basis of the ANOVA’s F-test, which is a ratio of variance: \\[ F = \\frac{Variance_{between}}{Variance_{within}} \\] We will see this more in practice on the next page, but the F-statistic, which we use as part of significance testing in ANOVAs, is calculated by simply dividing the between-group variance by the within-group variance. 7.1.2 The F-distribution Like the other tests we’ve encountered so far, ANOVAs have their own underlying distribution. In this instance, this is the F distribution, which describes how the F-statistic behaves. We won’t go far into the maths around this, but the key here is that the F distribution is characterised by two sets of degrees of freedom: one that relates to the between-groups variance/effect, and one that relates to the within-groups variance. One thing to note here is the terminology in the headers of each graph: the first number in the brackets refers to the between-groups variance, while the second number refers to the within-groups variance. 7.1.3 F-statistic tables And, once again, we have a special F-table to determine critical F-stat values, given two degrees of freedom values. However, given that we have two degrees of freedom the process is a little bit more complicated. Most F-stat tables actually provide multiple tables, corresponding to different alpha levels. For now, we will always use the table corresponding to alpha = 0.05, a snippet of which is below: knitr::include_graphics(here(&quot;img/w9_f-table.png&quot;)) To read this table, you need to know both degrees of freedom (more on how to find that on the next page). The columns, marked df1, refer to the first df value (between-groups), while the rows correspond to the second df value (within-groups). So, for example, if we have an F-test with degrees of freedom of (4, 10), we would first need to find the column that corresponds to 4 (our df1), and then the row that corresponds to df2 = 10. Reading the cell at the intersection would give us a critical F-statistic of 3.48; this is the value our F-statistic would need to be greater than to be significant at the p &lt; .05 level. "],["the-anova-table.html", "7.2 The ANOVA table", " 7.2 The ANOVA table Every time we do an ANOVA, there are actually a raft of calculations that have to occur in order to get our test statistic and p-value. Statistical software will display all of this in the form of an ANOVA table, which we cover below. Don’t stress too much about the maths here - while the quiz does include one question about this, the question itself isn’t difficult (and you won’t be expected to do any of this by hand otherwise). The reason why we go through this, however, is to demonstrate the conceptual logic from the previous page. 7.2.1 The ANOVA table On the previous page, we talked in depth about how the F statistic is calculated, and what shapes the overall distribution. But how do we actually calculate all of that stuff to begin with from raw data? Enter the ANOVA table, a beautiful (remember, beauty is subjective) way of calculating this information and laying it all out to see. The basic ANOVA table looks like this, which you will see on any statistical package you use. Sums of squares (SS) Degrees of freedom (df) Mean square (MS) F p Group (our effect) Error/Residual Total A couple of terminology-related things here. ‘Group’ in this context is our independent variable (i.e. the effect of group)- this is our between-subject variance. ‘Error’ or ‘Residual’ is the within-group variance. Let’s use the below data to calculate this by hand. Group 1 Group 2 Group 3 1 2 5 2 4 8 3 5 6 2 3 7 Mean: 2 Mean: 3.5 Mean: 6 The grand mean (i.e. the mean across all 12 pieces of data) is 4. Keep this in mind. Time to strap in - there are a lot of formulae involved! 7.2.2 Sums of squares The sum of squares quantifies how far each observation is from the average - just like how it is calculated in the formula for standard deviations. For an ANOVA, we have two sets of SS to calculate - one for the between-groups term, and one for the within-groups/error. Between The formula for the between-groups sum of squares (\\(SS_b\\)) is: \\[ SS_b = \\Sigma n(\\bar x - \\bar X)^2 \\] In words, this means: Take each group’s mean (\\(\\bar x\\)), and subtract them from the grand mean (\\(\\bar X\\)) Square that difference Multiply it by n, the size of each group Add them all up. Let’s do that for our fictional data. Our group means are Group 1 = 2, Group 2 = 3.5 and Group 3 = 6.5. \\(SS_b = 4(2-4)^2 + 4(3.5-4)^2 + 4(6.5-4)^2\\) \\(SS_b = (4 \\times 4) + (4 \\times 0.25) + (4 \\times 6.25)\\) \\(SS_b = 42\\) Within For the within-group sum of squares, the formula is: \\[ SS_w = \\Sigma (x - \\bar x)^2 \\] This one is a bit more tedious. It means: Take each observation (\\(x\\)), and subtract them from their group mean (\\(\\bar x\\)) Square that difference Add them all up. So for our data, it would look something like.. \\[ SS_w = (1-2)^2 + (2-2)^2 + (3-2)^2 + (2-2)^2 + (2-3.5)^2 + (4-3.5)^2 + (5-3.5)^2 + (3-3.5)^2 + (5-6.5)^2 + (8-6.5)^2 + (6-6.5)^2 + (7-6.5)^2 \\] \\(SS_w = 12\\). 7.2.3 Degrees of freedom Because we have a term for between-groups and within-groups effects, we also have degrees of freedom for both (think back to the previous page). Thankfully, unlike the mess above the formulae here are relatively simple. Between The between-groups df is given as: \\[ df_b = k - 1 \\] Where k = the number of groups. So, in our data, \\(df_b = 3 -1 = 2\\) (as we have 3 groups). Within The within-groups df is given as: \\[ df_b = N - k \\] Where k = the number of groups, and N is the total sample size (in our case, 12). So \\(df_w = 12 -3 = 9\\) (as we have 3 groups and 12 data points in total). 7.2.4 Mean squares The mean squares is another value for variance that essentially standardises the sum of squares by the degrees of freedom. The formula for MS between and within is the same: \\[ MS = \\frac{SS}{df} \\] We’ve calculated SS and df for both our between and within-groups effects, so we can substitute these values in to calculate a mean square value for both: \\[MS_b = \\frac{SS_b}{df_b}\\] \\[MS_w = \\frac{SS_w}{df_w}\\] Putting in our values that we calculated earlier, we get: \\[MS_b = \\frac{42}{2}\\] \\[MS_w = \\frac{12}{9}\\] This gives us \\(MS_b = 21\\) and \\(MS_w = 1.33\\). 7.2.5 Calculating F Remember on the previous page, how we talked about the F-statistic being a ratio between two variances (between divided by within)? That’s exactly what we’re going to do next, using our calculatd mean-square values: \\[ F = \\frac{MS_b}{MS_w} \\] This gives us: \\[ F = \\frac{21}{1.33} = 15.75 \\] 7.2.6 Putting it all together Phew! Now we’ve calculated everything we need to for our ANOVA - in essence, we’ve done the majority of the ANOVA by hand. Let’s put all of our values into the table below: Sums of squares (SS) Degrees of freedom (df) Mean square (MS) F p Group (our effect) 42 2 21 15.75 Error/Residual 12 9 1.33 Total 54 Before we move on, it’s also worth noting that \\(SS_b + SS_w = SS_{total}\\) ; this goes back to the fundamental we discussed on the previous page, where an ANOVA breaks down total variance into between and within-groups variance. 7.2.7 Consulting the F-table Now that we have our observed F-value, we can now consult an F-table and use our two degrees of freedom to find out what our critical F-value is (setting alpha = .05): knitr::include_graphics(&quot;img/w9_f-table.png&quot;) Reading the column for df1 = 2, and the row for df2 = 9, we get a critical F-statistic of 4.2565. Now we can say that because our observed test statistic (15.75) is greater than this critical value, our ANOVA is significant at the alpha = .05 level. In reality, like many of the other tests, we would calculate a p-value by observing where our test statistic falls on the F-distribution, and the associated probability of getting that value or greater. Our software will do all of this stuff for us, but it helps to know exactly how an ANOVA works! "],["multiple-comparisons.html", "7.3 Multiple comparisons", " 7.3 Multiple comparisons On this page we talk about what happens after a significant ANOVA result, as well as some more detail about the multiple comparisons problem, and how it can be accounted for. 7.3.1 Post-hoc tests The ANOVA table on the previous page allows us to calculate by hand whether there is a significant effect of our IV. However, it doesn’t tell us where that difference lies. Remember, we’re comparing at least three groups with each other - so we need some way of figuring out where the actual significant differences between means are. Enter post-hoc comparisons (post-hoc = after the fact). These are essentially a series of t-tests where we compare each group with each other. So, if we have groups A, B and C, and we get a significant omnibus ANOVA that tells us there is a significant difference somewhere in those means, we would run post-hoc comparisons by running individual t-tests on A vs B, then B vs C, then A vs C. However, we can’t just do this blindly, and there’s a good reason why… 7.3.2 The multiple comparisons problem In Module 6, we talked a fair bit about Type I and II error rates - i.e., the rate of making a false positive and false negative judgement respectively. Here, we’ll focus on Type I error. Recall that whenever we do a hypothesis test, we set a value for alpha, which forms our significance level. Alpha is essentially the probability of Type I errors we are willing to accept in a given test. In other words, when we use the conventional alpha = .05 as our criterion for significance, we are saying that we’re willing to accept a Type I error occurring 5% of the time - or 1 in 20. This becomes problematic when we have to conduct multiple tests at once, like what we have to do in an ANOVA. For example, let’s say a variable has 5 levels - A, B, C, D and E. If we ran an ANOVA on this data and found a significant omnibus effect, we would want to find out where that effect is. But that means that we’d have to compare A, B, C, D and E all against each other, in a round-robin tournament way (e.g. A vs B, then A vs C…). Where this becomes an issue is that each comparison will have its own 5% Type I error rate (assuming we use alpha = .05). So in this scenario, the Type I error rate will stack with each new comparison, meaning that our overall Type I error rate will climb higher and higher. This means that the chance of finding a false positive will increase (see graph below). This overall rate of Type I errors is called the family-wise error rate (FWER). ‘Family’ in this context refers to a family of comparisons - like the comparison across A to E that we saw above. 7.3.3 Correction methods One way of dealing with this is to correct the p-values to set the family-wise error rate back to 5%. We’ll go through three main types (two of which are already covered in the seminar). Tukey’s HSD Tukey’s Honest Significant Differences (Tukey HSD for short) is appropriate specifically for post-hoc tests after ANOVAs. It essentially works by first calculating the largest possible difference between each group’s means, then using that to calculate a critical mean difference. Any mean difference that is greater than this critical threshold is significant. Bonferroni The Bonferroni correction is a method that can be used both within ANOVAs and in more general contexts. The Bonferroni correction works by multiplying each p-value in a set of comparisons by the number of comparisons (i.e. the number of p-values). For example, if a set of comparisons gives the following p-values: 0.05, 0.25 and 0.10, we would apply a Bonferroni correction by multiplying each p-value by 3 (as there are 3 p-values) to get 0.15, 0.75 and 0.30 respectively. (Note: in reality, Bonferroni works by dividing alpha by the number of comparisons, i.e. 0.05/3 - but for the purpose of significance testing, the maths works out to be the same). Holm The Holm correction is another correction method. In this method, each p-value is first ranked from smallest to largest. and then numbered in a descending fashion. Using the same p-values as above, we would get 0.05, 0.10 and 0.25. The p = .05 would be ranked as 3, the p = .10 would be 2 and p = .25 would be 1. You then multiply the p-value by its rank, like the table below, to get your adjusted p-values: tibble( p = c(0.05, 0.25, 0.10), p_sort = c(0.05, 0.10, 0.25), rank = c(3, 2, 1), p_adj = c(0.15, 0.20, 0.25) ) %&gt;% knitr::kable( col.names = c(&quot;Original p-value&quot;, &quot;Sorted p-value&quot;, &quot;Rank&quot;, &quot;p-value x rank&quot;), align = &quot;l&quot; ) Original p-value Sorted p-value Rank p-value x rank 0.05 0.05 3 0.15 0.25 0.10 2 0.20 0.10 0.25 1 0.25 The Bonferroni procedure is very conservative, in that it may actually overcorrect (and subsequently reject too many); the Holm correction still corrects the overall FWER without also increasing the overall Type II rate as well. "],["one-way-anova.html", "7.4 One-way ANOVA", " 7.4 One-way ANOVA In the previous section, we looked at how to compare differences between either two groups, or two points in time. But how do we compare more than that? The answer is the ANOVA (analysis of variance) - one of the most common general statistical models for hypothesis testing. If you do some statistical testing as part of your thesis, the ANOVA is likely going to be one of your most useful tools to have. 7.4.1 The basic one-way ANOVA The between-groups one-way ANOVA is the most basic form of ANOVA, which aims to test differences between two or more groups. (Typically, ANOVAs are used when there are three or more groups, but can easily be used in situations where there are only two groups.) We’ve briefly mentioned the general hypotheses for all ANOVAs, but here they are again: \\(H_0\\): The means between groups are not significantly different. (i.e. \\(\\mu_1 = \\mu_2 = ... \\mu_k\\)) \\(H_1\\): The means between groups are significantly different. (i.e.\\(\\mu_1 \\neq \\mu_2 \\neq ... \\mu_k\\)) 7.4.2 Example data In the seminar, we talk about an example from Watts et al. (2003). Below is another simple example, comparing taste ratings across three different types of slices: caramel slices, vanilla slices and lemon slices. Participants were randomly allocated to taste one of the three slices blindfolded, and were then asked to verbally rate its taste on a scale from 1-10 (10 being super tasty). w9_slices &lt;- read_csv(here(&quot;data&quot;, &quot;week_9&quot;, &quot;w9_slices.csv&quot;)) ## Rows: 63 Columns: 2 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): group ## dbl (1): rating ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(w9_slices) 7.4.3 Assumption checks There are four main assumptions for a basic ANOVA. The key difference with R however is that you need to run the ANOVA first to access the residuals. The basic function for this in R is the aov() function, which takes a formula input (much like t.test()). The key difference is that anything created from aov needs to be assigned to a new variable so that we can use it later. This creates an aov object: w9_slices_aov &lt;- aov(rating ~ group, data = w9_slices) Data must be independent of each other - in other words, one person’s response should not be influenced by another. This should come as a feature of good experimental design. The residuals should be normally distributed. We can assess this using the same methods as t-tests: either a QQ plot or a normality test (e.g. Shapiro-Wilks). You can access the residuals from the aov object directly like this: w9_slices_aov$residuals This lets us do the SW test: shapiro.test(w9_slices_aov$residuals) ## ## Shapiro-Wilk normality test ## ## data: w9_slices_aov$residuals ## W = 0.95117, p-value = 0.01409 A SW test on this data suggests the assumption is violated (W=.951, p = .014), suggesting that the residuals are not normally distributed. This is a bit of a problem in our dataset because our sample is relatively small. However - the ANOVA is fairly robust to violations of the normality assumption, meaning that non-normal residuals aren’t a major problem so long as a) you have a big enough sample size and b) the skew isn’t huge (or driven by outliers). The equality of variance (homoscedasticity) assumption. This means that the variances within each group are the same. We test this using Levene’s test, using the levene_test() function. Helpfully, levene_test() is flexible enough to work with either a formula or an aov object we have already fitted: # Either one of these will work w9_slices %&gt;% levene_test(rating ~ group, center = &quot;mean&quot;) levene_test(w9_slices_aov, center = &quot;mean&quot;) The assumption doesn’t appear to be violated (p = .491). If it was, we might consider using a Welch’s ANOVA, which can be done with welch_anova_test() from rstatix. For now though, we’ll press ahead. 7.4.4 Output Here’s our main output from the ANOVA, generated using the summary() function. This is called an omnibus, because it is a general test of the hypotheses: summary(w9_slices_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group 2 22.22 11.111 6.897 0.00201 ** ## Residuals 60 96.67 1.611 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Our main results suggest that there is a significant difference between means (F(2, 60) = 6.90, p = .002). 7.4.5 Post-hoc tests The output above told us that we could reject our basic null hypothesis - that there was no difference between means. However, remember that it doesn’t tell us where those differences are. To figure out where they are, we need to follow up that ANOVA with post-hocs tests. To generate post-hocs, there are a couple of ways. Tukey tests can be calculated with TukeyHSD() or tukey_hsd(): TukeyHSD(w9_slices_aov) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = rating ~ group, data = w9_slices) ## ## $group ## diff lwr upr p adj ## lemon-caramel -0.4761905 -1.4175618 0.4651809 0.4486738 ## vanilla-caramel 0.9523810 0.0110096 1.8937523 0.0467882 ## vanilla-lemon 1.4285714 0.4872001 2.3699428 0.0015928 # These will give the same output tukey_hsd(w9_slices_aov) w9_slices %&gt;% tukey_hsd(rating ~ group) Here, you can see that we run a test for caramel vs lemon, caramel vs vanilla and lemon vs vanilla. For post-hocs in one-way ANOVAs, it is ok to stick to the Tukey-adjusted p-values. We can see that: There is no significant difference between ratings of caramel and lemon slices (mean difference, MD = .48; p = .449). There is a (marginal) significant difference between caramel and vanilla slices; participants rated vanilla slices higher than caramel slices (MD = .95, p = .047). There is a significant difference between lemon and vanilla slices, in that participants preferred vanilla slices (MD = 1.43, p = .002). To generate Bonferroni or Holm-corrected p-values, you need to use the pairwise.t.test() function: pairwise.t.test(x = w9_slices$rating, g = w9_slices$group, p.adjust.method = &quot;bonferroni&quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: w9_slices$rating and w9_slices$group ## ## caramel lemon ## lemon 0.6866 - ## vanilla 0.0541 0.0017 ## ## P value adjustment method: bonferroni rstatix::pairwise_t_test() will also work, and looks like this: w9_slices %&gt;% pairwise_t_test(rating ~ group, p.adjust.method = &quot;bonferroni&quot;) In both instances, you must provide p.adjust.method as an argument. By default, both versions will use Holm corrections. "],["one-way-repeated-measures-anova.html", "7.5 One-way repeated measures ANOVA", " 7.5 One-way repeated measures ANOVA The second form of ANOVA that we will cover in this module is the repeated measures ANOVA (sometimes abbreviated as RM-ANOVA). While it shares many features with the basic one-way ANOVA, the nature of repeated measures data introduces some key differences in the interpretation and calculation of this test. 7.5.1 Repeated measures ANOVAs In principle, a repeated-measures ANOVA is very similar to the paired-samples t-test: both are repeated measures versions of their respective between-groups versions. So naturally, repeated-measures ANOVAs are used when we test one sample two or more times (again, like a regular ANOVA, it’s typically for 3+ times but can be used for two). It shares many similarities with the basic one-way ANOVA, albeit with one additional step in calculation: because we now test the same sample repeatedly, we need to account for variance within subjects. We won’t go too into detail about how that’s calculated here, but essentially we split within-groups variance into subject variance and error/residual variance. This essentially adds an extra step to the ANOVA table. 7.5.2 Example data In this example, we’ll use a dataset derived from McPherson (2005). This is a subset of data where children were scored on their ability to play songs from memory over three years - 1997 - 1999. We’re interested in seeing whether this change over time is significant - therefore, time (3 levels) is our independent variable, while playing from memory is our dependent variable. w9_memory &lt;- read_csv(here(&quot;data&quot;, &quot;Week_9&quot;, &quot;w9_playing_from_memory.csv&quot;)) ## Rows: 95 Columns: 4 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (4): Participant, PFM_97, PFM_98, PFM_99 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(w9_memory) For further analyses, we’ll shape this into long format: w9_memory_wide &lt;- w9_memory w9_memory &lt;- w9_memory %&gt;% pivot_longer( cols = PFM_97:PFM_99, names_to = &quot;time&quot;, values_to = &quot;memory_score&quot; ) 7.5.3 Assumption checks There are two main assumptions for a repeated-measures ANOVA. Note that while the independence assumption as we know it doesn’t apply here (by definition, repeated data is dependent), good experimental design should still aim to ensure that participants are independent of each other. The residuals should be normally distributed. Our usual tests apply here too. Below is a QQ plot, which might suggest that our residuals aren’t normally distributed. (Use the data and perform a SW test on it to see what happens!) aov(memory_score ~ time, data = w9_memory) %&gt;% broom::augment() %&gt;% ggplot(aes(sample = .std.resid)) + geom_qq() + geom_qq_line() ## Warning: The `augment()` method for objects of class `aov` is not maintained by the broom team, and is only supported through the `lm` tidier method. Please be cautious in interpreting and reporting broom output. ## ## This warning is displayed once per session. The sphericity assumption. Sphericity is assumed if, the variances of the differences between each level of the IV are equal. Think of it as a form of the equality of variances assumption, where we assumed that the variances within groups were equal. The sphericity assumption applies to the differences between T1 and T2, then T2 and T3… etc etc. Note that sphericity only applies when you have at least 3 levels of your IV (i.e. 3 timepoints). We can formally test it using Mauchly’s test of sphericity. If sphericity is violated, it means that our degrees of freedom are too high for the data, which inflates the Type I error rate. What next? We need to apply a correction to the omnibus ANOVA, which will alter the p-value. There are two on offer: Greenhouse-Geisser and Huynh-Feldt corrections. To help decide which one to use, R also calculates a value called epsilon (\\(\\epsilon\\)). Epsilon, in short, is a measure of sphericity; if sphericity is assumed, \\(\\epsilon\\) = 1. If \\(\\epsilon\\) is below 1, sphericity is violated; the smaller it is, the greater the violation and therefore the greater the correction needs to be. Therefore, these corrections alter the degrees of freedom for each test to account for this higher error rate. (Mathematical note; the corrected dfs are calculated by multiplying the original dfs by \\(\\epsilon\\). So e.g. if your original df is 10 and \\(\\epsilon\\) = 0.9, your new corrected df will be 9.) Broadly: If epsilon (\\(\\epsilon\\)) &gt; .75, use the Huynh-Feldt correction. If epsilon (\\(\\epsilon\\)) &lt; .75, use the Greenhouse-Geisser correction. Here, our epsilon value is high (~.95), so let’s apply the Hyunh-Feldt correction. 7.5.4 ANOVA output R-Note: Repeated measures ANOVAs in R are not trivial in the slightest, and adapting this was a genuine challenge. To some extent this probably reflects the differences in approach between point-and-click software compared to actually having to code the anova model: the former is easy but you perhaps make many assumptions about what’s going on along the way. Below is the easiest way to run this analysis. Here’s our overall output from the ANOVA. It looks (and reads) pretty much the same as our previous example, but just bear in mind that the top row in the within-subjects effect (i.e. the effect of interest). In addition, this time you can see that there are now three rows to be read: w9_memory_aov &lt;- w9_memory %&gt;% anova_test(dv = memory_score, wid = Participant, within = time) w9_memory_aov ## ANOVA Table (type III tests) ## ## $ANOVA ## Effect DFn DFd F p p&lt;.05 ges ## 1 time 2 188 132.625 1.19e-36 * 0.231 ## ## $`Mauchly&#39;s Test for Sphericity` ## Effect W p p&lt;.05 ## 1 time 0.939 0.054 ## ## $`Sphericity Corrections` ## Effect GGe DF[GG] p[GG] p[GG]&lt;.05 HFe DF[HF] p[HF] p[HF]&lt;.05 ## 1 time 0.943 1.89, 177.21 1.05e-34 * 0.961 1.92, 180.72 2.45e-35 * Knowing which row to read is crucial here, as each one corresponds to a correction (or the original ANOVA). Here, our p-value is almost significant (p = .054); rather than going by a strict arbitrary cutoff, let’s assume that it has been violated (both for teaching and for methodological purposes). The associated epislon value is \\(\\epsilon = .943\\); when reporting the repeated measures ANOVA we need to look at the rows corresponding to the Huynh-Feldt correction. The two columns to look for are the ones labelled DF[HF] - this gives the adjusted degrees of freedom with the Huynh-Feldt corrections (DF[GG] would give Greenhouse-Geisser dfs) - and p[HF], which gives the adjusted p-value. Thus, with the corrected output we can see our effect of time is significant (F(1.92, 180.72) = 132.63, p &lt; .001). 7.5.5 Post-hoc tests As per usual, we follow up a significant overall ANOVA with a series of post-hoc comparisons (note this time we set paired = TRUE: w9_memory %&gt;% pairwise_t_test(memory_score ~ time, paired = TRUE, p.adjust.method = &quot;bonferroni&quot;) From this, we can see that all three means are significantly different from each other (p &lt; .001, for brevity’s sake). Scores increased year on year, i.e. 1997 &lt; 1998 &lt; 1999. 7.5.6 Alternative using car::Anova() Below is an alternative way of running a repeated measures ANOVA, this time using the car package. The function factorial_design() from the rstatix package is a useful helper function that can generate the necessary arguments to Anova() for repeated measures tests. w9_memory_design &lt;- factorial_design( data = w9_memory, dv = memory_score, within = time, wid = Participant ) The important things that are generated by this function are: name$model: this essentially creates an ANOVA model name$idata: this specifies the levels of the within-subject (repeated measures) factor name$idesign: this creates a formula-style notation specifying what the within-subjects IV(s) are We then just need to feed this to car::Anova() as follows. Setting type = 3 isn’t strictly necessary here (this is more relevant for factorial ANOVAs): w9_memory_alternate &lt;- car::Anova( mod = w9_memory_design$model, idata = w9_memory_design$idata, idesign = w9_memory_design$idesign, type = 3 ) Now that we’ve built our ANOVA model we can ask for the output like so. multivariate = FALSE is just an argument to specify that we’re doing a univariate analysis (i.e. one dependent outcome). summary(w9_memory_alternate, multivariate = FALSE) ## ## Univariate Type III Repeated-Measures ANOVA Assuming Sphericity ## ## Sum Sq num Df Error SS den Df F value Pr(&gt;F) ## (Intercept) 3490869 1 259027 94 1266.83 &lt; 2.2e-16 *** ## time 98948 2 70130 188 132.63 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Mauchly Tests for Sphericity ## ## Test statistic p-value ## time 0.93912 0.053894 ## ## ## Greenhouse-Geisser and Huynh-Feldt Corrections ## for Departure from Sphericity ## ## GG eps Pr(&gt;F[GG]) ## time 0.94261 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## HF eps Pr(&gt;F[HF]) ## time 0.9612827 2.447568e-35 "],["eta-squared.html", "7.6 Eta-squared", " 7.6 Eta-squared TIme to go over a new effect size measure! This time, we use eta-squared (\\(\\eta^2\\)) as an effect size for ANOVAs. 7.6.1 Eta-squared and variance At the start of this module, we introduced the concept of how ANOVA partitions total variance into both between and within-subject variance: \\[ Variance_{total} = Variance_{between} + Variance_{within} \\] This partitioning allows for a simple but important way of calculating effect sizes for ANOVAs. If we’re interested in the effect of our IV (group, which is between-subject variance), we simply need to know how much of the total variance is explained by this variable. This is essentially eta-squared (\\(\\eta^2\\)), our effect size for ANOVAs. Eta-squared gives us a percentage of how much variance can be attributed to the main effect. So an eta-squared of .778 means that 77.8% of the total variance is because of the effect. It is calculated as follows: \\[ \\eta^2 = \\frac{SS_{effect}}{SS_t} \\] In other words, we divide the sum of squares for the main effect by the total sum of squares. For repeated-measures ANOVAs, the formula is the same in practice (i.e. divide the SS in the top row in the ANOVA table by the total). A related version of this is partial eta-squared (\\(\\eta^2_p\\)). This describes the effect of the IV after accounting for the variance explained by other factors. This is not so relevant for one-way designs - regular and partial eta-squared will give the same answer - but becomes more relevant when you move into factorial designs. The formula for partial eta-squared is: \\[ \\eta^2_p = \\frac{SS_{effect}}{SS_{effect} + SS_{error}} \\] Where \\(SS_{error}\\) is the sums of squares for the error/residual term in an ANOVA. In general, it’s good practice to default to at least reporting partial eta-squared. Again, in a one-way ANOVA this will give the same answer as regular eta-squared, but in a factorial design (where you have more than one IV) it will give a more precise estimate of each variable’s effect size. One more variant you will see is generalised eta-squared, which is similar to the partial variant. While partial eta-squared is great, it is sensitive to design - in other words, what variables are included in the analysis will influence the calculation of \\(\\eta^2_p\\), meaning that it is only really comparable across studies of similar design. However, not every design will manipulate every predictor (e.g. gender), and so generalised eta-squared (\\(\\eta^2_G\\)) can handle this. This effect size is best used in meta-analyses. By default, anova_test() (and other ANOVA-related packages in R) will calculate generalised eta squared (labelled ges in the output). If you want partial eta-squared, you just need to give anova_test() the extra argument effect.size = \"pes as follows: w9_memory %&gt;% anova_test(dv = memory_score, wid = Participant, within = time, effect.size = &quot;pes&quot;) ## ANOVA Table (type III tests) ## ## $ANOVA ## Effect DFn DFd F p p&lt;.05 pes ## 1 time 2 188 132.625 1.19e-36 * 0.585 ## ## $`Mauchly&#39;s Test for Sphericity` ## Effect W p p&lt;.05 ## 1 time 0.939 0.054 ## ## $`Sphericity Corrections` ## Effect GGe DF[GG] p[GG] p[GG]&lt;.05 HFe DF[HF] p[HF] p[HF]&lt;.05 ## 1 time 0.943 1.89, 177.21 1.05e-34 * 0.961 1.92, 180.72 2.45e-35 * 7.6.2 An example Here’s the ANOVA table we worked out by hand in Section 9.3: Sums of squares (SS) Degrees of freedom (df) Mean square (MS) F p Group (our effect) 42 2 21 15.75 Error/Residual 12 9 1.33 Total 54 If we were to calculate an eta-squared value for this, we would use SS effect (42) and SSt (54) like so: \\[ \\eta^2 = \\frac{42}{54} = .778 \\] 7.6.3 Interpreting eta-squared Cohen (1988) provided the following guidelines for interpreting eta-squared: \\(\\eta^2\\) = .01 is a small effect \\(\\eta^2\\) = .06 is a medium effect \\(\\eta^2\\) = .14 is a large effect With these guidelines (which aren’t perfect), a \\(\\eta^2\\) of .778 is astronomically huge. To give yourself a bit of practice, go back into Sections 9.5 and 9.6, find the eta-squared values for each in the outputs (they’re in the omnibus tables) and interpret them a) as percentages of total variance and b) using Cohen’s (1988) guidelines. "],["linear-relationships.html", "Chapter 8 Linear relationships", " Chapter 8 Linear relationships Up until now we’ve been dealing almost exclusively with categorical variables in some way. For example, chi-square tests are for testing relationships between categorical variables; likewise, t-tests and ANOVAs deal with categorical IVs against continuous DVs. In reality though, many of the things we’re interested in are inherently continuous in nature. There are very few psychological constructs that aren’t continuous in some way, and so working with continuous variables forms a core part of doing statistical analyses. Enter the linear regression and its many other forms - in some ways, the bedrock of many of the research that we do (more on this in Week 11). When we’re dealing with continuous variables, linear regressions are generally the first place to start. We won’t go much further beyond the basics here (certainly as it applies to a lot of psychological research), but even these concepts are foundational for many reasons. By the end of this module you should be able to: Describe how hypothesis testing works in regressions - including what is being tested Conduct appropriate assumption tests for a linear regression Run a linear regression and interpret the output Make predictions based on the output of a linear regression Run and interpret a multiple regression "],["correlations.html", "8.1 Correlations", " 8.1 Correlations We’ve talked a surprising amount about correlations in this subject, but we haven’t considered how to actually test if two things are correlated to begin with. We change that this week with an overview of correlation coefficients. 8.1.1 Correlation coefficients We can quantify the strength of two variables using a correlation coefficient, which gives us a measure of how tightly these two variables are related. To start, we need to know what covariance is. Covariance simply describes how two variables change with each other. For instance, if two variables have a positive covariance, this means that as one variable increases, so does the other. Similarly, if two variables have a negative covariance, this means that as one increases the other decreases. See if you can describe what the covariances would be like below: Correlation coefficients are just another way of describing this. Correlation coefficients have the following properties: They are between -1 and +1 The sign of the correlation describes the direction - a positive value represents a positive correlation The numerical value describes the magnitude A correlation of 1 means a perfect correlation; a correlation of 0 means a negative correlation A rough guideline for this subject, r = .20 is weak, r = .50 is moderate, r = .70 is strong Visually, a magnitude of 0 corresponds to a flat line; the steeper the line, the higher the magnitude There are many types of correlation coefficients, but the most common is the Pearson’s correlation coefficient. It’s calculated using the below (simplified) formula: \\[ r = \\frac{Cov_{xy}}{SD_x \\times SD_y} \\] In this subject we won’t expect you to calculate a correlation coefficient by hand, but the key takeaway here is that by dividing a value by a standard deviation (or, in this case, a product of two SDs), we are standardising the covariance. Hence, a correlation coefficient is a standardised measure, meaning that we can compare correlation coefficients quite easily across variables regardless of their scale. 8.1.2 Activity 8.1.3 Testing correlations in R Statistical programs like Jamovi and R will allow us to not only quantify a correlation between two variables, but test whether this correlation is significant. Generally, when working with continuous data it never hurts to run a basic correlation. This can easily be done in R by using the cor.test() function. You simply need to give it two If you need to run multiple correlations at once, there are two ways to visualise them. Below are two types, using fictional questionnaire data. The first is simply a table, like below: Q1 Q2 Q3 Q4 Q1 1.00 0.24 -0.56 0.72 Q2 0.24 1.00 -0.38 0.43 Q3 -0.56 -0.38 1.00 -0.11 Q4 0.72 0.43 -0.11 1.00 The second is a correlation heatmap, which is especially effective with many correlations at once (common when working with huge questionnaires or neuroimaging). As shown by the legend on the right, the colour and shade of each square are determined by the strength of the correlation. This can be easily done by the `ggcorrplot package, if you have a correlation matrix formatted in R: library(ggcorrplot) ## Warning: package &#39;ggcorrplot&#39; was built under R version 4.3.2 ## ## Attaching package: &#39;ggcorrplot&#39; ## The following object is masked from &#39;package:rstatix&#39;: ## ## cor_pmat ggcorrplot(cor_mat, type = &quot;lower&quot;, show.diag = TRUE) "],["regression-hypotheses.html", "8.2 Regression hypotheses", " 8.2 Regression hypotheses Let’s move on from correlations to regressions, where we test whether one variable can predict another. To do that, let’s start by considering what it is we actually hypothesise in doing a linear regression. 8.2.1 Terminology Let’s kick off the regression portion of this module with a bit of terminology: - The line is called the line of best fit. The slope of the line is, well, the slope. It describes how much Y changes if X changes by one unit. - The point at which the line crosses the y-axis is called the intercept (the y-intercept in full). The intercept is one of the two paramaters of a regression line (the other being the slope). Linear regressions involve plotting a line of best fit to the data, and using this line to make predictions. If you think back to high school, you may have learned something like y = mx + c or y = ax + b in algebra to describe a straight line. In linear regression, we use the same concepts to both describe our line and make predictions using it (more on that later). The key difference is that we change the letters a bit: \\[ y = \\beta_0 + \\beta_1x + \\epsilon_i \\] Let’s break this down: y is simply our predicted value (i.e. the line of best fit) \\(\\beta_0\\) is our intercept (the c in y = mx + c) \\(x\\) simply refers to our independent variable \\(\\beta_1\\) is the slope for the independent variable - in other words, how much y increases for every unit increase of x. We also call this B \\(\\epsilon_i\\) is error, which is essentially random variation (due to sampling). This error is normally distributed. Keep this in mind for now - we’ll come back to this later in the module! 8.2.2 Hypothesis testing in regressions When we conduct a linear regression, in part we’re testing if the two variables are correlated. However, we’re also testing whether we can predict our DV from our IV, so our statistical hypotheses are formulated around this idea. Our null and alternative hypotheses are therefore about the slope (\\(\\beta_1\\)): \\(H_0: \\beta_1 = 0\\), i.e. the slope is 0 \\(H_1: \\beta_1 \\neq 0\\), i.e. the slope is not equal to 0 Consider the two graphs below. On the left is a graph where the line of best fit has a slope of 0 (the null). No matter what value X is, the value of Y is always the same (2.5 in this example) - in other words, X does not predict Y. The graph on the right side, on the other hand, is an example of the alternative hypothesis in this scenario. Here, X does clearly predict Y - as X increases, Y increases as well. But how do we actually test this? The answer is something we’ve seen before - we do a t-test! We came across t-tests in context of comparing two means against each other. The logic here is exactly the same, except now we compare two slopes with each other - the slope we actually observe (B) minus the null hypothesis slope (0). We can use this logic to calculate a t-statistic using the below formula: \\[ t = \\frac{B}{SE_B} \\] Where B = observed slope and SE = standard error of B. This is the same formula as the calculation for a t-test, albeit that the top row is just B (because it is B - 0). We can do this for each predictor, and then use the same t-distribution to test whether this slope is significant - in other words, whether our predictor (IV) significantly predicts our outcome (DV). "],["least-squares-regression.html", "8.3 Least squares regression", " 8.3 Least squares regression Linear regression starts with trying to fit a line to our continuous data. But… how on earth do we figure out where that line sits? 8.3.1 The regression line See if you can take a guess where we should draw a line of best fit on the plot below: You might have some good guesses, and there’s every chance that you’ve drawn a line that fits the data points pretty well. But for this data, it’s pretty obvious where the line would sit, and the data that we do work with won’t always be as clear cut. The line of best fit sits where the distance between the line and every point is minimised. See the example below, where we have 10 data points to illustrate. Each dot represents a single data point, while the solid blue line is our line of best fit. The dashed lines represent the difference between the blue line (which is what we predict - more on this later) and the actual data point. This difference is called a residual. So, in other words, the line of best fit sits where the residuals are minimised. ## `geom_smooth()` using formula = &#39;y ~ x&#39; We do this using the least squares principle. In essence, we calculate a squared deviation for each residual using the formula below - it might be familiar… \\[ SS_{res} = \\Sigma (y_i - \\bar y_i)^2 \\] Where \\(y_i\\) is each individual (ith) value on the y-axis, and \\(\\bar y_i\\) is the predicted y-value (i.e. the regression line). Essentially, we calculate the residual for each data point, square it and add it all up. The actual minimising part is something we won’t concern ourselves with for this subject (or at all), because quite frankly it’s complicated - the key here is to understand how a regression line is fit to the data. "],["assumption-tests.html", "8.4 Assumption tests", " 8.4 Assumption tests 8.4.1 Linearity Believe it or not, for a linear regression your data should be… linear. Wild, right? Bitter sarcasm aside, linearity is an important assumption that needs to be met before conducting a linear regression. Not all data will follow a linear pattern; some data may instead sit on a curve. Compare the two examples below, where one is clearly non-linear: If the data shows no clear linear relationship between your IV and DV, it is likely because the two are weakly correlated (and so a linear regression would not be useful anyway). If instead the data lies on a very obvious curve, you can either: Transform the data to make it linear (but you must be clear about what this represents) Attempt to fit a curve and see whether this has more explanatory power, but madness lies this way for the unprepared 8.4.2 Homoscedasticity The homoscedasticity assumption is one that we’ve seen before - it is essentially a version of the equality of variance test. In a linear regression context, however, this assumption works a little bit differently; if this assumption is met, then the variance should be equal at all levels of our predictor (i.e. the x-axis). The easiest way to test this is to create a fitted vs residuals graph. As the name implies, we take the fitted values for each level of the predictor in our data, and plot that against the residuals (fitted - actual). Here are some fitted vs residual plots for two sets of data. The data on the left has an even spread of variance as X increases, meaning that it is homoscedastic; the data on the right, on the other hand, spreads out like a cone. The data on the right therefore is likely heteroscedastic. ## Warning: Removed 7 rows containing missing values or values outside the scale range (`geom_point()`). There are a couple of ways to overcome this, such as either transforming the raw variables or weighting them. 8.4.3 Independence This one is the same - data points should be independent of each other. Like we have seen with other tests, this should ideally be a feature of good experimental design. In linear regression, a specific issue is autocorrelation - where the residuals between two values of X are not independent. This can distort the relationship between your IV and DV, and is most commonly an issue with time series data. 8.4.4 Normality The normality assumption is the same here as it has been elsewhere - the residuals must be normally distributed. Here, it’s a little bit easier to visualise what these ‘residuals’ are because we can calculate and see them. That being said, the way we test for these are exactly the same - either use a Q-Q plot or a Shapiro-Wilks test. 8.4.5 Multicollinearity The multicollinearity (sometimes just called collinearity) assumption only applies to multiple regressions, where you have more than one predictor in your test. multicollinearity occurs when two predictors are too similar to one another (i.e. they are highly correlated with each other. This becomes a problem at the individual predictor level, because what happens is that the effect of predictor A becomes muddled by predictor B - in other words, if two predictors are collinear, it becomes impossible to tell apart which one is contributing what to the regression. There are three basic ways you can assess this. The simplest is to test a correlation between the two predictors. If they are very highly correlated (use r = .80 as a rule of thumb), this is likely to be a problem. The variance inflation factor (VIF) is a more formal measure of multicollinearity. It is a single number, and a higher value means greater collinearity. If a VIF is greater than 5 this suggests an issue. Tolerance is a related value to VIF - in fact, it is just 1 divided by the FID - and works in much the same way - except smaller values mean greater collinearity. As a rule of thumb, if tolerance is smaller than 0.20 this suggests an issue. To calculate VIF, you can use the VIF() function from the DescTools package - more on the relevant page. "],["linear-regressions-in-r.html", "8.5 Linear regressions in R", " 8.5 Linear regressions in R 8.5.1 Scenario Musical sophistication describes the various ways in which people engage with music. In general, the more ways in which people engage with music, the more musically sophisticated they are. One hypothesis is that years of musical training will clearly influence musical sophistication. While this is a bit of a no-brainer hypothesis, we’ll test it using some example data. w10_goldmsi &lt;- read_csv(here(&quot;data&quot;, &quot;week_10&quot;, &quot;w10_goldmsi.csv&quot;)) ## Rows: 74 Columns: 2 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (2): years_training, GoldMSI ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # This extra code is here for later w10_goldmsi &lt;- w10_goldmsi %&gt;% mutate( id = seq(1, nrow(w10_goldmsi)) ) head(w10_goldmsi) 8.5.2 Assumptions In R the linear regression needs to be coded first before testing assumptions. Linear regression models can be built using lm(), and the same formula notation used in aov(): w10_goldmsi_lm &lt;- lm(GoldMSI ~ years_training, data = w10_goldmsi) Let’s test our assumptions from the previous page. Linearity: a simple scatterplot tells us that our data probably is suitable for a linear regression: w10_goldmsi %&gt;% ggplot( aes(x = years_training, y = GoldMSI) ) + geom_point() Homoscedasticity: To plot a residual vs fitted plot in R, there are two ways: - plot(model, which = 1). plot(w10_goldmsi_lm, which = 1) Using ggplot, but giving the model name instead of the data and setting the x and y aesthetics as follows: ggplot(w10_goldmsi_lm, aes(x = .fitted, y = .resid)) + geom_point() Normality: our SW test is non-significant, so this assumption is not violated. Like aov models, residuals in lm models can be accessed using the $ operator. shapiro.test(w10_goldmsi_lm$residuals) ## ## Shapiro-Wilk normality test ## ## data: w10_goldmsi_lm$residuals ## W = 0.98995, p-value = 0.8289 Output The output that R gives us for a linear regression comes in two parts: a test of the overall model, and a breakdown of the predictors and their slopes. summary(w10_goldmsi_lm) ## ## Call: ## lm(formula = GoldMSI ~ years_training, data = w10_goldmsi) ## ## Residuals: ## Min 1Q Median 3Q Max ## -35.617 -9.227 2.025 9.773 33.666 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 53.901 5.233 10.300 8.33e-16 *** ## years_training 4.858 1.138 4.271 5.86e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.61 on 72 degrees of freedom ## Multiple R-squared: 0.2021, Adjusted R-squared: 0.191 ## F-statistic: 18.24 on 1 and 72 DF, p-value: 5.859e-05 Let’s start with the overall model. This tells us whether the regression model as a whole is significant. A couple of things to note here. R2 is called the coefficient of determination. This value tells how much variance in the outcome is explained by the predictor. Our value is .202, which means that 20.2% of the variance in Gold-MSI scores is explained by years of training. The overall model test is essentially an ANOVA (we won’t go too deep into why just yet!), which tells us whether the model is significant. In this case, it is (F(1, 72) = 18.24, p &lt; .001). Now we can look at the middle of the output, which gives the model coefficients. This part of the output tells us whether the predictors are significant. (Given the overall model is, with one predictor we’d expect this to be significant as well!). The Estimate column gives us the B coefficient, which is our slope. This tells us how much our outcome increases when our predictor increases by 1 unit. From this, we can see that years of training is a significant predictor of Gold-MSI scores (t = 4.27, p &lt; .001); for every year of training, Gold-MSI scores increase by 4.86 (given by the value under Estimate, next to our predictor variable). "],["predictions.html", "8.6 Predictions", " 8.6 Predictions A significant result from a linear regression tells us that our IV significantly predicts our outcome. We can actually use the results of the regression to make predictions about our outcome. This can be really useful in a number of contexts. 8.6.1 Revisiting the linear regression equation Let’s come back to the equation for a linear regression: \\[ y = \\beta_0 + \\beta_1x + \\epsilon_i \\] The results from the linear regression on the previous page allow us to construct a line of best fit. Using this line of best fit, we can make predictions about a participant’s score on the dependent variable, given their score on the independent/predictor variable. 8.6.2 Building a regression equation Here’s the coefficient table from the previous page: summary(w10_goldmsi_lm) ## ## Call: ## lm(formula = GoldMSI ~ years_training, data = w10_goldmsi) ## ## Residuals: ## Min 1Q Median 3Q Max ## -35.617 -9.227 2.025 9.773 33.666 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 53.901 5.233 10.300 8.33e-16 *** ## years_training 4.858 1.138 4.271 5.86e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.61 on 72 degrees of freedom ## Multiple R-squared: 0.2021, Adjusted R-squared: 0.191 ## F-statistic: 18.24 on 1 and 72 DF, p-value: 5.859e-05 This table tells us the following things: The value of the intercept, \\(\\beta_0\\), is 53.901 The value of the slope, \\(\\beta_1\\), is 4.858 Now we can make our equation as such: \\[ GoldMSI = 53.901 + (4.858 \\times Years) \\] We can now use this to predict scores! 8.6.3 An example prediction Participant 47, highlighted in green below, has 5 years of musical training. What would their predicted Gold-MSI score to be? ## `geom_smooth()` using formula = &#39;y ~ x&#39; We can use the equation we just built to calculate a predicted score: Therefore, we would predict someone with 5 years of musical training to have a Gold-MSI score of 78.191. This is where the line sits. Notice however, that the predicted value is noticeably different to the participant’s actual value (which in this instance is 56). The difference between the predicted and the actual value is called the residual - precisely the same residual that we aim to minimise when we fit a regression line to begin with (as well as the same residuals we do assumption tests on). \\(GoldMSI = 53.901 + (4.858 \\times Years)\\) \\(GoldMSI = 53.901 + (4.858 \\times 5)\\) \\(=78.191\\) A warning While these predictions can be useful, there are two warnings that should be kept in mind. Extrapolation is dangerous. While we might get data that appears linear, there is nothing to say that this data will remain linear outside of the bounds of our data. Extrapolating data refers to making inferences beyond the available range, and should be avoided. Don’t forget that some data have logical boundaries. For example, the Gold-MSI’s maximum possible score is 126 across all scales. Any preditions that are higher than this are therefore quite easily nonsensical. "],["multiple-regression-theory.html", "8.7 Multiple regression: Theory", " 8.7 Multiple regression: Theory If all of that stuff on the previous page made sense then great! You’re now ready to tackle multiple regressions, which are an extension of the simple linear regression. You’ll see that much of the same stuff applies here, but a few things change… 8.7.1 Multiple regression Multiple regression is used when we want to test multiple predictors against an outcome variable. It’d be a safe bet to say that multiple regression and its various forms are probably one of the most used statistical tests in music psychology literature as a whole - you’ll see them everywhere! No introduction to linear regression would really be complete without at least scratching the surface of multiple regression. The name “multiple regression” is actually a fairly generic term in some respects, as it describes any instance of regression with two or more predictors. I say that because there are several forms of multiple regression, such as: Standard multiple regression, which we will cover in this subject Hierarchical multiple regression, where you split your analysis into blocks Stepwise multiple regression, where algorithms attempt to select the best predictors And more! 8.7.2 The regression equation, once again Recall that in a simple linear regression, we had this formula to describe the line of best fit: \\[ y = \\beta_0 + \\beta_1x + \\epsilon_i \\] In a multiple regression, we work with an extension of this formula. The key part here is the part of the equation labelled \\(\\beta_1x\\) - this is the part of the equation that relates to the individual predictor and its slope (i.e. how it predicts the outcome). We extend this in a multiple regression. For example, say we now had two predictors: \\[ y = \\beta_0 + \\beta_1x_1 +\\beta_2x_2 + \\epsilon_i \\] We now have a term for our first predictor \\(\\beta_1x_1\\) and for our second: \\(\\beta_2x_2\\). x1 and x2 simply mean predictor 1 and predictor 2. The betas here are still regression slopes; the subscript numbers just indicate which predictor they correspond to. From here on, much of the same reasoning that we saw in the early pages of this module apply. The primary hypotheses are now about whether each slope is significantly different from zero - which would indicate whether each predictor does significantly predict the outcome. 8.7.3 Assumption testing in multiple regressions All of the following assumption tests apply: Linearity Homoscedasticity Normality Multicollinearity We’ll test these on the next page! "],["multiple-regressions-in-r.html", "8.8 Multiple regressions in R", " 8.8 Multiple regressions in R Phew - we’re almost there! Let’s round out this week’s module by doing a standard multiple regression in Jamovi. 8.8.1 Example data Let’s look at a real set of data from Rakei et al. (2022). Their study looked at what predicts how prone people are to flow - the experience of being ‘in the moment’ and extremely focused while doing a task, such as performing. They measured a wide range of personality and emotion-related variables, but we’ll focus on just a subset here: Trait anxiety: broadly, refers to people’s tendency to feel anxious Openness to experience: a personality trait that describes how likely people are to seek new experiences DFS_Total: a measure of proneness to flow. Let’s see if trait anxiety and openness predict flow proneness. w10_flow &lt;- read_csv(here(&quot;data&quot;, &quot;week_10&quot;, &quot;w10_flow.csv&quot;)) ## Rows: 811 Columns: 6 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (6): id, age, GoldMSI, DFS_Total, trait_anxiety, openness ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. To build a multiple regression model in R, we simply need to add (literally) more terms to lm(): w10_flow_lm &lt;- lm(DFS_Total ~ trait_anxiety + openness, data = w10_flow) 8.8.2 Assumption checks The Shapiro-Wilks test suggests that our normality assumption is violated - though if you look at the relevant Q-Q plot, it doesn’t appear to be very severe. shapiro.test(w10_flow_lm$residuals) ## ## Shapiro-Wilk normality test ## ## data: w10_flow_lm$residuals ## W = 0.99311, p-value = 0.0008488 The residuals-fitted plot for our data looks ok - no obvious conical shape, suggesting that the homoscedasticity assumption is intact. ggplot(w10_flow_lm, aes(x = .fitted, y = .resid)) + geom_point() Lastly, our multicollinearity statistics look good - no violations suggested here. # Variance DescTools::VIF(w10_flow_lm) ## trait_anxiety openness ## 1.025955 1.025955 # Tolerance 1/DescTools::VIF(w10_flow_lm) ## trait_anxiety openness ## 0.9747012 0.9747012 8.8.3 Output Let’s start as always by taking a look at our output. Our regression explains 13.6 of the variance in flow proneness (R2 = .136), and our overall model is significant (F(2, 808) = 63.81, p &lt; .001). With that in mind, let’s turn to our individual predictors. Remember, now we have two predictors to consider - and we need to interpret them individually as well. summary(w10_goldmsi_lm) ## ## Call: ## lm(formula = GoldMSI ~ years_training, data = w10_goldmsi) ## ## Residuals: ## Min 1Q Median 3Q Max ## -35.617 -9.227 2.025 9.773 33.666 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 53.901 5.233 10.300 8.33e-16 *** ## years_training 4.858 1.138 4.271 5.86e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.61 on 72 degrees of freedom ## Multiple R-squared: 0.2021, Adjusted R-squared: 0.191 ## F-statistic: 18.24 on 1 and 72 DF, p-value: 5.859e-05 We can see that both trait anxiety and openness are significant predictors of flow proneness, but interpreting the direction is also really important here. For that, we turn to our estimates: The estimate for trait anxiety is B = -.111, suggesting that higher trait anxiety predicts lower flow proneness (p &lt; .001) On the other hand, the estimate for openness is B = .834, suggesting that higher openness predicts higher flow proneness (p &lt; .001). Therefore, we can conclude that both are significant predictors and have opposite effects to each other. We can write this up as follows: We ran a multiple regression with trait anxiety and openness as predictors, and flow proneness as an outcome. The overall regression model was significant (F(2, 808) = 63.81, p &lt; .001), and explained 13.6% of the variance in flow proneness. Trait anxiety significantly predicted flow proneness (B = -.11, t = -8.70, p &lt; .001); higher trait anxiety predicted lower flow proneness. Openness was also a significant predictor (B = .83, t = 5.74, p &lt; .001); higher openness predicted higher flow proneness. We ran a multiple regression with trait anxiety and openness as predictors, and flow proneness as an outcome. The overall regression model was significant (F(2, 808) = 63.81, p &lt; .001), and explained 13.6% of the variance in flow proneness. Trait anxiety significantly predicted flow proneness (B = -.11, t = -8.70, p &lt; .001); higher trait anxiety predicted lower flow proneness. Openness was also a significant predictor (B = .83, t = 5.74, p &lt; .001); higher openness predicted higher flow proneness. But there’s one more thing we can consider here… 8.8.4 Comparing predictive strength When we do a multiple regression, we can actually identify which of our predictors is the strongest predictor of the outcome. Our estimate column alone won’t tell us that, because these are unstandardised coefficients. They are unstandardised because they describe the relationship in terms of the original units of each measure. An increase of 1 unit on trait anxiety is not necessarily the same thing as a 1 unit increase in openness because they are on different scales, so we can’t compare them directly! However, if we were to standardise these coefficients, we would then bring them all into the same scale (think back to z-scores!) - which would allow us to directly compare which predictor leads to a greater change in the outcome. Jamovi will calculate a standardised estimate (sometimes called standardised coefficient), which is presented in the rightmost column of the above table. These are simply regression coefficients that have been standardised, meaning that they are all on the same scale. This means that we can just compare the numbers of the standardised estimates (ignoring positive/negative signs) to see which one is the greatest predictor of the outcome. In the example above, trait anxiety has a standard coefficient of .288, whereas openness has a standard coefficient of .190. Trait anxiety is therefore a stronger predictor of flow proneness, because a 1 standard unit increase in trait anxiety leads to a greater increase in flow proneness. "],["factorial-anovas.html", "Chapter 9 Factorial ANOVAs ", " Chapter 9 Factorial ANOVAs "],["introduction.html", "9.1 Introduction", " 9.1 Introduction In Week 9, when we talk about ANOVAs we conduct one-way ANOVAs. These tests from Week 9 are called one-way ANOVAs because there is only one IV with multiple levels being tested. However, in many research designs we will want to test the effect of two or more categorical variables at the same time (for example, experimental conditions or variables that capture important categories, such as participant sex). When we want to test the effect of more than one IV, we start getting into what we call factorial ANOVAs. Factorial ANOVAs are used when we have two or more IVs, each with at least two levels per variable. This is common in a lot of research designs, where either multiple categorical variables are collected as part of the data collection phase or categorical variables are created as part of the analysis process. We will talk about this more on the next page, but factorial designs are particularly useful for testing interactions, and the effects of both of your IVs together. 9.1.1 Conventions for factorial designs When reporting results for factorial designs, it is expected that you report how many levels each variable had. For instance, let’s say your two IVs are participant sex (male and female) and experimental group (group A, B and C). If you were to test this factorial design, there are a couple of ways you could report this: A Sex (2) x Group (3) factorial ANOVA A 2 x 3 factorial ANOVA A Sex x Group (2 x 3) factorial ANOVA The first one is the preferable because it lays out the conditions clearly. "],["interactions.html", "9.2 Interactions", " 9.2 Interactions 9.2.1 What is an interaction? Pretend you’ve been enacting a singing intervention in a school of kids, where one group of kids have been singing daily and another group have not been. You’re interested in whether the singing intervention has an effect on their wellbeing. By and large, the singing intervention does - there is a clear difference between the kids who get singing sessions and kids who don’t. However, you notice that how effective the intervention is depends on whether they are boys or girls. The girls appear to benefit the most, while the boys don’t seem to as much. In other words, the effectiveness of the intervention is contingent on the biological sex of the child. This is an example of an interaction, where the effect of one IV depends on the effect of another IV. The consequence of an interaction is that the two IVs both influence the DV together (in a non-additive manner). Interactions can be important for understanding how certain phenomena work. Consider the two plots below, that show the relationship between two predictors (X and Group) and one outcome (on the y-axis). In the graph on the left, there is a clear difference between groups 1 and 2. There is also a clear difference between A, B and C on X; however, this is constant. In the graph on the right, there’s still a clear difference between groups 1 and 2. However, the difference is greater between different groups. For group 1, there is no change from A to C, but there is for group 2; in other words, the effect of X depends on the effect of Group. The easiest way of demonstrating an interaction is by using an interaction plot, like the one above. This kind of graph plots means as dots, and joins different groups/IVs together by lines. Interaction plots with error bars (e.g. +/- 1 standard error) provide the clearest way of graphing of an interaction effect. 9.2.2 Testing for interactions We can test for interactions when we have at least two independent variables/outcomes, using both ANOVAs and regressions. The majority of this module will focus on instances with two predictors, as they are easiest to conceptualise. By default, if we have two predictors - A, and B, and an outcome, Y - our model will have the following terms: A, or the main effect of A (i.e. of A only) B, the other main effect A x B, which is our interaction effect Therefore, we end up with two types of effects that we need to interpret: main effects, and interaction effects. An interaction effect is what we call a higher-order term, in that it is a more complex term in our model. We test the significance of each term, giving us three p-values and sets of test statistics. Here’s an example of a two-way ANOVA with a significant interaction. Notice that there are three effects here: one for gender, one for education and the gender x education level interaction. How do we interpret this? Clearly, we have no main effect of gender (p = .448) but we do have an effect of education level (p &lt; .001). We also have a significant interaction term: gender x education (p = .002). Here is where something called the principle of marginality kicks in. The principle of marginality, states that if two variables interact with each other, the main effects of each variable are marginal to their interaction. In more simple terms, this means that a significant interaction is a better explanation of the main effects than the main effects themselves. In context, this means that the significant effect of education level is actually best explained by decomposing the gender x education level interaction. Therefore, if you have a significant interaction you want to break this down first. If the interaction is not significant, you can run post-hocs on the main effects only. But like a regular ANOVA, this only tells us that there is an interaction. How do we find out which means are different? 9.2.3 Simple effects tests One option is to conduct post-hoc tests like normal, and run post-hocs on the interaction term. But this is not necessarily meaningful: A more targeted approach is to conduct simple effects tests. Simple effects tests are a form of pairwise comparisons that are run to break down an interaction. It involves running pairwise comparisons between one predictor at every level of the other predictor. Using the example above, this might include running pairwise comparisons between education levels for males and females separately: Or, to spin it the other way, you might compare males and females for each education level separately: Generally, it is wise to run simple effects tests in only one way - as this decomposes the interaction into something that is interpretable. This is usually guided by theoretical reasons (i.e. a hypothesis about which simple effects to run). Of course, a good graph will tell the rest of the story: "],["r-specific-considerations.html", "9.3 R-specific considerations", " 9.3 R-specific considerations This page discusses some basic considerations about performing factorial ANOVAs in R. 9.3.1 The afex package In the sections on one-way ANOVAs we largely stuck to using base R’s aov() function, or the anova_test() function from the rstatix package. While we can absolutely continue to use these for factorial ANOVAs, one problem is that factorial ANOVAs are inherently more complex models than their one-way progenitors. For instance, aov() only really works with balanced designs, which is where every cell (i.e. group x group combination of your IVs/predictors) has the same number of participants. To use an example, a 2 x 2 ANOVA where each of the four possibilities (e.g. Level 1 Variable A + Level 1 Variable B…) has 20 participants is a balanced design. In contrast, if A1-B2 had 40 participants and the other possibilities had 20 participants, this would be an unbalanced design. Unbalanced designs introduce several complexities for ANOVAs. For that reason, now is a good time to introduce the afex package, which stands for “analysis of factorial experiments”. The afex package is designed for factorial ANOVAs in particular. It provides a very nice and convenient way of running factorial ANOVAs in a way that’s not too hard, while still being flexible enough for more hardcore R users. In particular, the function you will want to keep in mind is aov_ez(). 9.3.2 Writing interactions in R The aov_ez() function that was just mentioned doesn’t use the same kind of formula notation that we saw in the other test sections, and that’s to make it easier to run. However, these analyses absolutely can be recreated in formula notation, and you will see many instances of this. Recall the basic layout for a formula in R: {r} eval = FALSE} lm(outcome ~ predictor, data = dataset) aov(outcome ~ predictor, data = dataset) To extend this to two variables with no interaction, you can simply add the second predictor as such: {r} eval = FALSE} aov(outcome ~ predictor_1 + predictor_2, data = dataset) To code for an interaction term, however, there either needs to be an explicit term for the interaction (denoted using a colon between the two interaction variables): aov(outcome ~ predictor_1 + predictor_2 + predictor_1:predictor_2, data = dataset) OR as the shorthand for above, which uses the asterisk. Note that the asterisk will include both the interaction term and all of its main effects (a la principle of marginality): {r} eval = FALSE} aov(outcome ~ predictor_1 * predictor_2, data = dataset) For what we do in this section, this is ok - but you may find in certain circumstances that you don’t want all of these terms together, so the first approach will let you specify what terms to include in the model. 9.3.3 Simple effects tests in R Simple effects tests, weirdly, are difficult to do in Jamovi - you have to filter your data, run pairwise tests/one-way ANOVAs on them, and then make sure you save the output before running the next test (as Jamovi’s self-updating nature will mean that your output will change every time your filter does). Alternatively, you have to install the gamlj module and learn how to code linear models for neater simple effects tests. In R, this is relatively easy through the use of group_by() and pairwise_t_test(). The code below will run simple effects tests for variable 1: data %&gt;% group_by(variable_1) %&gt;% pairwise_t_test(outcome ~ variable_2, p.adjust.method = &quot;holm&quot;) To run simple effects tests for variable two, the variable names simply need to be swapped. "],["two-way-between-groups-anova.html", "9.4 Two-way between groups ANOVA", " 9.4 Two-way between groups ANOVA 9.4.1 Introduction Two-way between-groups ANOVAs are used when you have two categorical IVs, both of which are between-groups variables. For example, you might have the following IVs: Sex: Male or female Experimental group: Group A or Group B There are four possibilities here - males in Group A, females in Group A, males in Group B and females in Group B. These are mutually exclusive categories in the context of this design. 9.4.2 Example One of the earliest modern studies on music’s effect on consumer behaviour came from Hargreaves and North, who played either French or German music in a wine shop. They looked at how much people spent on French and German wines, and tested to see whether the effect of background music and wine type had an effect on spending. We’re going to step through an analogous (fictional) example. Pretend we played either country or classical music, and looked at when people purchased beer or wine. (We’re going to assume we’ve removed people who bought both beer and wine together.) We’re interested in whether these two variables had an effect on how much people spent. Therefore, we have two between-groups IVs: Alcohol type being purchased: was the person buying beer or wine? Genre of background music: was country or classical music being played at the time? We therefore has two independent variables: alcohol type (beer, wine) and genre of background music (country, classical). Her dependent variable is total spend. This can be described in two key ways: Two-way ANOVA (alcohol type x genre) 2 x 2 ANOVA: more specifically, 2 (beer, wine) x 2 (country, classical) ANOVA between alcohol type and genre We’d use a two-way ANOVA to model the effects of two IVs on one dependent variable. Let’s start by generating descriptives and a graph to visualise what we’re looking at: twoway_alcohol &lt;- read_csv(here(&quot;data&quot;, &quot;factorial&quot;, &quot;twoway_bganova.csv&quot;)) ## Rows: 200 Columns: 4 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): alcohol_type, genre ## dbl (2): ptcpt, spend ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. twoway_alcohol %&gt;% group_by(alcohol_type, genre) %&gt;% summarise( mean = mean(spend), na.rm = TRUE, sd = sd(spend, na.rm = TRUE), se = sd/length(spend) ) %&gt;% ggplot( aes(x = alcohol_type, y = mean, colour = genre, group = genre) ) + geom_point() + geom_line() + geom_errorbar(aes(ymin = mean - se, ymax = mean + se), width = 0.2) ## `summarise()` has grouped output by &#39;alcohol_type&#39;. You can override using the `.groups` argument. 9.4.3 Assumption testing The assumptions for a two-way ANOVA are the same as a one-way between groups ANOVA: The data should be independent of each other. Equality of variance: Look at Levene’s test: twoway_alcohol_aov &lt;- aov(spend ~ alcohol_type * genre, data = twoway_alcohol) twoway_alcohol %&gt;% levene_test(spend ~ alcohol_type * genre) 9.4.4 Normality of residuals: Look at the Shapiro-Wilk test (on the residuals) or a Q-Q plot: shapiro.test(twoway_alcohol_aov$residuals) ## ## Shapiro-Wilk normality test ## ## data: twoway_alcohol_aov$residuals ## W = 0.99647, p-value = 0.9295 Both assumptions appear to be intact. 9.4.5 Output Let’s first look at our output for the omnibus ANOVA. The way to read this table is much like the same for a one-way ANOVA, but now we need to read across each line - as each line represents a different effect. So we see that: There is no main effect for alcohol (p = .137), There is a main effect for genre (p &lt; .001), Importantly, there is a significant interaction effect for alcohol x genre (p &lt; .001). Note that here, we use the aov_ez() function from the afex package. Because in many instances we work with unbalanced data (i.e. each group x group combination does not have the same number of participants), we tend to calculate something called Type III Sums of Squares/ANOVAs. We won’t dive too much into this, but this is essentially about how sums of squares are calculated, and how the effects are tested. Learning Statistics with R has an excellent explanation of what the various types are. twoway_alcohol_aov &lt;- aov_ez( data = twoway_alcohol, id = &quot;ptcpt&quot;, dv = &quot;spend&quot;, between = c(&quot;alcohol_type&quot;, &quot;genre&quot;), anova_table = list(es = &quot;pes&quot;), include_aov = TRUE ) ## Converting to factor: alcohol_type, genre ## Contrasts set to contr.sum for the following variables: alcohol_type, genre twoway_alcohol_aov ## Anova Table (Type 3 tests) ## ## Response: spend ## Effect df MSE F pes p.value ## 1 alcohol_type 1, 196 1.02 2.23 .011 .137 ## 2 genre 1, 196 1.02 4295.08 *** .956 &lt;.001 ## 3 alcohol_type:genre 1, 196 1.02 519.96 *** .726 &lt;.001 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 Because we have a significant interaction term, we now need to turn to conducting simple effects tests to decompose where this effect is. You might want to use Jamovi’s post-hoc functions to generate every possible pairwise comparison, like so: tukey_hsd(twoway_alcohol_aov$aov) Remember, for a simple effects test we want to test for differences between the levels of one variable, at every level of the other variable. In our instance, that may be comparing genre type (classical vs country) for beer purchases, and then for wine purchases. Our post-hoc table gives us this information - but not all rows are necessarily important here. Let’s hold alcohol as a constant, and compare genre for each type of alcohol purchased. That means that we need to look for the following comparisons: For beer, country vs classical For wine, country vs classical twoway_alcohol %&gt;% group_by(alcohol_type) %&gt;% pairwise_t_test(spend ~ genre, detailed = TRUE, p.adjust.method = &quot;holm&quot;) If we were to frame it in terms of the variables in the dataset, we would be looking for: Beer country - beer classical Wine country - wine classical Conveniently, this is captured in the very top and bottom rows of our post-hoc output table. We can see that beer purchases were significantly greater with classical music on in the background than country (t = 30.24, p &lt; .001). Likewise, wine purchases were also significantly greater with classical music than country music (t = 62.42, p &lt; .001). Of course, it might make more sense to do the simple effects tests the other way round; in other words, hold genre constant and compare how much was spent on each alcohol type. You can still absolutely find out this information, but now you would be looking for the following rows: For country, wine vs beer (wine country - beer country): t = 15.34, p &lt; .001 For classical, wine vs beer (wine classical - beer classical): t = 16.85, p &lt; .001 twoway_alcohol %&gt;% group_by(genre) %&gt;% pairwise_t_test(spend ~ alcohol_type, p.adjust.method = &quot;holm&quot;) 9.4.6 Write-up A two-way ANOVA was conducted to test whether alcohol type and music genre had an effect on spending habits. We found a significant main effect of music genre (F(1, 196) = 4295.08, p &lt; .001) but no significant main effect of alcohol type (p = .137). We found a significant two-way interaction between alcohol type and genre (F(1, 196) = 519.97, p &lt; .001). To follow up this two-way interaction, we conducted simple effects tests. The simple effect of genre was analysed for each level of alcohol type with Holm corrections. On average, people spent $6.11 more on beer if they heard classical music compared to country music (t(196) = 30.242, p &lt; .001). Likewise, on average people spent $12.64 more on wine if they heard classical music compared to country (t(196) = 62.416, p &lt; .001). "],["two-way-repeated-measures-anova.html", "9.5 Two-way repeated measures ANOVA", " 9.5 Two-way repeated measures ANOVA 9.5.1 Introduction Similar to its one-way counterpart, in a two-way ANOVA we assess the effect of two within-subjects variables on a dependent variable. The following example is completely fictional, and has no real grounding in theory (as far as I’m aware). It’s intentionally facetious, but hopefully demonstrates the process of doing a two-way repeated measures ANOVA. 9.5.2 Example You met Victor and Gloria in the second statistics assignment, who were working with data looking at what predicts high school GPA. They’ve now moved onto a new project about whether listening to happy or sad music may affect exam performance. To do this, they design a nifty little study. They recruit 20 participants and bring them into the lab. This is their procedure: Participants come into the lab and do a series of standard maths exams. The maths exams are either easy, medium or hard, and participants are randomly assigned to do either the easy or the hard one first. They are also randomly assigned either happy or sad background music as they do the exam. Once they have completed the first exam, they take a 10 minute break and then do another exam that is easy, medium, or hard, and with either happy or soft music in the background. This process repeats until they have done all combinations of difficulty and background music. Each exam is scored out of 100. We have two within-subjects independent variables here: Difficulty of the exam (3 levels: easy, medium, hard) Background music (2 levels: happy or sad) Every participant therefore does 6 maths exams: easy-happy, easy-sad, medium-happy, medium-sad, hard-happy and hard-sad. The dependent variable of interest is their exam score. We’re interested in whether the difficulty and the music type have an effect on exam performance. Because our two IVs are within-subject variables, we will want to use a two-way repeated measures ANOVA. twoway_exam &lt;- read_csv(here(&quot;data&quot;, &quot;factorial&quot;, &quot;twoway_rmanova_long.csv&quot;)) ## Rows: 80 Columns: 4 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): difficulty, music ## dbl (2): ptcpt, grade ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Let’s start with the usual descriptives and graphs: Eyeballing the graph, we can see that there might be some sort of effect happening. Unsurprisingly, it looks like the easy maths exams are, well. easier, because people are scoring better on them. There might be an effect of music, because in both easy and hard conditions it looks like people do better with happy music compared to sad music. But the interaction plot tells us there’s something clearly going on, and it paints a really interesting story on its own. It appears that there’s almost a ‘plateau’-ing effect with medium difficulty, in that both happy and sad hit the same point in exam scores. However, it looks like for happy music, harder exams see no further decrease in performance - but there is a sharp drop again for sad music. 9.5.3 Assumptions The assumptions for a two-way RM ANOVA is the same as a one-way: The data from the conditions should be normally distributed. (More specifically, the residuals should be normally distributed.) The data for each subject should be independent of every other subject. Sphericity must be met. As is the case with one-way repeated measures ANOVAs, the assumption of sphericity is only tested when there are three or more levels; with only two levels, the assumption is always met. The output is below with the main ANOVA output. The sphericity for all of our effects is intact, so we don’t have any issues here, but if we did the same principle would apply - we would apply our corrections depending on the value of epsilon. Let’s also see if our variables are normally distributed: R-Note: For a repeated measures ANOVA, for some reason the closest I can get to generating what Jamovi does is to build an aov() model without an explicit Error() term - as would be the case for a fully between-subjects ANOVA. You can then use the standardised residuals to create a Q-Q plot using the below code. Note that broom:augment() is just a helper function that nicely extracts the residuals: aov(grade ~ difficulty * music, data = twoway_exam) %&gt;% broom::augment() %&gt;% ggplot(aes(sample = .std.resid)) + geom_qq() + geom_qq_line() It’s not… great but for the purposes of demonstration, we’ll run with it anyway. 9.5.4 Output Here’s our output from R: twoway_exam_aov &lt;- aov_ez( data = twoway_exam, id = &quot;ptcpt&quot;, dv = &quot;grade&quot;, within = c(&quot;difficulty&quot;, &quot;music&quot;), anova_table = list(es = &quot;pes&quot;), include_aov = TRUE ) twoway_exam_aov ## Anova Table (Type 3 tests) ## ## Response: grade ## Effect df MSE F pes p.value ## 1 difficulty 1, 19 43.52 288.35 *** .938 &lt;.001 ## 2 music 1, 19 44.94 28.84 *** .603 &lt;.001 ## 3 difficulty:music 1, 19 44.24 6.02 * .241 .024 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 # To get sphericity output summary(twoway_exam_aov) ## ## Univariate Type III Repeated-Measures ANOVA Assuming Sphericity ## ## Sum Sq num Df Error SS den Df F value Pr(&gt;F) ## (Intercept) 261518 1 605.55 19 8205.5166 &lt; 2.2e-16 *** ## difficulty 12550 1 826.95 19 288.3499 6.095e-13 *** ## music 1296 1 853.95 19 28.8365 3.503e-05 *** ## difficulty:music 266 1 840.55 19 6.0229 0.02394 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Once again this is quite a busy table! We can see that there is a significant main effect of difficulty (F(2, 38) = 189.61, p &lt; .001), as well as a significant main effect of music type (F(1, 19) = 18.39, p &lt; .001). There is also a significant interaction effect of difficulty and music (F(2, 38) = 10.29, p &lt; .001). We can also see our sphericity output here; neither the difficulty variable (W = .880) nor the difficulty x music interaction (W = .788) terms show significant violation of sphericity (p .05). Therefore we have not corrected for anything in our main ANOVA output. Note that because music only has two levels, there is no sphericity test for it. To decompose this, we’ll do our usual simple effects tests - holding one variable constant and running pairwise comparisons with the other. For this example, we’ll just hold the difficulty constant and compare music genres. We can absolutely reverse the simple effects, but for the sake of simplicity we’ll just do it the one way round for this example. From this we can see that for easy exams, on average people scored 4.4 points higher with happy music compared to sad music (p = .023). On medium exams, there was no difference between happy and sad music (p = .900). Lastly, for harder exams people who listened to happy music scored, on average, 11.7 points higher than people who listened to sad music (p &lt; .001). This suggests overall that happy music is probably better for completing exams than sad music, though the medium difficulty exam is a strange one here. If, instead, we wanted to hold music type constant and test differences in difficulty, we would get this output. This paints a much more interesting picture about what is going on, and might be a more interesting way of decomposing the interaction. But remember - only do one or the other! "],["two-way-mixed-anova.html", "9.6 Two-way mixed ANOVA", " 9.6 Two-way mixed ANOVA 9.6.1 Introduction In a mixed factorial design, we want to see the effect of a between groups variable and a within-groups variable on a continuous DV. These are also sometimes called repeated measures ANOVAs (i.e. repeated measures designs with a between-group variable), but this can be a little confusing; for that reason, calling them a mixed factorial model is preferred. The data we’ll use for this one is of a mock longitudinal randomised-controlled trial. This kind of design is common when testing the effect of an intervention - and that’s exactly what we’ll do here. This mock dataset is also a bit more complex than the previous ones we’ve looked at just to give the full range of assumption testing and modelling that we have to do. 9.6.2 Example Elaine and Chaise ran an RCT testing the effect of a music listening intervention on anxiety scores. To do this, participants were randomly allocated to three conditions: a music listening intervention (listen to a podcast for 30 minutes a day), a control exercise intervention (walk for 30 minutes a day) and a control nothing intervention (do nothing). Participants were tested at three timepoints: at the start of the study, at the 6 week mark and then at the 12 week mark. (With thanks to the datarium package for providing a perfect test dataset for this example.) In other words, we have two variables: Intervention: a between-groups variable, because participants were randomly assigned to one of three interventions (3 levels; Control, Exercise, Music) Timepoint, a within-groups variable as all participants were measured at 3 timepoints (3 levels: 0 weeks, 6 weeks, 12 weeks) This gives us a two-way, 3 x 3 mixed factorial ANOVA. Let’s start by visualising anxiety scores for the three interventions. A boxplot or bar graph is useful here. twoway_mixed &lt;- read_csv(here(&quot;data&quot;, &quot;factorial&quot;, &quot;twoway_mixed.csv&quot;)) ## Rows: 135 Columns: 4 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): group, time ## dbl (2): id, anxiety ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. twoway_mixed %&gt;% ggplot( aes(x = time, y = anxiety, colour = group) ) + geom_boxplot(size = 1) + # scale_x_discrete(labels = c(&quot;0 weeks&quot;, &quot;6 weeks&quot;, &quot;12 weeks&quot;)) + labs( x = &quot;Time point&quot;, y = &quot;Anxiety score&quot;, colour = &quot;Intervention\\ngroup&quot; ) + scale_colour_brewer(palette = &quot;Set1&quot;) + theme(legend.position = &quot;right&quot;) In a mixed ANOVA, we test the effect of both a between-groups and a within-groups variable. This means that the specific assumptions that apply to both between- and within-subject designs now carry over into this design. This means that the following assumptions apply: The data should have multivariate normality. Our QQ plot looks a little non-linear, so we might have an issue here. aov(anxiety ~ group * time, data = twoway_mixed) %&gt;% broom::augment() %&gt;% ggplot(aes(sample = .std.resid)) + geom_qq() + geom_qq_line() Homogeneity of variance - the between-subject groups should have the same variance, at each level of the within-subjects variable. None of the tests are significant (p &gt; .05), so we’re ok here. To do this in R, we run Levene’s test at each timepoint: twoway_mixed %&gt;% group_by(time) %&gt;% levene_test(anxiety ~ group, center = &quot;mean&quot;) ## Warning: There were 3 warnings in `mutate()`. ## The first warning was: ## ℹ In argument: `data = map(.data$data, .f, ...)`. ## Caused by warning in `leveneTest.default()`: ## ! group coerced to factor. ## ℹ Run `dplyr::last_dplyr_warnings()` to see the 2 remaining warnings. Sphericity of the within-subject variable must be met. Our sphericity assumption technically isn’t violated (p = .079), but this is so close to an arbitrary threshold that we might consider reporting corrected versions anyway. However, we’ll forge ahead with our original omnibus model and report that. aov_ez() can give you this in the output below. 9.6.3 Output Below is our main output from R. You’ll note that the omnibus outputs are actually split across two tables - one for the within-subject effects, and another for the between-subject effects. This simply makes it easy to identify which variables are what types. Based on the output, we can see that we have a significant main effect of group (F(2, 42) = 4.352, p = .019), and also a significant main effect of time (F(2, 84) = 394.909, p &lt; .001). We also have a significant interaction (F(4, 84) = 110.188, p &lt; .001). twoway_mixed_aov &lt;- aov_ez( data = twoway_mixed, id = &quot;id&quot;, dv = &quot;anxiety&quot;, between = &quot;group&quot;, within = &quot;time&quot;, anova_table = list(es = &quot;pes&quot;) ) ## Converting to factor: group ## Contrasts set to contr.sum for the following variables: group twoway_mixed_aov ## Anova Table (Type 3 tests) ## ## Response: anxiety ## Effect df MSE F pes p.value ## 1 group 2, 42 7.12 4.35 * .172 .019 ## 2 time 1.79, 75.24 0.09 394.91 *** .904 &lt;.001 ## 3 group:time 3.58, 75.24 0.09 110.19 *** .840 &lt;.001 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 ## ## Sphericity correction method: GG # To get details on sphericity summary(twoway_mixed_aov) ## ## Univariate Type III Repeated-Measures ANOVA Assuming Sphericity ## ## Sum Sq num Df Error SS den Df F value Pr(&gt;F) ## (Intercept) 34919 1 299.146 42 4902.6660 &lt; 2e-16 *** ## group 62 2 299.146 42 4.3518 0.01916 * ## time 67 2 7.081 84 394.9095 &lt; 2e-16 *** ## group:time 37 4 7.081 84 110.1876 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Mauchly Tests for Sphericity ## ## Test statistic p-value ## time 0.88364 0.079193 ## group:time 0.88364 0.079193 ## ## ## Greenhouse-Geisser and Huynh-Feldt Corrections ## for Departure from Sphericity ## ## GG eps Pr(&gt;F[GG]) ## time 0.89577 &lt; 2.2e-16 *** ## group:time 0.89577 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## HF eps Pr(&gt;F[HF]) ## time 0.9330916 1.037156e-40 ## group:time 0.9330916 1.461019e-30 Let’s decompose this with simple effects. Doing this in Jamovi is not trivial at all for some reason - you have to really get your hands dirty with Jamovi in order to make this work. There are two main ways you can do this: Run a one-way ANOVA at each timepoint - this would be the simple effect of group twoway_mixed %&gt;% group_by(time) %&gt;% pairwise_t_test(anxiety ~ group, p.adjust.method = &quot;none&quot;) Run a one-way repeated measures ANOVA for each group - this would be the simple effect of time twoway_mixed %&gt;% group_by(group) %&gt;% pairwise_t_test(anxiety ~ time, p.adjust.method = &quot;none&quot;, paired = TRUE) Let’s bulletpoint the main feature of each, and you can refer to the output below as you go through: For controls, anxiety scores were not significantly different between the 0 week and 6 week mark (p = .065). However, anxiety scores were significantly lower at 12 weeks compared to 6 weeks (p = .004). Scores were also significantly lower at 12 weeks compared to 0 weeks (p = .002). For the exercise group, anxiety scores were not significantly different from 0 weeks to 6 weeks (p = .089). However, anxiety scores significantly decreased from 6 weeks to 12 weeks (p &lt; .001). Scores were also significantly lower after 12 weeks than at 0 weeks (p &lt; .001). For the music group, anxiety scores significantly decreased between 0 weeks and 6 weeks (p &lt; .001). Scores were significantly lower at the 12 week mark again compared to the 6 week mark (p &lt; .001). Scores were also significantly lower at 12 weeks compared to 0 weeks (p &lt; .001). These comparisons are best supplemented with the differences in means. "],["three-way-anova.html", "9.7 Three-way ANOVA", " 9.7 Three-way ANOVA 9.7.1 Introduction Of course, we don’t have to stop at two IVs. Sometimes we may have designs where we want to At this point though, I want to ask you… why are you going to that complex of a design? To illustrate, consider a three-way ANOVA with variables A, B and C. In line with the principle of marginality, an ANOVA with all possible interactions will have: 3 main effects: A, B and C 3 simple (two-way) interaction effects: AxB, AxC, BxC 1 three-way interaction: AxBxC This is not a trivial thing to interpret! A significant three-way interaction means that e.g. the main effect of variable A depends on both variable B and variable C. This quickly becomes really difficult to interpret, particularly when one or more variables have multiple levels. What is the effect of A when B = 2 and C = 4, for instance? What about A when B = 2 and C = 3? etc etc. Nevertheless, three-way ANOVAs (and even beyond that) are not necessarily uncommon, so it doesn’t hurt to know about them. We’ll unpack the main concepts as we step through an example, for the sake of brevity. 9.7.2 Example This dataset is from a fictional experiment looking at how gender (male and female), risk of migraine (low or high) and different treatments (labelled X, Y and Z) impacted pain scores associated with a migraine headache. Below is a snippet of what our data looks like: library(datarium) ## Warning: package &#39;datarium&#39; was built under R version 4.3.2 data(headache) head(headache) We want to run a gender (2) x risk (2) x treatment (3) three-way ANOVA to see whether these predictors have an effect on overall pain scores. Let’s start by visualising this data. We cannot fully visualise a three-way interaction because we inherently need four dimensions (as we have three predictors and one outcome variable). The best way to approach this is to draw a series of two-way interaction plots, split by the third variable. In the example below, the graphs show treatment on the x-axis, pain scores on the y-axis, different lines for low and high risk and separate graphs for male and female participants: headache %&gt;% group_by(gender, risk, treatment) %&gt;% summarise( mean_pain = mean(pain_score), sd_score = sd(pain_score) ) %&gt;% ggplot( aes(x = treatment, y = mean_pain, colour = risk, group = risk) ) + geom_point(size = 2) + geom_line(size = 1) + facet_wrap(~c(gender)) + theme_pubr() ## `summarise()` has grouped output by &#39;gender&#39;, &#39;risk&#39;. You can override using the `.groups` argument. 9.7.3 Simple simple effects Consider a scenario where a three-way interaction is significant. Naturally, as we saw in the examples with two-way ANOVAs, we now have to turn to breaking down the lower-order terms to unpack this interaction. For a two-way ANOVA, this was easy - we simply interpreted the effect of variable A at each level of variable B, and vice versa. We called this a simple effects test. But now, we need to break down a three-way interaction by looking at every two-way interaction that is now significant. The ‘simple effect’ is now each two-way interaction, hence why the terminology of simple interaction effect/test is sometimes used. That, of course, means that if the two-way is significant, we need to run the next level of simple effects for each main effect. At this level, we call them simple simple main effects. That’s because we’re now looking at main effects that are simple effects of a simple interaction effect, which is itself a simple effect of a three-way interaction. If that didn’t make sense, consider running an alternate analysis to a three-way ANOVA. It helps to visualise this, so I’ll use the headache data to do so. Imagine that the three-way interaction between treatment, risk and gender is significant. Imagine that every other effect is also significant (we’ll test this further down). This would give us three sets of simple interactions to test: Treatment x risk, for each gender Treatment x gender, for each risk level Gender x risk, for each treatment Here is the first interaction visualised - treatment and risk, with separate plots for males and females: headache_summary &lt;- headache %&gt;% group_by(gender, risk, treatment) %&gt;% summarise( mean_pain = mean(pain_score), sd_score = sd(pain_score) ) ## `summarise()` has grouped output by &#39;gender&#39;, &#39;risk&#39;. You can override using the `.groups` argument. headache_summary %&gt;% ggplot( aes(x = treatment, y = mean_pain, colour = risk, group = risk) ) + geom_point(size = 2) + geom_line(linewidth = 1) + facet_wrap(~c(gender)) + theme_pubr() Here’s the second, which plots treatment x gender for each risk level: headache_summary %&gt;% ggplot( aes(x = treatment, y = mean_pain, colour = gender, group = gender) ) + geom_point(size = 2) + geom_line(linewidth = 1) + facet_wrap(~c(risk)) + theme_pubr() And finally, here is gender and risk by each treatment: headache_summary %&gt;% ggplot( aes(x = risk, y = mean_pain, colour = gender, group = gender) ) + geom_point(size = 2) + geom_line(linewidth = 1) + facet_wrap(~c(treatment)) + theme_pubr() And then within that, you would also want the main effects. 9.7.4 Assumptions and output Let’s run our basic ANOVA. Just like in Jamovi, we’ll do our usual assumption checks for our ANOVA model too. Based on Levene’s test and the Shapiro-Wilks test, we have no violations in multivariate equality of variance (p = .994) or normality (p = .398). headache %&gt;% levene_test(pain_score ~ gender * risk * treatment, center = &quot;mean&quot;) threeway_aov &lt;- aov_ez( data = headache, id = &quot;id&quot;, dv = &quot;pain_score&quot;, between = c(&quot;gender&quot;, &quot;risk&quot;, &quot;treatment&quot;), anova_table = list(es = &quot;pes&quot;), include_aov = TRUE ) ## Contrasts set to contr.sum for the following variables: gender, risk, treatment shapiro.test(threeway_aov$aov$residuals) ## ## Shapiro-Wilk normality test ## ## data: threeway_aov$aov$residuals ## W = 0.98212, p-value = 0.3981 threeway_aov ## Anova Table (Type 3 tests) ## ## Response: pain_score ## Effect df MSE F pes p.value ## 1 gender 1, 60 19.35 16.20 *** .213 &lt;.001 ## 2 risk 1, 60 19.35 92.70 *** .607 &lt;.001 ## 3 treatment 2, 60 19.35 7.32 ** .196 .001 ## 4 gender:risk 1, 60 19.35 0.14 .002 .708 ## 5 gender:treatment 2, 60 19.35 3.34 * .100 .042 ## 6 risk:treatment 2, 60 19.35 0.71 .023 .494 ## 7 gender:risk:treatment 2, 60 19.35 7.41 ** .198 .001 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 So what do our results tell us? Well: We have significant main effects of gender (F(1, 60) = 16.20, p &lt; .001), risk group (F(1, 60) = 92.69, p &lt; .001) and treatment (F(2, 60) = 7.32, p = .001). We have a significant two-way gender x treatment interaction (F(2, 60) = 3.34, p = .04), but no significant interactions between gender x risk (p = .708) and risk x treatment (p = .494). Our highest-order interaction, gender x risk x treatment is significant (F(2, 60) = 7.41, p = .001). Great, so we know that we’re dealing with a three-way interaction. But… how do we break that down? As in a two-way, we now need to break down simple main effects and simple interaction effects. In a two-way ANOVA, we broke down the two-way interaction by essentially running tests on the main effects (hence, simple main effects). In a three-way ANOVA, we attempt to start by testing for interactions between A and B, at each level of C. Think of it like running multiple two-way ANOVAs. In our case, let’s look at whether risk and treatment interact, for men and women separately. For simplicity, I’m defaulting to using rstatix’s anova_test() here, but an alternative approach is to filer the data by men and women and use these smaller datasets as calls to aov_ez. headache %&gt;% group_by(gender) %&gt;% anova_test(pain_score ~ treatment * risk, type = 3) Here, the risk x treatment interaction is significant for men (p = .016). However, for women the interaction is not quite significant (p = .054). Let’s break this down for the men by doing simple simple effects. Following the principle of starting with the highest-order factor first, we now need to decompose the two-way risk x treatment interaction for men. Is there a difference between treatments when risk is low? Here’s a graph to visualise what comparisons we’re looking at: Here’s one way to run the simple simple effets of treatment (with Holm adjustments by default): headache %&gt;% filter(gender == &quot;male&quot;) %&gt;% group_by(risk) %&gt;% pairwise_t_test(pain_score ~ treatment, detailed = TRUE) From this we can infer the following: - For high risk men, there was a significant difference in treatment between X and Y (p = .004), a significant difference between X and Z (p = .001) but no difference between X and Z (p = .347) - No significant difference between treatments for low risk men And here are the simple simple effects for risk in men: headache %&gt;% group_by(gender, risk) %&gt;% anova_test(pain_score ~ treatment, type = 3) headache %&gt;% group_by(gender, risk) %&gt;% emmeans_test(pain_score ~ treatment) This tells us that: - For treatment X there was a significant difference in pain scores between high and low risk (p &lt; .001) - Same for treatment Y - a significant difference between high and low risk in pain (p = .009) - No significant difference between high and low risk for treatment Z (p = .071). "],["multiple-regression-continued.html", "Chapter 10 Multiple regression continued", " Chapter 10 Multiple regression continued This section deals with some more advanced topics in ANOVAs and regression. It serves as a continuation to the first chapter on regression, and in particular focuses on multiple regressions. This chapter will cover the following: ANCOVA MANOVA Hierarchical regressions Model selection This module won’t cover continuous interactions because they are often considered under the topic of moderation - for which there will be a separate module. "],["revisiting-multiple-regression.html", "10.1 Revisiting multiple regression", " 10.1 Revisiting multiple regression Recall that the basic multiple regression looks something like this: \\[ y = \\beta_0 + \\beta_1x_1 +\\beta_2x_2 + \\epsilon_i \\] As a reminder, the coefficients in this formula correspond to the following: \\(\\beta_1\\) is the coefficient for predictor \\(x_1\\); i.e. as \\(x_1\\) increases by 1 unit, \\(\\hat y\\) (the predicted y value) increases by \\(\\beta_1\\) units, assuming \\(x_2\\) does not change \\(\\beta_2\\) is the coefficient for predictor \\(x_2\\), and describes how \\(\\hat y\\) changes assuming \\(x_1\\) does not change \\(\\epsilon_i\\) is the error term, which we assume is normally distributed We can expand this formula out to include \\(n\\) predictors, as follows: \\[ y = \\beta_0 + \\beta_1x_1 +\\beta_2x_2 + ... \\beta_nx_n + \\epsilon_i \\] "],["ancovas.html", "10.2 ANCOVAs", " 10.2 ANCOVAs 10.2.1 Introduction ANCOVA stands for Analysis of Co-variance. Its basic definition is that it is an ANOVA, but with the inclusion of a covariate (or a variable we need to control for). The basic idea is that we are performing an ANOVA between our predictors and outcomes after adjusting our predictors (our model) by another variable. Just like the ANOVA, ANCOVA is a fairly generic term that can refer to a multitude of analyses. In this book we’ll stick with ANOVAs relating to between-subjects predictors only. 10.2.2 Example The following data comes from Plaster (1989). The dataset is described as below: Male participants were shown a picture of one of three young women. Pilot work had indicated that the one woman was beautiful, another of average physical attractiveness, and the third unattractive. Participants rated the woman they saw on each of twelve attributes. These measures were used to check on the manipulation by the photo. Then the participants were told that the person in the photo had committed a Crime, and asked to rate the seriousness of the crime and recommend a prison sentence, in Years. Our main questions are: Does the perceived attractiveness of the “defendant” (the women in the photo) influence the number of years the mock juror (the participant) sentence them for? How does this relationship change after controlling for the perceived seriousness of the crime? Our dataset, jury_data, contains the following variables: Attr: The perceived attractiveness of the defendant (Beautiful, Average, Unattractive) Crime: The crime that was commited by the defendant (Burglary, Swindle) Serious: The perceived seriousness of the crime from 1-10 Years: The number of years the participant sentenced the defendant for 10.2.3 One-way ANCOVA To start us off, let’s begin with a simple one-way ANOVA between attractiveness and years. We can see this below. jury_data &lt;- read_csv(here(&quot;data&quot;, &quot;regression_2&quot;, &quot;anova_mockjury.csv&quot;)) jury_aov &lt;- aov(Years ~ Attr, data = jury_data) summary(jury_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Attr 2 70.9 35.47 2.77 0.067 . ## Residuals 111 1421.3 12.80 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 eta_squared(jury_aov, alternative = &quot;two.sided&quot;) ## For one-way between subjects designs, partial eta squared is equivalent to eta squared. ## Returning eta squared. From this we can infer that the effect of attractiveness is not significant (F(2, 111) = 2.77, p = .067, \\(\\eta^2\\) = .05, 95% CI = [0, .14]). In other words, perceived attractiveness does not appear to relate to the years of sentencing. Let’s now add the covariate Serious in. To do this in R, we simply add in the predictor to the aov model just like we would with adding a second predictor in a multiple regression - by specifying IV + covariate within the relevant function. See below for two ways of running an ANCOVA: # One way using car::Anova - this is useful for getting the correct eta squared confidence intervals options(contrasts = c(&quot;contr.helmert&quot;, &quot;contr.poly&quot;)) jury_acov &lt;- aov(Years ~ Attr + Serious, data = jury_data) jury_acov_sum &lt;- car::Anova(jury_acov, type = 3) jury_acov_sum eta_squared(jury_acov_sum, alternative = &quot;two.sided&quot;) # Another way using rstatix jury_data %&gt;% anova_test(Years ~ Attr + Serious, effect.size = &quot;pes&quot;, type = 3) What do we see? Well, the main effect of attractiveness is now significant (F(2, 110) = 3.87, p = .024, \\(\\eta^2_p\\) = .07, 95% CI = [0, .16]). The effect of the covariate is also significant (F(1, 110) = 40.31, p &lt; .001, \\(\\eta^2_p\\) = .27, 95% CI = [.14, .39]). What’s actually going on here? Well, the first model showed us that by itself, attractiveness was not a significant part of the model. However, once we factored in the effect of Seriousness (and more importantly, controlled for it) we saw that Attractive did in fact relate to the sentence length. Unsurprisingly, the seriousness of the crime also predicted sentence length. 10.2.4 Assumptions By and large, the assumptions required for an ANCOVA are the same as that of a regular ANOVA. However, there are two new ones in bold below: Normality of residuals Homogeneity of variances Linearity of the covariate Homogeneity of regression slopes Let’s check each of these below. First, the normality of residuals can simply be tested as in the usual way. Our residuals are not normally distributed in this model (W = .97, p = .006). shapiro.test(jury_acov$residuals) ## ## Shapiro-Wilk normality test ## ## data: jury_acov$residuals ## W = 0.96616, p-value = 0.005529 The homoegeneity of variance assumption is also largely tested in the same way. Our homogeneity of variance assumption also isn’t met (F(2, 111) = 5.68, p = .004)… jury_data %&gt;% levene_test(Years ~ Attr, center = &quot;mean&quot;) ## Warning in leveneTest.default(y = y, group = group, ...): group coerced to factor. 10.2.5 Linearity of the covariate A third assumption tests whether the covariate is linearly related to the DV. This assumption is essentially similar to the linearity assumption in a regression model - because we are still dealing with linear models, our covariates must also be linearly related to our outcome. This is simple enough to test just by visualising the relationship. In general, this assumption appears to hold - it looks like there’s a vague linear relationship in there. (Note that due to the data being in integers - i.e. whole numbers - I’ve used geom_jitter() in place of geom_point() to help visualise this a bit better.) jury_data %&gt;% ggplot( aes(x = Serious, y = Years) ) + geom_jitter() + labs(x = &quot;Perceived seriousness of crime&quot;, y = &quot;Sentence length (Years)&quot;) Finally, the homogeneity of regression slopes assumption specifies that for each group, the slope of the relationship between the covariate and the dependent variable are the same. To test this, we need to run an ANOVA that allows for an interaction between the predictor and covariate. jury_data %&gt;% anova_test(Years ~ Attr * Serious, effect.size = &quot;pes&quot;, type = 3) Uh oh - this isn’t good. A significant interaction suggests that the slope of the relationship between Serious and Years differs for each level of attractiveness, as indicated by the significant interaction effect (p = .03). We can see as much if we fit separate regression lines to the scatterplot above: jury_data %&gt;% ggplot( aes(x = Serious, y = Years, colour = Attr) ) + geom_jitter() + labs(x = &quot;Perceived seriousness of crime&quot;, y = &quot;Sentence length (Years)&quot;) + geom_smooth(method = lm, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; As we can clearly see, the slopes are not identical for each group. In particular, the Unattractive group has a much stronger slope between seriousness and sentence length (indicating that unattractive people basically have it harder if they’re perceived to have committed more serious crimes). Overall, given that many of our assumptions are not met - particularly the important one of homogeneity of regression slopes - this indicates that an ANCOVA isn’t a suitable model for our data. What would we do in this instance, then? We’d probably model a regression that allows for the interaction between attractiveness and seriousness. A final note on ANCOVAs: naturally, we can extend an ANCOVA model to have multiple predictors and multiple covariates. In this instance, we would need to model multi-way interactions to test all of our effects and assumptions. Below is a lightly annotated example of a two-way ANCOVA using attractiveness and type of crime as predictors, seriousness as a covariate and sentence length as an outcome. # Build two way ANCOVA jury_twoway_acov &lt;- aov(Years ~ Attr * Crime + Serious, data = jury_data) # Normality of residuals shapiro.test(jury_twoway_acov$residuals) ## ## Shapiro-Wilk normality test ## ## data: jury_twoway_acov$residuals ## W = 0.96895, p-value = 0.009416 # Homogeneity of variance jury_data %&gt;% levene_test(Years ~ Attr * Crime, center = &quot;mean&quot;) # Linearity of covariate + homogeneity of regression slopes jury_data %&gt;% ggplot( aes(x = Serious, y = Years, colour = Attr) ) + geom_jitter() + labs(x = &quot;Perceived seriousness of crime&quot;, y = &quot;Sentence length (Years)&quot;) + geom_smooth(method = lm, se = FALSE) + facet_wrap(~Crime) ## `geom_smooth()` using formula = &#39;y ~ x&#39; jury_data %&gt;% anova_test(Years ~ Attr * Crime * Serious, effect.size = &quot;pes&quot;) # Output ANCOVA Anova(jury_twoway_acov, type = 3) "],["hierarchical-regression.html", "10.3 Hierarchical regression", " 10.3 Hierarchical regression Hierarchical regression is a form of multiple regression where we test the effects of predictors in blocks. The aim of doing a hierarchical regression is generally to test theoretical predictions about the effects of specific variables, especially before/after we control for other variables. The other aim is to explore how the model changes after we add additional predictors into the model. The basic principle of a hierarchical regression is something like this: Start by defining block 1, which is our basic regression model. This is the regression we start with. Run the regression defined in block 1. Identify which variables will be entered into block 2, which is the first round of additional predictors Run a second multiple regression with all predictors in block 2. Compare block 1 with block 2 in terms of overall model fit. The choice of what variables to enter in which blocks must be guided by theory - in other words, you cannot simply add variables at random. 10.3.1 Example Let’s return to the proneness to flow example introduced in the multiple regression section. As a reminder, here are our variables: Trait anxiety: broadly, refers to people’s tendency to feel anxious Openness to experience: a personality trait that describes how likely people are to seek new experiences DFS_Total: a measure of proneness to flow. age: participant’s age. w10_flow &lt;- read_csv(here(&quot;data&quot;, &quot;week_10&quot;, &quot;w10_flow.csv&quot;)) ## Rows: 811 Columns: 6 ## ── Column specification ────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (6): id, age, GoldMSI, DFS_Total, trait_anxiety, openness ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. In the first regressions module, we simply ran everything in one go as a multiple regression. Now let’s imagine we want to run this as a hierarchical regression, with the following blocks: Block 1: GOld MSI predicting proneness to flow (DFS_Total) Block 2: Gold MSI and openness predicting proneness to flow Block 3: Gold MSI, openness and trait anxiety predicting proneness to flow The assumption tests in multiple regressions are identical for hierarchical regressions. 10.3.2 Building blocks and output Let’s start by building block 1. We can do this with lm() as per normal. I will call this flow_block1: flow_block1 &lt;- lm(DFS_Total ~ GoldMSI, data = w10_flow) To build block 2, we simply need to create a new regression model with both predictors, as if we were running this in one go: flow_block2 &lt;- lm(DFS_Total ~ GoldMSI + openness, data = w10_flow) Finally, we do the same thing for block 3: flow_block3 &lt;- lm(DFS_Total ~ GoldMSI + openness + trait_anxiety, data = w10_flow) Now let’s print the summary of each model. We can see in block 1 that Gold MSI scores significantly predict proneness to flow: summary(flow_block1) ## ## Call: ## lm(formula = DFS_Total ~ GoldMSI, data = w10_flow) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.1367 -2.4567 0.0448 2.2783 12.2886 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 16.3429 1.0848 15.06 &lt;2e-16 *** ## GoldMSI 2.9231 0.1983 14.74 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.538 on 809 degrees of freedom ## Multiple R-squared: 0.2118, Adjusted R-squared: 0.2108 ## F-statistic: 217.4 on 1 and 809 DF, p-value: &lt; 2.2e-16 In Block 2, both the Gold MSI and openness are significant predictors of flow proneness. summary(flow_block2) ## ## Call: ## lm(formula = DFS_Total ~ GoldMSI + openness, data = w10_flow) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.3099 -2.3925 0.0643 2.2613 11.6213 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.6287 1.1912 12.281 &lt; 2e-16 *** ## GoldMSI 2.7179 0.2061 13.185 &lt; 2e-16 *** ## openness 0.4818 0.1425 3.382 0.000755 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.515 on 808 degrees of freedom ## Multiple R-squared: 0.2228, Adjusted R-squared: 0.2209 ## F-statistic: 115.8 on 2 and 808 DF, p-value: &lt; 2.2e-16 Finally, in block 3 we can see that all three remain significant predictors. However, the effect of openness to experience has changed slightly (an unreliable heuristic for this is that the p-value has increased): summary(flow_block3) ## ## Call: ## lm(formula = DFS_Total ~ GoldMSI + openness + trait_anxiety, ## data = w10_flow) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.0424 -2.2409 -0.0931 2.1484 12.3474 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 21.08246 1.33150 15.834 &lt;2e-16 *** ## GoldMSI 2.66545 0.19623 13.583 &lt;2e-16 *** ## openness 0.29958 0.13700 2.187 0.0291 * ## trait_anxiety -0.10662 0.01154 -9.237 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.345 on 807 degrees of freedom ## Multiple R-squared: 0.2971, Adjusted R-squared: 0.2945 ## F-statistic: 113.7 on 3 and 807 DF, p-value: &lt; 2.2e-16 On the next page we’ll talk about model comparison in a more formal manner. However, if we wanted to write these results up we would need to talk about the results from each block. For example: A hierarchical regression was conducted to examine the effect "],["comparing-models.html", "10.4 Comparing models", " 10.4 Comparing models On the previous page, we ended up with three models relating to the flow data. That’s all well and good, and seeing how each model changed the predictors was valuable in its own right. But how do we actually… decide which model to run with? 10.4.1 Comparing \\(R^2\\) The most commonly cited method of comparing between regression models is to examine their \\(R^2\\) values, which you may recall is a measure of how much variance in the outcome is explained by the predictors. This is easy enough to do visually. You can see the \\(R^2\\) values in the output. We can extract this easily using the following code. Whenever we use summary() on an lm() model, the summary object will contain a variable for the \\(R^2\\) that we can easily pull: summary(flow_block1)$r.squared ## [1] 0.211779 summary(flow_block2)$r.squared ## [1] 0.22278 summary(flow_block3)$r.squared ## [1] 0.2971018 We can see that Block 3 has the highest \\(R^2\\) at .297, meaning that the Block 3 model explains about 29.7% of the variance in the outcome. Block 2 explains 22.3% while Block 1 explains 21.2%. Therefore, based on this alone we might say that Block 2 explains only a little bit of extra variance in flow proneness than Block 1, while Block 3 explains substantially more - therefore, we should go with Block 3. However… \\(R^2\\) will always increase with more predictors! The very fact that each additional predictor will explain more variance - even if only a tiny amount at a time - means that selecting based on \\(R^2\\) alone will naturally favour models with more predictors. This isn’t necessarily a useful thing! 10.4.2 Nested model tests This is a slightly more ‘formal’ test of whether a more complex model leads to a significant change in fit. This works by comparing nested models. Imagine model A and model B, two linear regressions fit on the same dataset. Model A has three predictors, and is the ‘full’ model of the thing we’re trying to the estimate. Model B drops one of the predictors from Model A, but keeps the other two. Model B is considered a nested model of Model A. The principle of this test is based on the idea of seeing whether a nested (reduced) model is a significantly better fit than a full model. If a nested model is a better fit, the residual sums of squares will decrease - less residuals indicate better fit. The anova() test works on this principle, but in a sort of reverse way. Because we’re testing whether a model with additional predictors is a better fit, naturally we should expect that the ‘nested’ model (in this case, our original model) will be a worse fit than the new model. In that case, a significant result indicates that the more complex model is the better fit. Nested model tests can be done with the anova() function from base R by simply giving it two model names in order. Let’s start by comparing Blocks 1 and 2: anova(flow_block1, flow_block2) And now between Blocks 2 and 3: anova(flow_block2, flow_block3) From this, we can conclude that Block 2 is a better fit to the data than Block 1 (F(1, 808) = 11.437, p &lt; .001), and also that Block 3 is again a better fit than Block 2 (F(1, 807) = 85.329, p &lt; .001). Therefore, using this method we would consider using the model in Block 3 for interpretation, as this provides a better fit of the data. This sort of lines up with what we saw with the \\(R^2\\) change (but this probably won’t always be the case). 10.4.3 Fit indices An alternative approach is to use fit indices, which are various measures that essentially indicate how well a model fits the data. Importantly, unlike \\(R^2\\) these measures penalise based on the complexity of the model - i.e. models with more predictors are penalised more due to their complexity. Two of the most widely used fit indices are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). They work similarly, but are just calculated in slightly different ways. The AIC and BIC are calculated by: \\[ AIC = 2k -2 ln(\\hat L) \\] \\[ BIC = k ln(n) - 2 ln (\\hat L) \\] \\(\\hat L\\) is called the likelihood, which is a whole thing that we won’t dive too much into. However, \\(2 ln (\\hat L)\\) - or -2LL, or minus two log likelihood - goes by the name of deviance (as in deviation). Deviance is essentially the residual sum of squares, and thus serves as a measure of model fit. R provides some really neat functions called - you guessed it - AIC() and BIC(). These will calculate the AIC and BIC values for every model name you give it. So, we can enter all of our values at once: AIC(flow_block1, flow_block2, flow_block3) BIC(flow_block1, flow_block2, flow_block3) "],["extra-technical-details-of-anova.html", "10.5 Extra: Technical details of anova()", " 10.5 Extra: Technical details of anova() 10.5.1 The default test in anova() Technical note: this test works not too unlike a regular ANOVA, except the F-test is being conducted on the residual sums of scores. From a mathematical point of view, we are essentially conducting an F-test on the change in the residual sum of squares with the following formula: \\[ F(df_{df_b - df_a}, df_a) = \\frac{MS_{comp}}{MS_{a}} = \\frac{(SS_b - SS_a)/(p_a - p_b)}{SS_a/df_a} \\] Consider two models, Model A and Model B. Imagine Model B is a nested version of Model A - i.e. it it the same model as Model A but with less predictors. In our case, imagine Model B is flow_block1 (which only had one predictor) and Model A is flow_block2 (which had two). \\(p_a\\) is the number of coefficients in Model A including the intercept, and same with \\(p_b\\). The exact process is: Calculate the difference between residual SS in the two models - this is the \\((SS_b - SS_a)\\) part of the formula above. This is just the difference in RSS between model 1 (model B) and 2 (model A), i.e. 10124.2 - 9982.9 = 141.3. Calculate the difference in df \\((p_a - p_b)\\). In flow_block1 we have one predictor and one intercept, so we have 2 terms - this is \\(p_b\\). In flow_block2 we have two predictors and one intercept, which makes \\(p_a = 3\\). Therefore, \\((p_a - p_b) = 3 - 2 = 1\\). Calculate a mean square ratio for the comparison, which is \\(MS_{comp}\\). Essentially, we divide the result in step 1 (143.1) by the result in step 2 (1). This follows the same formula for mean squarews as we have seen before: \\(MS = \\frac{SS_{comp}}{df_{comp}}\\), so \\(MS_{comp} = \\frac{141.3}{1} = 141.3.\\) While this is identical to the sum of squares value in the table above, note that this is not the same value. Calculate a mean square ratio between RSS and df for the new model. This is the \\(SS_a/df_a\\) part of the equation. \\(df_a\\) is calculated as \\(n - p_a\\), where n is the original sample size. So \\(df_a = 811 - 3 = 808\\). Note that the value for row 2 (which corresponds to Model A/flow_block2) under Res.df is 808. Note that \\(df_b\\) is the same; \\(n - p_b = 811 - 2 = 809\\). Same deal as above after that, except this time we use the values from the new model only, i.e. residual SS for model A (flow_block2) and the residual df. \\(MS_a = \\frac{SS_{a}}{df_{a}}\\) \\(MS_a = \\frac{9982.9}{808} = 12.33507\\) Calculate an F ratio between the MS of the comparison and the MS of the new model to calculate a value for F. This is exactly the same formula as it would be for a regular ANOVA, just that now we are doing: \\[ F = \\frac{MS_{comp}}{MS_{a}} \\] \\(F = \\frac{141.3}{12.33507} = 11.45514\\) Calculate a p-value for this F-statistic by comparing the p against an F distribution. The two dfs in the original formula are a) \\(df_b - df_a\\) and b) \\(df_a\\). Which means: \\(df_b\\) is 809 - \\(df_a\\) is 808 = 1 \\(df_a\\) is 808 So we end up with a test statistic of \\(F(1, 808) = 11.455\\). We can use this to calculate a p-value by calculating the probability of getting a value of at least 11.455, on an F distribution with degrees of freedom parameters described above. We can visualise this below. Note that because the values for F are so infinitesimally small with these parameters and at this F-value, I’ve zoomed in the plot to visualise the highlighted area: tibble( x = seq(0, 15, by = .5), y = df(x, df1 = 1, df2 = 809) ) %&gt;% ggplot( aes(x = x, y = y) ) + geom_line(linewidth = 1) + theme_pubr() + geom_vline(xintercept = 11.455, linewidth = 1, colour = &quot;royalblue&quot;) + annotate(&quot;text&quot;, x = 12, y = 0.0024, label = &quot;F = 11.455&quot;) + stat_function(fun = df, args = list(df1 = 1, df2 = 809), geom = &quot;area&quot;, xlim = c(11.455, 15), fill = &quot;royalblue&quot;, alpha = 0.5) + scale_x_continuous(expand = c(0, 0), limits = c(10, NA)) + scale_y_continuous(expand = c(0, 0), limits = c(0, 0.003)) + labs(x = &quot;F-value&quot;, y = &quot;Density&quot;) ## Warning: Computation failed in `stat_function()`. ## Caused by error in `fun()`: ## ! could not find function &quot;fun&quot; ## Warning: Removed 20 rows containing missing values or values outside the scale range (`geom_line()`). R can manually calculate a p-value with the pf() function. pf() will calculate the probability of a value on the F distribution, given the two degrees of freedom parameters to characterise the distribution. lower.tail = FALSE is used to indicate that we want to calculate the probability of getting something above our critical F-value; lower.tail = TRUE would calculuate the probability below it. pf(11.45514, df1 = 1, df2 = 808, lower.tail = FALSE) ## [1] 0.0007473355 Note that our p-value isn’t exactly the same as the value in the table - this is because we’ve used rounded values. The code below extracts the unrounded values and uses them in the calculations. As you can see we get the exact p-value in the table. x &lt;- anova(flow_block1, flow_block2) x # Using the output from anova() to manually calculate p-value SSb &lt;- x$RSS[1] SSa &lt;- x$RSS[2] pa &lt;- length(coef(flow_block2)) pb &lt;- length(coef(flow_block1)) dfa &lt;- nrow(w10_flow) - pa dfb &lt;- nrow(w10_flow) - pb f_val &lt;- ((SSb-SSa)/(pa-pb))/(SSa/dfa) pf(f_val, df1 = dfb-dfa, df2 = dfa, lower.tail = FALSE) ## [1] 0.0007546911 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
