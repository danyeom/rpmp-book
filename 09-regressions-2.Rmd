# Multiple regression continued

```{r warning = FALSE, message = FALSE, echo = FALSE}
library(tidyverse)
library(ggpubr)
library(here)
library(rstatix)
library(effectsize)
library(car)
```

This section deals with some more advanced topics in ANOVAs and regression. It serves as a continuation to the first chapter on regression, and in particular focuses on multiple regressions. This chapter will cover the following:

-   ANCOVA
-   Hierarchical regressions
-   Model selection

This module won't cover continuous interactions because they are often considered under the topic of moderation - for which there will be a separate module. 

## Revisiting multiple regression

Recall that the basic multiple regression looks something like this:

$$
y = \beta_0 + \beta_1x_1 +\beta_2x_2 + \epsilon_i
$$
As a reminder, the coefficients in this formula correspond to the following:

- $\beta_1$ is the coefficient for predictor $x_1$; i.e. as $x_1$ increases by 1 unit, $\hat y$ (the predicted y value) increases by $\beta_1$ units, *assuming* $x_2$ does not change
- $\beta_2$ is the coefficient for predictor $x_2$, and describes how $\hat y$ changes assuming $x_1$ does not change
- $\epsilon_i$ is the error term, which we assume is normally distributed

We can expand this formula out to include $n$ predictors, as follows:

$$
y = \beta_0 + \beta_1x_1 +\beta_2x_2 + ... \beta_nx_n + \epsilon_i
$$



## ANCOVAs

### Introduction

ANCOVA stands for Analysis of **Co-**variance. Its basic definition is that it is an ANOVA, but with the inclusion of a covariate (or a variable we need to control for). The basic idea is that we are performing an ANOVA between our predictors and outcomes after adjusting our predictors (our model) by another variable.

Just like the ANOVA, ANCOVA is a fairly generic term that can refer to a multitude of analyses. In this book we'll stick with ANOVAs relating to between-subjects predictors only.

### Example

The following data comes from Plaster (1989). The dataset is described as below:

>Male participants were shown a picture of one of three young women. Pilot work had indicated that the one woman was beautiful, another of average physical attractiveness, and the third unattractive. Participants rated the woman they saw on each of twelve attributes. These measures were used to check on the manipulation by the photo.
>Then the participants were told that the person in the photo had committed a Crime, and asked to rate the seriousness of the crime and recommend a prison sentence, in Years.

Our main questions are:

1. Does the perceived attractiveness of the "defendant" (the women in the photo) influence the number of years the mock juror (the participant) sentence them for?
2. How does this relationship change after controlling for the perceived seriousness of the crime?

Our dataset, `jury_data`, contains the following variables:

-   Attr: The perceived attractiveness of the defendant (Beautiful, Average, Unattractive)
-   Crime: The crime that was commited by the defendant (Burglary, Swindle)
-   Serious: The perceived seriousness of the crime from 1-10
-   Years: The number of years the participant sentenced the defendant for

### One-way ANCOVA

To start us off, let's begin with a simple one-way ANOVA between attractiveness and years. We can see this below.

```{r message = FALSE}
jury_data <- read_csv(here("data", "regression_2", "anova_mockjury.csv"))
```


```{r}
jury_aov <- aov(Years ~ Attr, data = jury_data)
summary(jury_aov)
eta_squared(jury_aov, alternative = "two.sided")
```
From this we can infer that the effect of attractiveness is not significant (*F*(2, 111) = 2.77, *p* = .067, $\eta^2$ = .05, 95% CI = [0, .14]). In other words, perceived attractiveness does not appear to relate to the years of sentencing.

Let's now add the covariate `Serious` in. To do this in R, we simply add in the predictor to the `aov` model just like we would with adding a second predictor in a multiple regression - by specifying `IV + covariate` within the relevant function. See below for two ways of running an ANCOVA:

```{r}
# One way using car::Anova - this is useful for getting the correct eta squared confidence intervals

options(contrasts = c("contr.helmert", "contr.poly"))

jury_acov <- aov(Years ~ Attr + Serious, data = jury_data)
jury_acov_sum <- car::Anova(jury_acov, type = 3)
jury_acov_sum

eta_squared(jury_acov_sum, alternative = "two.sided")

# Another way using rstatix
jury_data %>%
  anova_test(Years ~ Attr + Serious, effect.size = "pes", type = 3)
```

What do we see? Well, the main effect of attractiveness is now significant (*F*(2, 110) = 3.87, *p* = .024, $\eta^2_p$ = .07, 95% CI = [0, .16]). The effect of the covariate is also significant (*F*(1, 110) = 40.31, *p* < .001, $\eta^2_p$ = .27, 95% CI = [.14, .39]).

What's actually going on here? Well, the first model showed us that by itself, attractiveness was not a significant part of the model. However, once we factored in the effect of Seriousness (and more importantly, controlled for it) we saw that Attractive *did* in fact relate to the sentence length. Unsurprisingly, the seriousness of the crime also predicted sentence length.

### Assumptions

By and large, the assumptions required for an ANCOVA are the same as that of a regular ANOVA. However, there are two new ones in bold below:

-   Normality of residuals
-   Homogeneity of variances
-   **Linearity of the covariate**
-   **Homogeneity of regression slopes**

Let's check each of these below.

First, the normality of residuals can simply be tested as in the usual way. Our residuals are not normally distributed in this model (*W* = .97, *p* = .006).

```{r}
shapiro.test(jury_acov$residuals)
```
The homoegeneity of variance assumption is also largely tested in the same way. Our homogeneity of variance assumption also isn't met (*F*(2, 111) = 5.68, *p* = .004)...

```{r}
jury_data %>%
  levene_test(Years ~ Attr, center = "mean")
```

### Linearity of the covariate

A third assumption tests whether the covariate is linearly related to the DV. This assumption is essentially similar to the linearity assumption in a regression model - because we are still dealing with linear models, our covariates must also be linearly related to our outcome.

This is simple enough to test just by visualising the relationship. In general, this assumption appears to hold - it looks like there's a vague linear relationship in there. 

(Note that due to the data being in integers - i.e. whole numbers - I've used `geom_jitter()` in place of `geom_point()` to help visualise this a bit better.)

```{r fig.align = "center"}
jury_data %>%
  ggplot(
    aes(x = Serious, y = Years)
  ) + 
  geom_jitter() + 
  labs(x = "Perceived seriousness of crime", y = "Sentence length (Years)")
```

Finally, the **homogeneity of regression slopes** assumption specifies that for each group, the slope of the relationship between the covariate and the dependent variable are the same. To test this, we need to run an ANOVA that allows for an interaction between the predictor and covariate.

```{r}
jury_data %>%
  anova_test(Years ~ Attr * Serious, effect.size = "pes", type = 3)
```
Uh oh - this isn't good. A significant interaction suggests that the slope of the relationship between `Serious` and `Years` differs for each level of attractiveness, as indicated by the significant interaction effect (*p* = .03). We can see as much if we fit separate regression lines to the scatterplot above:

```{r fig.align = "center"}
jury_data %>%
  ggplot(
    aes(x = Serious, y = Years, colour = Attr)
  ) + 
  geom_jitter() + 
  labs(x = "Perceived seriousness of crime", y = "Sentence length (Years)") +
  geom_smooth(method = lm, se = FALSE)
```
As we can clearly see, the slopes are not identical for each group. In particular, the Unattractive group has a much stronger slope between seriousness and sentence length (indicating that unattractive people basically have it harder if they're perceived to have committed more serious crimes). 

Overall, given that many of our assumptions are not met - particularly the important one of homogeneity of regression slopes - this indicates that an ANCOVA isn't a suitable model for our data. What would we do in this instance, then? We'd probably model a regression that allows for the interaction between attractiveness and seriousness. 

A final note on ANCOVAs: naturally, we can extend an ANCOVA model to have multiple predictors *and* multiple covariates. In this instance, we would need to model multi-way interactions to test all of our effects and assumptions. Below is a lightly annotated example  of a two-way ANCOVA using attractiveness and type of crime as predictors, seriousness as a covariate and sentence length as an outcome.

```{r}
# Build two way ANCOVA
jury_twoway_acov <- aov(Years ~ Attr * Crime + Serious, data = jury_data)

# Normality of residuals
shapiro.test(jury_twoway_acov$residuals)

# Homogeneity of variance
jury_data %>%
  levene_test(Years ~ Attr * Crime, center = "mean")

# Linearity of covariate + homogeneity of regression slopes

jury_data %>%
  ggplot(
    aes(x = Serious, y = Years, colour = Attr)
  ) +
  geom_jitter() + 
  labs(x = "Perceived seriousness of crime", y = "Sentence length (Years)") +
  geom_smooth(method = lm, se = FALSE) +
  facet_wrap(~Crime)
  
jury_data %>%
  anova_test(Years ~ Attr * Crime * Serious, effect.size = "pes")

# Output ANCOVA
Anova(jury_twoway_acov, type = 3)
```


## Hierarchical regression

Hierarchical regression is a form of multiple regression where we test the effects of predictors in **blocks.** The aim of doing a hierarchical regression is generally to test theoretical predictions about the effects of specific variables, especially before/after we control for other variables. The other aim is to explore how the *model* changes after we add additional predictors into the model. 

The basic principle of a hierarchical regression is something like this:

1.    Start by defining block 1, which is our basic regression model. This is the regression we start with. Run the regression defined in block 1.
2.    Identify which variables will be entered into block 2, which is the first round of additional predictors
3.    Run a second multiple regression with all predictors in block 2.
4.    Compare block 1 with block 2 in terms of overall model fit.

The choice of what variables to enter in which blocks must be guided by theory - in other words, you cannot simply add variables at random. 

### Example

Let's return to the proneness to flow example introduced in the multiple regression section. As a reminder, here are our variables:

-    Trait anxiety: broadly, refers to people's tendency to feel anxious
-    Openness to experience: a personality trait that describes how likely people are to seek new experiences 
-    DFS_Total: a measure of proneness to flow.
-   age: participant's age.

```{r}
w10_flow <- read_csv(here("data", "week_10", "w10_flow.csv"))
```

In the first regressions module, we simply ran everything in one go as a multiple regression. Now let's imagine we want to run this as a hierarchical regression, with the following blocks:

-   Block 1: GOld MSI predicting proneness to flow (DFS_Total)
-   Block 2: Gold MSI and openness predicting proneness to flow
-   Block 3: Gold MSI, openness and trait anxiety predicting proneness to flow


The assumption tests in multiple regressions are identical for hierarchical regressions. 

### Building blocks and output

Let's start by building block 1. We can do this with `lm()` as per normal. I will call this `flow_block1`:

```{r}
flow_block1 <- lm(DFS_Total ~ GoldMSI, data = w10_flow)
```

To build block 2, we simply need to create a new regression model with both predictors, as if we were running this in one go:

```{r}
flow_block2 <- lm(DFS_Total ~ GoldMSI + openness, data = w10_flow)
```

Finally, we do the same thing for block 3:

```{r}
flow_block3 <- lm(DFS_Total ~ GoldMSI + openness + trait_anxiety, data = w10_flow)
```

Now let's print the summary of each model. We can see in block 1 that Gold MSI scores significantly predict proneness to flow:

```{r}
summary(flow_block1)
```

In Block 2, both the Gold MSI and openness are significant predictors of flow proneness. 

```{r}
summary(flow_block2)
```

Finally, in block 3 we can see that all three remain significant predictors. However, the effect of openness to experience has changed slightly (an unreliable heuristic for this is that the p-value has increased):

```{r}
summary(flow_block3)
```

On the next page we'll talk about model comparison in a more formal manner. However, if we wanted to write these results up we would need to talk about the results from each block. For example:

A hierarchical regression was conducted to examine the effect 


## Comparing models

On the previous page, we ended up with three models relating to the flow data. That's all well and good, and seeing how each model changed the predictors was valuable in its own right. But how do we actually... decide which model to run with? 

### Comparing $R^2$

The most commonly cited method of comparing between regression models is to examine their $R^2$ values, which you may recall is a measure of how much variance in the outcome is explained by the predictors.

This is easy enough to do visually. You can see the $R^2$ values in the output. We can extract this easily using the following code. Whenever we use `summary()` on an `lm()` model, the summary object will contain a variable for the $R^2$ that we can easily pull:

```{r}
summary(flow_block1)$r.squared
summary(flow_block2)$r.squared
summary(flow_block3)$r.squared
```
We can see that Block 3 has the highest $R^2$ at .297, meaning that the Block 3 model explains about 29.7% of the variance in the outcome. Block 2 explains 22.3% while Block 1 explains 21.2%. Therefore, based on this alone we might say that Block 2 explains only a little bit of extra variance in flow proneness than Block 1, while Block 3 explains substantially more - therefore, we should go with Block 3. However... $R^2$ will *always* increase with more predictors! The very fact that each additional predictor will explain more variance - even if only a tiny amount at a time - means that selecting based on $R^2$ alone will naturally favour models with more predictors. This isn't necessarily a useful thing!

### Nested model tests

This is a slightly more 'formal' test of whether a more complex model leads to a significant change in fit. This works by comparing **nested** models. Imagine model A and model B, two linear regressions fit on the same dataset. Model A has three predictors, and is the 'full' model of the thing we're trying to the estimate. Model B drops one of the predictors from Model A, but keeps the other two. Model B is considered a *nested* model of Model A.

The principle of this test is based on the idea of seeing whether a nested (reduced) model is a significantly better fit than a full model. If a nested model is a better fit, the residual sums of squares will *decrease* - less residuals indicate better fit. The `anova()` test works on this principle, but in a sort of reverse way. Because we're testing whether a model with *additional* predictors is a better fit, naturally we should expect that the 'nested' model (in this case, our original model) will be a *worse* fit than the new model. In that case, a significant result indicates that the more complex model is the better fit.

Nested model tests can be done with the `anova()` function from base R by simply giving it two model names in order. Let's start by comparing Blocks 1 and 2:

```{r}
anova(flow_block1, flow_block2)
```

And now between Blocks 2 and 3:

```{r}
anova(flow_block2, flow_block3)
```

From this, we can conclude that Block 2 is a better fit to the data than Block 1 (F(1, 808) = 11.437, *p* < .001), and also that Block 3 is again a better fit than Block 2 (F(1, 807) = 85.329, *p* < .001). Therefore, using this method we would consider using the model in Block 3 for interpretation, as this provides a better fit of the data. This sort of lines up with what we saw with the $R^2$ change (but this probably won't always be the case).


### Fit indices

An alternative approach is to use **fit indices**, which are various measures that essentially indicate how well a model fits the data. Importantly, unlike $R^2$ these measures penalise based on the complexity of the model - i.e. models with more predictors are penalised more due to their complexity. 

Two of the most widely used fit indices are the **Akaike Information Criterion (AIC)** and the **Bayesian Information Criterion (BIC)**. They work similarly, but are just calculated in slightly different ways. 

The AIC and BIC are calculated by:

$$
AIC = 2k -2 ln(\hat L)
$$

$$
BIC = k ln(n) - 2 ln (\hat L)
$$

$\hat L$ is called the **likelihood**, which is a whole thing that we won't dive too much into. However, $2 ln (\hat L)$ - or -2LL, or minus two log likelihood - goes by the name of **deviance** (as in deviation). Deviance is essentially the residual sum of squares, and thus serves as a measure of model fit.

R provides some really neat functions called - you guessed it - `AIC()` and `BIC()`. These will calculate the AIC and BIC values for every model name you give it. So, we can enter all of our values at once:

```{r}
AIC(flow_block1, flow_block2, flow_block3)
BIC(flow_block1, flow_block2, flow_block3)
```



