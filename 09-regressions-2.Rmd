# Multiple regression continued

```{r warning = FALSE, message = FALSE, echo = FALSE}
library(tidyverse)
library(ggpubr)
library(here)
library(rstatix)

# i_am("code/bookdown/09-regressions-2.Rmd")
```

This section deals with some more advanced topics in ANOVAs and regression. It serves as a continuation to the first chapter on regression, and in particular focuses on multiple regressions. This chapter will cover the following:

-   ANCOVA
-   MANOVA
-   Hierarchical regressions
-   Model selection

This module won't cover continuous interactions because they are often considered under the topic of moderation - for which there will be a separate module. 

## Revisiting multiple regression

Recall that the basic multiple regression looks something like this:

$$
y = \beta_0 + \beta_1x_1 +\beta_2x_2 + \epsilon_i
$$
As a reminder, the coefficients in this formula correspond to the following:

- $\beta_1$ is the coefficient for predictor $x_1$; i.e. as $x_1$ increases by 1 unit, $\hat y$ (the predicted y value) increases by $\beta_1$ units, *assuming* $x_2$ does not change
- $\beta_2$ is the coefficient for predictor $x_2$, and describes how $\hat y$ changes assuming $x_1$ does not change
- $\epsilon_i$ is the error term, which we assume is normally distributed

We can expand this formula out to include $n$ predictors, as follows:

$$
y = \beta_0 + \beta_1x_1 +\beta_2x_2 + ... \beta_nx_n + \epsilon_i
$$



## ANCOVAs

ANCOVA stands for Analysis of **Co-**variance. Its basic definition is that it is an ANOVA, but with the inclusion of a covariate (or a variable we need to control for). The basic idea is that we are performing an ANOVA between our predictors and outcomes after adjusting our predictors (our model) by another variable.

Just like the ANOVA, ANCOVA is a fairly generic term that can refer to a multitude of analyses. In this book we'll stick with ANOVAs relating to between-subjects predictors only.

## Hierarchical regression

Hierarchical regression is a form of multiple regression where we test the effects of predictors in **blocks.** The aim of doing a hierarchical regression is generally to test theoretical predictions about the effects of specific variables, especially before/after we control for other variables. The other aim is to explore how the *model* changes after we add additional predictors into the model. 

The basic principle of a hierarchical regression is something like this:

1.    Start by defining block 1, which is our basic regression model. This is the regression we start with. Run the regression defined in block 1.
2.    Identify which variables will be entered into block 2, which is the first round of additional predictors
3.    Run a second multiple regression with all predictors in block 2.
4.    Compare block 1 with block 2 in terms of overall model fit.

The choice of what variables to enter in which blocks must be guided by theory - in other words, you cannot simply add variables at random. 

### Example

Let's return to the proneness to flow example introduced in the multiple regression section. As a reminder, here are our variables:

-    Trait anxiety: broadly, refers to people's tendency to feel anxious
-    Openness to experience: a personality trait that describes how likely people are to seek new experiences 
-    DFS_Total: a measure of proneness to flow.
-   age: participant's age.

```{r}
w10_flow <- read_csv(here("data", "week_10", "w10_flow.csv"))
```

In the first regressions module, we simply ran everything in one go as a multiple regression. Now let's imagine we want to run this as a hierarchical regression, with the following blocks:

-   Block 1: GOld MSI predicting proneness to flow (DFS_Total)
-   Block 2: Gold MSI and openness predicting proneness to flow
-   Block 3: Gold MSI, openness and trait anxiety predicting proneness to flow


The assumption tests in multiple regressions are identical for hierarchical regressions. 

### Building blocks and output

Let's start by building block 1. We can do this with `lm()` as per normal. I will call this `flow_block1`:

```{r}
flow_block1 <- lm(DFS_Total ~ GoldMSI, data = w10_flow)
```

To build block 2, we simply need to create a new regression model with both predictors, as if we were running this in one go:

```{r}
flow_block2 <- lm(DFS_Total ~ GoldMSI + openness, data = w10_flow)
```

Finally, we do the same thing for block 3:

```{r}
flow_block3 <- lm(DFS_Total ~ GoldMSI + openness + trait_anxiety, data = w10_flow)
```

Now let's print the summary of each model. We can see in block 1 that Gold MSI scores significantly predict proneness to flow:

```{r}
summary(flow_block1)
```

In Block 2, both the Gold MSI and openness are significant predictors of flow proneness. 

```{r}
summary(flow_block2)
```

Finally, in block 3 we can see that all three remain significant predictors. However, the effect of openness to experience has changed slightly (an unreliable heuristic for this is that the p-value has increased):

```{r}
summary(flow_block3)
```

On the next page we'll talk about model comparison in a more formal manner. However, if we wanted to write these results up we would need to talk about the results from each block. For example:

A hierarchical regression was conducted to examine the effect 


## Comparing models

On the previous page, we ended up with three models relating to the flow data. That's all well and good, and seeing how each model changed the predictors was valuable in its own right. But how do we actually... decide which model to run with? 

### Comparing $R^2$

The most commonly cited method of comparing between regression models is to examine their $R^2$ values, which you may recall is a measure of how much variance in the outcome is explained by the predictors.

This is easy enough to do visually. You can see the $R^2$ values in the output. We can extract this easily using the following code. Whenever we use `summary()` on an `lm()` model, the summary object will contain a variable for the $R^2$ that we can easily pull:

```{r}
summary(flow_block1)$r.squared
summary(flow_block2)$r.squared
summary(flow_block3)$r.squared
```
We can see that Block 3 has the highest $R^2$ at .297, meaning that the Block 3 model explains about 29.7% of the variance in the outcome. Block 2 explains 22.3% while Block 1 explains 21.2%. Therefore, based on this alone we might say that Block 2 explains only a little bit of extra variance in flow proneness than Block 1, while Block 3 explains substantially more - therefore, we should go with Block 3. However... $R^2$ will *always* increase with more predictors! The very fact that each additional predictor will explain more variance - even if only a tiny amount at a time - means that selecting based on $R^2$ alone will naturally favour models with more predictors. This isn't necessarily a useful thing!

### Nested model tests

This is a slightly more 'formal' test of whether a more complex model leads to a significant change in fit. This works by comparing **nested** models. Imagine model A and model B, two linear regressions fit on the same dataset. Model A has three predictors, and is the 'full' model of the thing we're trying to the estimate. Model B drops one of the predictors from Model A, but keeps the other two. Model B is considered a *nested* model of Model A.

The principle of this test is based on the idea of seeing whether a nested (reduced) model is a significantly better fit than a full model. If a nested model is a better fit, the residual sums of squares will *decrease* - less residuals indicate better fit. The `anova()` test works on this principle, but in a sort of reverse way. Because we're testing whether a model with *additional* predictors is a better fit, naturally we should expect that the 'nested' model (in this case, our original model) will be a *worse* fit than the new model. In that case, a significant result indicates that the more complex model is the better fit.

Nested model tests can be done with the `anova()` function from base R by simply giving it two model names in order. Let's start by comparing Blocks 1 and 2:

```{r}
anova(flow_block1, flow_block2)
```

And now between Blocks 2 and 3:

```{r}
anova(flow_block2, flow_block3)
```

From this, we can conclude that Block 2 is a better fit to the data than Block 1 (F(1, 808) = 11.437, *p* < .001), and also that Block 3 is again a better fit than Block 2 (F(1, 807) = 85.329, *p* < .001). Therefore, using this method we would consider using the model in Block 3 for interpretation, as this provides a better fit of the data. This sort of lines up with what we saw with the $R^2$ change (but this probably won't always be the case).


### Fit indices

An alternative approach is to use **fit indices**, which are various measures that essentially indicate how well a model fits the data. Importantly, unlike $R^2$ these measures penalise based on the complexity of the model - i.e. models with more predictors are penalised more due to their complexity. 

Two of the most widely used fit indices are the **Akaike Information Criterion (AIC)** and the **Bayesian Information Criterion (BIC)**. They work similarly, but are just calculated in slightly different ways. 

The AIC and BIC are calculated by:

$$
AIC = 2k -2 ln(\hat L)
$$

$$
BIC = k ln(n) - 2 ln (\hat L)
$$

$\hat L$ is called the **likelihood**, which is a whole thing that we won't dive too much into. However, $2 ln (\hat L)$ - or -2LL, or minus two log likelihood - goes by the name of **deviance** (as in deviation). Deviance is essentially the residual sum of squares, and thus serves as a measure of model fit.

R provides some really neat functions called - you guessed it - `AIC()` and `BIC()`. These will calculate the AIC and BIC values for every model name you give it. So, we can enter all of our values at once:

```{r}
AIC(flow_block1, flow_block2, flow_block3)
BIC(flow_block1, flow_block2, flow_block3)
```





## Extra: Technical details of `anova()`

### The default test in `anova()`

Technical note: this test works not too unlike a regular ANOVA, except the F-test is being conducted on the residual sums of scores. From a mathematical point of view, we are essentially conducting an F-test on the change in the residual sum of squares with the following formula:

$$
F(df_{df_b - df_a}, df_a) = \frac{MS_{comp}}{MS_{a}} = \frac{(SS_b - SS_a)/(p_a - p_b)}{SS_a/df_a}
$$

Consider two models, Model A and Model B. Imagine Model B is a nested version of Model A - i.e. it it the same model as Model A but with less predictors. In our case, imagine Model B is `flow_block1` (which only had one predictor) and Model A is `flow_block2` (which had two). $p_a$ is the number of coefficients in Model A *including* the intercept, and same with $p_b$. 

The exact process is:

1.    Calculate the difference between residual SS in the two models - this is the $(SS_b - SS_a)$ part of the formula above. This is just the difference in RSS between model 1 (model B) and 2 (model A), i.e. 10124.2 - 9982.9 = 141.3.

2.    Calculate the difference in df $(p_a - p_b)$. In `flow_block1` we have one predictor and one intercept, so we have 2 terms - this is $p_b$. In `flow_block2` we have two predictors and one intercept, which makes $p_a = 3$. Therefore, $(p_a - p_b) = 3 - 2 = 1$.

3.    Calculate a mean square ratio for the comparison, which is $MS_{comp}$. Essentially, we divide the result in step 1 (143.1) by the result in step 2 (1). This follows the same formula for mean squarews as we have seen before: $MS = \frac{SS_{comp}}{df_{comp}}$, so 
$MS_{comp} = \frac{141.3}{1} = 141.3.$ While this is identical to the sum of squares value in the table above, note that this is *not* the same value.

4.    Calculate a mean square ratio between RSS and df for the new model. This is the $SS_a/df_a$ part of the equation. $df_a$ is calculated as $n - p_a$, where n is the original sample size. So $df_a = 811 - 3 = 808$. Note that the value for row 2 (which corresponds to Model A/`flow_block2`) under `Res.df` is 808. 

Note that $df_b$ is the same; $n - p_b = 811 - 2 = 809$.

Same deal as above after that, except this time we use the values from the new model only, i.e. residual SS for model A (`flow_block2`) and the residual df. 

$MS_a = \frac{SS_{a}}{df_{a}}$
$MS_a = \frac{9982.9}{808} = 12.33507$

5.    Calculate an F ratio between the MS of the comparison and the MS of the new model to calculate a value for F.

This is exactly the same formula as it would be for a regular ANOVA, just that now we are doing:

$$
F = \frac{MS_{comp}}{MS_{a}}
$$

$F = \frac{141.3}{12.33507} = 11.45514$ 

6. Calculate a p-value for this F-statistic by comparing the p against an F distribution. The two dfs in the original formula are a) $df_b - df_a$ and b) $df_a$. Which means:

-   $df_b$ is 809 - $df_a$ is 808 = 1
-   $df_a$ is 808

So we end up with a test statistic of $F(1, 808) = 11.455$. We can use this to calculate a p-value by calculating the probability of getting a value of at least 11.455, on an F distribution with degrees of freedom parameters described above. We can visualise this below. Note that because the values for F are so infinitesimally small with these parameters and at this F-value, I've zoomed in the plot to visualise the highlighted area:

```{r}
tibble(
  x = seq(0, 15, by = .5),
  y = df(x, df1 = 1, df2 = 809)
) %>%
  ggplot(
    aes(x = x, y = y)
  ) +
  geom_line(linewidth = 1) +
  theme_pubr() +
  geom_vline(xintercept = 11.455, linewidth = 1, colour = "royalblue") +
  annotate("text", x = 12, y = 0.0024, label = "F = 11.455") +
  stat_function(fun = df, args = list(df1 = 1, df2 = 809), 
                geom = "area", xlim = c(11.455, 15),
                fill = "royalblue", alpha = 0.5) + 
  scale_x_continuous(expand = c(0, 0), limits = c(10, NA)) + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, 0.003)) +
  labs(x = "F-value", y = "Density")

```

R can manually calculate a p-value with the `pf()` function. `pf()` will calculate the probability of a value on the F distribution, given the two degrees of freedom parameters to characterise the distribution. `lower.tail = FALSE` is used to indicate that we want to calculate the probability of getting something *above* our critical F-value; `lower.tail = TRUE` would calculuate the probability *below* it.

```{r}
pf(11.45514, df1 = 1, df2 = 808, lower.tail = FALSE)
```


Note that our p-value isn't exactly the same as the value in the table - this is because we've used rounded values. The code below extracts the unrounded values and uses them in the calculations. As you can see we get the exact p-value in the table.

```{r}
x <- anova(flow_block1, flow_block2)
x

# Using the output from anova() to manually calculate p-value
SSb <- x$RSS[1]
SSa <- x$RSS[2]
pa <- length(coef(flow_block2))
pb <- length(coef(flow_block1))
dfa <- nrow(w10_flow) - pa
dfb <- nrow(w10_flow) - pb

f_val <- ((SSb-SSa)/(pa-pb))/(SSa/dfa)

pf(f_val, df1 = dfb-dfa, df2 = dfa, lower.tail = FALSE)

```
