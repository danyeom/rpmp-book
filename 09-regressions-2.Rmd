# Multiple regression continued

```{r warning = FALSE, message = FALSE, echo = FALSE}
library(tidyverse)
library(ggpubr)
library(here)
library(rstatix)
library(effectsize)
library(car)
```

This section deals with some more advanced topics in ANOVAs and regression. It serves as a continuation to the first chapter on regression, and in particular focuses on multiple regressions. This chapter will cover the following:

-   ANCOVA
-   Hierarchical regressions
-   Model selection
-   Identifying and handling outliers

This module won't cover continuous interactions because they are often considered under the topic of moderation - for which there will be a separate module. 

## Revisiting multiple regression

Recall that the basic multiple regression looks something like this:

$$
y = \beta_0 + \beta_1x_1 +\beta_2x_2 + \epsilon_i
$$
As a reminder, the coefficients in this formula correspond to the following:

- $\beta_1$ is the coefficient for predictor $x_1$; i.e. as $x_1$ increases by 1 unit, $\hat y$ (the predicted y value) increases by $\beta_1$ units, *assuming* $x_2$ does not change
- $\beta_2$ is the coefficient for predictor $x_2$, and describes how $\hat y$ changes assuming $x_1$ does not change
- $\epsilon_i$ is the error term, which we assume is normally distributed

We can expand this formula out to include $n$ predictors, as follows:

$$
y = \beta_0 + \beta_1x_1 +\beta_2x_2 + ... \beta_nx_n + \epsilon_i
$$



## ANCOVAs

### Introduction

ANCOVA stands for Analysis of **Co-**variance. Its basic definition is that it is an ANOVA, but with the inclusion of a covariate (or a variable we need to control for). The basic idea is that we are performing an ANOVA between our predictors and outcomes after adjusting our predictors (our model) by another variable.

Just like the ANOVA, ANCOVA is a fairly generic term that can refer to a multitude of analyses. In this book we'll stick with ANOVAs relating to between-subjects predictors only.

### Example

The following data comes from Plaster (1989). The dataset is described as below:

>Male participants were shown a picture of one of three young women. Pilot work had indicated that the one woman was beautiful, another of average physical attractiveness, and the third unattractive. Participants rated the woman they saw on each of twelve attributes. These measures were used to check on the manipulation by the photo.
>Then the participants were told that the person in the photo had committed a Crime, and asked to rate the seriousness of the crime and recommend a prison sentence, in Years.

Our main questions are:

1. Does the perceived attractiveness of the "defendant" (the women in the photo) influence the number of years the mock juror (the participant) sentence them for?
2. How does this relationship change after controlling for the perceived seriousness of the crime?

Our dataset, `jury_data`, contains the following variables:

-   Attr: The perceived attractiveness of the defendant (Beautiful, Average, Unattractive)
-   Crime: The crime that was commited by the defendant (Burglary, Swindle)
-   Serious: The perceived seriousness of the crime from 1-10
-   Years: The number of years the participant sentenced the defendant for

### One-way ANCOVA

To start us off, let's begin with a simple one-way ANOVA between attractiveness and years. We can see this below.

```{r message = FALSE}
jury_data <- read_csv(here("data", "regression_2", "anova_mockjury.csv"))
```


```{r}
jury_aov <- aov(Years ~ Attr, data = jury_data)
summary(jury_aov)
eta_squared(jury_aov, alternative = "two.sided")
```
From this we can infer that the effect of attractiveness is not significant (*F*(2, 111) = 2.77, *p* = .067, $\eta^2$ = .05, 95% CI = [0, .14]). In other words, perceived attractiveness does not appear to relate to the years of sentencing.

Let's now add the covariate `Serious` in. To do this in R, we simply add in the predictor to the `aov` model just like we would with adding a second predictor in a multiple regression - by specifying `IV + covariate` within the relevant function. See below for two ways of running an ANCOVA:

```{r}
# One way using car::Anova - this is useful for getting the correct eta squared confidence intervals

options(contrasts = c("contr.helmert", "contr.poly"))

jury_acov <- aov(Years ~ Attr + Serious, data = jury_data)
jury_acov_sum <- car::Anova(jury_acov, type = 3)
jury_acov_sum

eta_squared(jury_acov_sum, alternative = "two.sided")

# Another way using rstatix
jury_data %>%
  anova_test(Years ~ Attr + Serious, effect.size = "pes", type = 3)
```

What do we see? Well, the main effect of attractiveness is now significant (*F*(2, 110) = 3.87, *p* = .024, $\eta^2_p$ = .07, 95% CI = [0, .16]). The effect of the covariate is also significant (*F*(1, 110) = 40.31, *p* < .001, $\eta^2_p$ = .27, 95% CI = [.14, .39]).

What's actually going on here? Well, the first model showed us that by itself, attractiveness was not a significant part of the model. However, once we factored in the effect of Seriousness (and more importantly, controlled for it) we saw that Attractive *did* in fact relate to the sentence length. Unsurprisingly, the seriousness of the crime also predicted sentence length.

### Assumptions

By and large, the assumptions required for an ANCOVA are the same as that of a regular ANOVA. However, there are two new ones in bold below:

-   Normality of residuals
-   Homogeneity of variances
-   **Linearity of the covariate**
-   **Homogeneity of regression slopes**

Let's check each of these below.

First, the normality of residuals can simply be tested as in the usual way. Our residuals are not normally distributed in this model (*W* = .97, *p* = .006).

```{r}
shapiro.test(jury_acov$residuals)
```
The homoegeneity of variance assumption is also largely tested in the same way. Our homogeneity of variance assumption also isn't met (*F*(2, 111) = 5.68, *p* = .004)...

```{r}
jury_data %>%
  levene_test(Years ~ Attr, center = "mean")
```

### Linearity of the covariate

A third assumption tests whether the covariate is linearly related to the DV. This assumption is essentially similar to the linearity assumption in a regression model - because we are still dealing with linear models, our covariates must also be linearly related to our outcome.

This is simple enough to test just by visualising the relationship. In general, this assumption appears to hold - it looks like there's a vague linear relationship in there. 

(Note that due to the data being in integers - i.e. whole numbers - I've used `geom_jitter()` in place of `geom_point()` to help visualise this a bit better.)

```{r fig.align = "center"}
jury_data %>%
  ggplot(
    aes(x = Serious, y = Years)
  ) + 
  geom_jitter() + 
  labs(x = "Perceived seriousness of crime", y = "Sentence length (Years)")
```

Finally, the **homogeneity of regression slopes** assumption specifies that for each group, the slope of the relationship between the covariate and the dependent variable are the same. To test this, we need to run an ANOVA that allows for an interaction between the predictor and covariate.

```{r}
jury_data %>%
  anova_test(Years ~ Attr * Serious, effect.size = "pes", type = 3)
```
Uh oh - this isn't good. A significant interaction suggests that the slope of the relationship between `Serious` and `Years` differs for each level of attractiveness, as indicated by the significant interaction effect (*p* = .03). We can see as much if we fit separate regression lines to the scatterplot above:

```{r fig.align = "center"}
jury_data %>%
  ggplot(
    aes(x = Serious, y = Years, colour = Attr)
  ) + 
  geom_jitter() + 
  labs(x = "Perceived seriousness of crime", y = "Sentence length (Years)") +
  geom_smooth(method = lm, se = FALSE)
```
As we can clearly see, the slopes are not identical for each group. In particular, the Unattractive group has a much stronger slope between seriousness and sentence length (indicating that unattractive people basically have it harder if they're perceived to have committed more serious crimes). 

Overall, given that many of our assumptions are not met - particularly the important one of homogeneity of regression slopes - this indicates that an ANCOVA isn't a suitable model for our data. What would we do in this instance, then? We'd probably model a regression that allows for the interaction between attractiveness and seriousness. 

A final note on ANCOVAs: naturally, we can extend an ANCOVA model to have multiple predictors *and* multiple covariates. In this instance, we would need to model multi-way interactions to test all of our effects and assumptions. Below is a lightly annotated example  of a two-way ANCOVA using attractiveness and type of crime as predictors, seriousness as a covariate and sentence length as an outcome.

```{r}
# Build two way ANCOVA
jury_twoway_acov <- aov(Years ~ Attr * Crime + Serious, data = jury_data)

# Normality of residuals
shapiro.test(jury_twoway_acov$residuals)

# Homogeneity of variance
jury_data %>%
  levene_test(Years ~ Attr * Crime, center = "mean")

# Linearity of covariate + homogeneity of regression slopes

jury_data %>%
  ggplot(
    aes(x = Serious, y = Years, colour = Attr)
  ) +
  geom_jitter() + 
  labs(x = "Perceived seriousness of crime", y = "Sentence length (Years)") +
  geom_smooth(method = lm, se = FALSE) +
  facet_wrap(~Crime)
  
jury_data %>%
  anova_test(Years ~ Attr * Crime * Serious, effect.size = "pes")

# Output ANCOVA
Anova(jury_twoway_acov, type = 3)
```


## Hierarchical regression

Hierarchical regression is a form of multiple regression where we test the effects of predictors in **blocks.** The aim of doing a hierarchical regression is generally to test theoretical predictions about the effects of specific variables, especially before/after we control for other variables. The other aim is to explore how the *model* changes after we add additional predictors into the model. 

The basic principle of a hierarchical regression is something like this:

1.    Start by defining block 1, which is our basic regression model. This is the regression we start with. Run the regression defined in block 1.
2.    Identify which variables will be entered into block 2, which is the first round of additional predictors
3.    Run a second multiple regression with all predictors in block 2.
4.    Compare block 1 with block 2 in terms of overall model fit.

The choice of what variables to enter in which blocks must be guided by theory - in other words, you cannot simply add variables at random. 

### Example

Let's return to the proneness to flow example introduced in the multiple regression section. As a reminder, here are our variables:

-    Trait anxiety: broadly, refers to people's tendency to feel anxious
-    Openness to experience: a personality trait that describes how likely people are to seek new experiences 
-    DFS_Total: a measure of proneness to flow.
-   age: participant's age.

```{r}
w10_flow <- read_csv(here("data", "week_10", "w10_flow.csv"))
```

In the first regressions module, we simply ran everything in one go as a multiple regression. Now let's imagine we want to run this as a hierarchical regression, with the following blocks:

-   Block 1: GOld MSI predicting proneness to flow (DFS_Total)
-   Block 2: Gold MSI and openness predicting proneness to flow
-   Block 3: Gold MSI, openness and trait anxiety predicting proneness to flow


The assumption tests in multiple regressions are identical for hierarchical regressions. 

### Building blocks and output

Let's start by building block 1. We can do this with `lm()` as per normal. I will call this `flow_block1`:

```{r}
flow_block1 <- lm(DFS_Total ~ GoldMSI, data = w10_flow)
```

To build block 2, we simply need to create a new regression model with both predictors, as if we were running this in one go:

```{r}
flow_block2 <- lm(DFS_Total ~ GoldMSI + openness, data = w10_flow)
```

Finally, we do the same thing for block 3:

```{r}
flow_block3 <- lm(DFS_Total ~ GoldMSI + openness + trait_anxiety, data = w10_flow)
```

Now let's print the summary of each model. We can see in block 1 that Gold MSI scores significantly predict proneness to flow:

```{r}
summary(flow_block1)
```

In Block 2, both the Gold MSI and openness are significant predictors of flow proneness. 

```{r}
summary(flow_block2)
```

Finally, in block 3 we can see that all three remain significant predictors. However, the effect of openness to experience has changed slightly (an unreliable heuristic for this is that the p-value has increased):

```{r}
summary(flow_block3)
```

On the next page we'll talk about model comparison in a more formal manner. However, if we wanted to write these results up we would need to talk about the results from each block. For example:

A hierarchical regression was conducted to examine the effect 


## Comparing models

On the previous page, we ended up with three models relating to the flow data. That's all well and good, and seeing how each model changed the predictors was valuable in its own right. But how do we actually... decide which model to run with? 

### Comparing $R^2$

The most commonly cited method of comparing between regression models is to examine their $R^2$ values, which you may recall is a measure of how much variance in the outcome is explained by the predictors.

This is easy enough to do visually. You can see the $R^2$ values in the output. We can extract this easily using the following code. Whenever we use `summary()` on an `lm()` model, the summary object will contain a variable for the $R^2$ that we can easily pull:

```{r}
summary(flow_block1)$r.squared
summary(flow_block2)$r.squared
summary(flow_block3)$r.squared
```
We can see that Block 3 has the highest $R^2$ at .297, meaning that the Block 3 model explains about 29.7% of the variance in the outcome. Block 2 explains 22.3% while Block 1 explains 21.2%. Therefore, based on this alone we might say that Block 2 explains only a little bit of extra variance in flow proneness than Block 1, while Block 3 explains substantially more - therefore, we should go with Block 3. However... $R^2$ will *always* increase with more predictors! The very fact that each additional predictor will explain more variance - even if only a tiny amount at a time - means that selecting based on $R^2$ alone will naturally favour models with more predictors. This isn't necessarily a useful thing!

### Nested model tests

This is a slightly more 'formal' test of whether a more complex model leads to a significant change in fit. This works by comparing **nested** models. Imagine model A and model B, two linear regressions fit on the same dataset. Model A has three predictors, and is the 'full' model of the thing we're trying to the estimate. Model B drops one of the predictors from Model A, but keeps the other two. Model B is considered a *nested* model of Model A.

The principle of this test is based on the idea of seeing whether a nested (reduced) model is a significantly better fit than a full model. If a nested model is a better fit, the residual sums of squares will *decrease* - less residuals indicate better fit. The `anova()` test works on this principle, but in a sort of reverse way. Because we're testing whether a model with *additional* predictors is a better fit, naturally we should expect that the 'nested' model (in this case, our original model) will be a *worse* fit than the new model. In that case, a significant result indicates that the more complex model is the better fit.

Nested model tests can be done with the `anova()` function from base R by simply giving it two model names in order. Let's start by comparing Blocks 1 and 2:

```{r}
anova(flow_block1, flow_block2)
```

And now between Blocks 2 and 3:

```{r}
anova(flow_block2, flow_block3)
```

From this, we can conclude that Block 2 is a better fit to the data than Block 1 (F(1, 808) = 11.437, *p* < .001), and also that Block 3 is again a better fit than Block 2 (F(1, 807) = 85.329, *p* < .001). Therefore, using this method we would consider using the model in Block 3 for interpretation, as this provides a better fit of the data. This sort of lines up with what we saw with the $R^2$ change (but this probably won't always be the case).


### Fit indices

An alternative approach is to use **fit indices**, which are various measures that essentially indicate how well a model fits the data. Importantly, unlike $R^2$ these measures penalise based on the complexity of the model - i.e. models with more predictors are penalised more due to their complexity. 

Two of the most widely used fit indices are the **Akaike Information Criterion (AIC)** and the **Bayesian Information Criterion (BIC)**. They work similarly, but are just calculated in slightly different ways. 

The AIC and BIC are calculated by:

$$
AIC = 2k -2 ln(\hat L)
$$

$$
BIC = k ln(n) - 2 ln (\hat L)
$$

$\hat L$ is called the **likelihood**, which is a whole thing that we won't dive too much into. However, $2 ln (\hat L)$ - or -2LL, or minus two log likelihood - goes by the name of **deviance** (as in deviation). Deviance is essentially the residual sum of squares, and thus serves as a measure of model fit.

R provides some really neat functions called - you guessed it - `AIC()` and `BIC()`. These will calculate the AIC and BIC values for every model name you give it. So, we can enter all of our values at once:

```{r}
AIC(flow_block1, flow_block2, flow_block3)
BIC(flow_block1, flow_block2, flow_block3)
```


## Outliers

Up until this point, we've largely worked without considering outliers in our data. Outliers, however, are important for any and all analyses that we do, because they have an impact on our statistics.

An intuitive reason is simply because of the fact that many of our statistics involve differences between means - such as *t*-tests and ANOVAs - and means are susceptible to outliers. Consider the following set of values:

```{r}
vector_b <- c(1, 3, 2, 4, 2)
```

If we take a mean, this is fairly straightforward:

```{r}
mean(vector_b)
```
However, imagine we have an outlier in one of our datapoints:

```{r}
vector_b <- c(1, 3, 2, 50, 2)
mean(vector_b)
```
Our estimate is now wildly different due to this outlier. Here's another visualisation of this effect at play, with a simple regression model:

```{r echo = FALSE, message = FALSE, fig.align='center', fig.dpi = 600}
cormat <- matrix(
  c(1, 0.6, 0.6, 1),
  nrow = 2
)

set.seed(2025)

example_cor <- simstudy::genCorData(
  n = 120, 
  mu = c(4, 10),
  sigma = c(1.2, 2),
  corMatrix = cormat,
  cnames = c("X", "V1")
) %>%
  mutate(
    V2 = V1
  )

example_cor$V2[19] <- 36


example_cor %>%
  pivot_longer(
    cols = V1:V2,
    names_to = "var",
    values_to = "Y"
  ) %>%
  ggplot(
    aes(x = X, y = Y)
  ) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~var) +
  ggpubr::theme_pubr() +
  ggpubr::stat_cor(label.y = 20) 
```

You can see that in the graph on the left, there are no visible outliers, while the graph on the right has a very clear outlier (the dot at the very top). The resulting estimate of the correlation is vastly different - *r* = .57 without the outlier, vs *r* = .21 with it included!

In short, outliers have the potential to distort our estimates. Therefore, being able to systematically identify outliers and handle them is an important step in any analysis.

Although outliers are worth considering across all types of analyses, we'll only consider them in context of regression models here for parity with Jamovi.

Aside from visual inspection, we have two main methods of identifying outliers in our regression models. For both examples, we will use `flow_block3` from the previous page.

### Cook's distance

**Cook's distances** are a method for identifying **influential points** in a regression model. The basic rationale is that if a point is highly influential, it has a large effect in shaping the overall parameter estimates in the regression model. Thus, points with high Cook's distances values are worth looking into.

Cook's distances can be easily calculated using the `cooks.distance()` function in base R. You just need to spply the name of the regression model (created using `lm()`):

```{r}
flow_cooksd <- cooks.distance(flow_block3)
```

This returns a singular vector of Cook's distance values. We can then perform basic descriptives on this variable to get a sense of the range of Cook's distance values:

```{r}
mean(flow_cooksd)
sd(flow_cooksd)
median(flow_cooksd)
min(flow_cooksd)
max(flow_cooksd)
```
If you prefer a tidyverse implementation, you can use the `augment()` function from the `broom` package. This returns a whole range of values, including fitted values, residuals (both of which are useful for Q-Q plots) and Cook's distances:

```{r}
library(broom)
flow_extra <- augment(flow_block3)

flow_extra
```

The Cook's distances are contained in the `.cooksd` column - note the full stop in front of the name is part of the column name.

As this returns a dataframe, we can then use normal tidyverse functions to wrangle this like any other dataframe.

```{r}
flow_extra %>%
  summarise(
    mean = mean(.cooksd),
    sd = sd(.cooksd),
    median = median(.cooksd),
    min = min(.cooksd),
    max = max(.cooksd)
  )
```

Base R also provides an easy way of visualising Cook's distances in regression models using `plot()`. To do this, you just feed in the name of the regression model (much like how you check for homoscedasticity during regression diagnostics). Note that `which = 4` must be specified.

```{r fig.dim='center', fig.dpi=600}
plot(flow_block3, which = 4)
```

R will automatically label points that it thinks are influential data points in this plot. Based on this, data points 539, 627 and 789 might be worth a closer look.

How might you identify influential data points in general? There are *many* rules of thumb out there for Cook's distances. Cook's original guideline was to flag any point with a distance > 1, but this might be relatively rare (note that our maximum distance in this model is 0.05!). Other rules include $\frac{2k}{4}$, where *k* is the number of predictors in the model, $\frac{4}{n}$ where *n* = sample size, and $2\sqrt{\frac{k}{n}}$. Truthfully, there is no singular metric that applies, so it's up to you to choose one and apply it consistently.


### Mahalanobis' distance, $D^2$

Mahalanobis' distance, $D^2$, is a measure of *multivariate* outliers - ie. outliers across two or more predictors. The basic idea is that it is the literal distance between a 'cloud' of all of the predictors in your model. Outliers can be described as data points that are the furthest away from the centre of this 'cloud', quantified by having a high Mahalanobis distance.

Generating Mahalanobis' distances in R is not difficult, and can be calculated using the `mahalanobis_distance()` function from the `rstatix`. This needs to be piped from the original dataset, and given the names of the *predictors* in the regression model.

In this case, as we are working with three predictors in this model - Gold-MSI scores, trait anxiety and openness - we give the names of these variables from our dataframe to this function.


```{r}
w10_flow <- w10_flow %>%
  mahalanobis_distance(GoldMSI, trait_anxiety, openness)

w10_flow
```

Note that this function will automatically flag whether a datapoint is an outlier. This is essentially done by calculating a critical Mahalanobis distance at *p* < .001. Any datapoint that sits above this critical Mahalanobis distance is flagged as an outlier (see expanded details below under "How do Mahalanobis distances work?")

We can view which datapoints are flagged as outliers by simply filtering our dataset using `filter()`:

```{r}
w10_flow %>%
  filter(is.outlier == TRUE)
```

Here, we can see that participants 378 and 528 are multivariate outliers, and thus we may want to consider doing something about them.

<details>
<summary>How do Mahalanobis distances work?</summary>
::: {style="background-color: #f0f0f0;"}
To calculate Mahalanobis distances by hand, you can use the `mahalanobis()` function from base R. To do this, you need to give the function a dataframe with just the predictors in it.

```{r}
flow_temp <- w10_flow %>%
  select(GoldMSI, trait_anxiety, openness)

head(flow_temp)
```

Then, feed it into the `mahalanobis()` function as below. Note the extra arguments; `colMeans(flow_temp)` specifies the means of the columns (i.e. the variables), and `cov(flow_temp)` generates the variance-covariance matrix between these variables.

```{r}
mahal <- mahalanobis(flow_temp, colMeans(flow_temp), cov(flow_temp))

head(mahal)
```
Mahalanobis distances follow a chi-square distribution. Thus, we can leverage the properties of the chi-square to calculate *p*-values for each Mahalanobis distance. For this test, degrees of freedom is equal to the number of predictors (i.e. df = 3 as we have three predictors).

```{r}
p <- pchisq(mahal, df = 3, lower.tail = FALSE)

head(p)
```
For Mahalanobis distances, the convention is to set the significance level at *p* < .001. Thus, at this point we can just go ahead and identify any data points that have a *p*-value below this cutoff.

Alternatively - and this is what `rstatix` does - you can calculate the Mahalanobis distance corresponding to *p* = .001, which will tell you the critical Mahalanobis distance value for a significant outlier.

```{r}
qchisq(0.999, df = 3)
```
This means that any Mahalanobis distance above 16.27 will be significant.

:::
</details>


### What happens if there is an outlier?

Say that you've identified a bona fide outlier in your data. What do you do with it? 

In these kinds of scenarios, the best-practice approach will most likely involve doing a **sensitivity analysis**. Sensitivity analyses are a general technique that broadly refer to testing your analyses under different conditions to see whether they hold under said conditions. The basic idea here is that you run *two* versions of your analyses:

1. First, a version *with* the outliers.
2. Next, a version *without* the outliers.

The aim is to see whether the effect(s) of interest change as a result of excluding the outliers, and by how much.

Let's see an example of this using the same model as before. Here is the original `flow_block3` model, just under a different name now:

```{r}
flow_lm1 <- lm(DFS_Total ~ GoldMSI + trait_anxiety + openness, data = w10_flow)
summary(flow_lm1)
```

Now, let's remove the two outliers that we identified using Mahalanobis distances. These were participants 378 and 528. We will use `filter()` for simplicity.

The code below lets us remove specific rows. Let's break down what this does: 

- The exclamation mark `!` before `id` means "not" - i.e. do not do what comes next. 
- The `%in%` operator is a special operator in R that checks if a vector of values is present in another vector. 
- Here, `id %in% c(378, 528)` essentially means to see whether the `id` column has a 378 and 528 in it. By wrapping this in `filter()` we are essentially telling R to filer the rows where `id` equals 378 and 528.

However, since all of that is preceded with the exclamation mark, the code is actually telling R to filter all the rows that are *not* `id` = 378 or 528 - in essence, it filters these rows *out*. It's a bit complex, but it's good to know as it applies to many scenarios!

```{r}
flow_no_outliers <- w10_flow %>%
  filter(!id %in% c(378, 528))
```


Now that we have a dataframe without the two outliers, let's now refit our model. The only argument we need to change is the name of the dataset:

```{r}
flow_lm2 <- lm(DFS_Total ~ GoldMSI + trait_anxiety + openness, data = flow_no_outliers)
summary(flow_lm2)
```

We can see that even without the outliers, the pattern of results is very similar to the original model we had above. Therefore, in this kind of setting we don't really gain anything by removing the outliers, and we might choose to keep the original model. Every participant lost is power lost, and the more information we have, the better! However, if your results do change substantially after excluding outliers then you will need to examine the differences carefully.

There are other methods that you could consider:

- Using a *non-parametric* test, which are not affected by outliers see [the section on non-parametric tests](#nonpara)
- Transforming the data to 'un-outlier' the outliers - this can be tricky to interpret

Regardless, it is a good idea to report both versions of the model - with and without outliers - for transparency, to show that you have actively considered these residuals.