# (APPENDIX) Appendix {-}

```{r warning = FALSE, message = FALSE, echo = FALSE}
library(tidyverse)
library(ggpubr)
library(here)
library(rstatix)
library(effectsize)
library(car)
library(emmeans)
```


# R colour palettes {#colours}


```{r echo = FALSE, results = "hide"}
colour_list <- colours()

colour_rgb <- col2rgb(colour_list)

colour_rgb <- rgb(colour_rgb[1,], colour_rgb[2,], colour_rgb[3,],
    names = colour_list,
    maxColorValue = 255)

# colour_rgb <- colour_rgb[!duplicated(colour_rgb)]

colour_df <- data.frame(
  colour = names(colour_rgb),
  hexcode = colour_rgb,
  row.names = NULL
) 

colour_df$colour <- factor(colour_df$colour, levels = colour_df$colour)
```

```{r echo = FALSE, fig.dim = c(5, 130)}

colourplot <- ggplot(
  data = colour_df,
    aes(
      x = 1, y = 1:nrow(colour_df), 
      fill = colour,
      label = paste0(colour, " ", hexcode)
      )
  ) +
  geom_tile(width = 0.5, height = 1, colour = "black")  +
  scale_fill_manual(values = colour_df$hexcode) +
  theme_void() +
  theme(legend.position = "none") +
  geom_label(fill = "white", alpha = 0.5)  +
  # geom_text(aes(color = ))
  scale_y_continuous(limits = c(0, NA), expand = c(0, 0)) 

colourplot
```



# The `broom()` package {#broom}

The `broom()` package provides a couple of helper functions for tidying up output from numerous R functions for running tests. This can be useful in a couple of instances, particularly for either a) accessing certain aspects of models that are not immediately accessible or b) simply for neater manipulation for multiple models.

While `broom` is a part of the tidyverse, it is not one of the tidyverse's default packages. Therefore, to use the package you need to manually call it:

```{r}
library(broom)
```

For the examples on this page, we will use datasets from the `datarium` package (which we have seen before), just because it provides a nice set of datasets for demonstrating how these functions work. So let's go ahead and load that too:

```{r}
library(datarium)
```

The main functions in `broom` are generic functions with what R calls several *methods* - i.e. ways that the function handles different types of data or objects. Every method comes with a different set of arguments that can change the output, depending on what object the function is run on. For example, if you use `tidy()` on an `lm` object, you get different optional arguments than for an `aov()` object, and so on. The next page gives examples using the `datarium` datasets.

## The `tidy()` function

You may have noticed that many of the outputs from common R functions, like `lm()` and `aov()`, print their results in a certain format - namely, it essentially prints as text. The `tidy()` function will simply turn the core output into a data frame. This is useful when you are running multiple models at once, or if for some reason you want to work with the values in model outputs directly. This function will work with just about every standard test function you get in R.

Here an example with the `marketing` dataset, which contains continuous variables. Let's fit a multiple regression using `lm()`, and call `summary()` on the results:

```{r}
data(marketing)

marketing_lm <- lm(sales ~ youtube + facebook + newspaper, data = marketing)
summary(marketing_lm)
```
`tidy()` works directly on *model* objects, not raw data, so we use the `tidy()` function on our regression model. As you can see, the data is now in data frame format:

```{r}
tidy(marketing_lm)
```

For `lm()` objects, you can return a confidence interval on the regression coefficients:

```{r}
tidy(marketing_lm, conf.int = TRUE, conf.level = 0.95)
```

### Correlations

For a correlation:

```{r}
marketing_cor <- cor.test(marketing$youtube, marketing$facebook)
marketing_cor
```
```{r}
tidy(marketing_cor)
```

### Chi-squares

A chi-square test object:

```{r}
data("properties")

properties_table <- table(properties$property_type, properties$buyer_type)
properties_chisq <- chisq.test(properties_table, correct = FALSE)
properties_chisq
```
```{r}
tidy(properties_chisq)
```

### t-tests

For a t-test object (applies to all t-tests):

```{r}
data("genderweight")

weight_t <- t.test(weight ~ group, data = genderweight)
weight_t
```

```{r}
tidy(weight_t)
```

### ANOVA objects

For a regular `aov()` object, you can optionally ask for the intercept term using `intercept = TRUE`:

```{r}
data("stress")

stress_aov <- aov(score ~ treatment * exercise, data = stress)
summary(stress_aov)
```
```{r}
tidy(stress_aov)
```

Objects fitted by the `Anova` package also work, *but* repeated measures designs do not work. It's best to stick to `rstatix` if you want a repeated measures ANOVA in dataframe format.

`TukeyHSD()` can also be used:

```{r}
TukeyHSD(stress_aov) %>%
  tidy()
```


`emmeans` objects can also be tidied, e.g. for simple effects tests:

```{r}
# Simple effects of exercise for every treatment

emmeans(stress_aov, ~ exercise, by = "treatment") %>%
  pairs() %>%
  tidy()
```


## The `glance()` function

The `glance()` function will generate model fit summaries from models. It works primarily with `lm()` and other models, and returns several indices of model fit (depending on the original object).

For `lm()` objects, importantly, it returns $R^2$ and adjusted $R^2$. IT also returns estimates for the AIC and BIC, as well as some other fit statistics.

```{r}
glance(marketing_lm)
```
`glance()` also works for `aov()` objects, but is perhaps less useful.

```{r}
glance(stress_aov)
```

## The `augment()` function

The a

### Chi-square tests

Using `augment()` on a chi-square object will print out a dataframe containing both the expected and the observed proportions for each cell.

```{r}
augment(properties_chisq)
```

### Regressions

`augment()` is probably most useful for regression models. It will print out the following:

- `.fitted` is the predicted score for each participant
- `.resid` is the residual for each participant (i.e. actual - fitted)
- `.cooksd` is Cook's distance, which is useful for outlier detection in some contexts

```{r}
augment(marketing_lm)
```


# Technical details of `anova()` 

```{r echo = FALSE}
w10_flow <- read_csv(here("data", "week_10", "w10_flow.csv"))
flow_block1 <- lm(DFS_Total ~ GoldMSI, data = w10_flow)
flow_block2 <- lm(DFS_Total ~ GoldMSI + openness, data = w10_flow)
```

Technical note: this test works not too unlike a regular ANOVA, except the F-test is being conducted on the residual sums of scores. From a mathematical point of view, we are essentially conducting an F-test on the change in the residual sum of squares with the following formula:

$$
F(df_{df_b - df_a}, df_a) = \frac{MS_{comp}}{MS_{a}} = \frac{(SS_b - SS_a)/(p_a - p_b)}{SS_a/df_a}
$$

Consider two models, Model A and Model B. Imagine Model B is a nested version of Model A - i.e. it it the same model as Model A but with less predictors. In our case, imagine Model B is `flow_block1` (which only had one predictor) and Model A is `flow_block2` (which had two). $p_a$ is the number of coefficients in Model A *including* the intercept, and same with $p_b$. 

The exact process is:

1.    Calculate the difference between residual SS in the two models - this is the $(SS_b - SS_a)$ part of the formula above. This is just the difference in RSS between model 1 (model B) and 2 (model A), i.e. 10124.2 - 9982.9 = 141.3.

2.    Calculate the difference in df $(p_a - p_b)$. In `flow_block1` we have one predictor and one intercept, so we have 2 terms - this is $p_b$. In `flow_block2` we have two predictors and one intercept, which makes $p_a = 3$. Therefore, $(p_a - p_b) = 3 - 2 = 1$.

3.    Calculate a mean square ratio for the comparison, which is $MS_{comp}$. Essentially, we divide the result in step 1 (143.1) by the result in step 2 (1). This follows the same formula for mean squarews as we have seen before: $MS = \frac{SS_{comp}}{df_{comp}}$, so 
$MS_{comp} = \frac{141.3}{1} = 141.3.$ While this is identical to the sum of squares value in the table above, note that this is *not* the same value.

4.    Calculate a mean square ratio between RSS and df for the new model. This is the $SS_a/df_a$ part of the equation. $df_a$ is calculated as $n - p_a$, where n is the original sample size. So $df_a = 811 - 3 = 808$. Note that the value for row 2 (which corresponds to Model A/`flow_block2`) under `Res.df` is 808. 

Note that $df_b$ is the same; $n - p_b = 811 - 2 = 809$.

Same deal as above after that, except this time we use the values from the new model only, i.e. residual SS for model A (`flow_block2`) and the residual df. 

$MS_a = \frac{SS_{a}}{df_{a}}$
$MS_a = \frac{9982.9}{808} = 12.33507$

5.    Calculate an F ratio between the MS of the comparison and the MS of the new model to calculate a value for F.

This is exactly the same formula as it would be for a regular ANOVA, just that now we are doing:

$$
F = \frac{MS_{comp}}{MS_{a}}
$$

$F = \frac{141.3}{12.33507} = 11.45514$ 

6. Calculate a p-value for this F-statistic by comparing the p against an F distribution. The two dfs in the original formula are a) $df_b - df_a$ and b) $df_a$. Which means:

-   $df_b$ is 809 - $df_a$ is 808 = 1
-   $df_a$ is 808

So we end up with a test statistic of $F(1, 808) = 11.455$. We can use this to calculate a p-value by calculating the probability of getting a value of at least 11.455, on an F distribution with degrees of freedom parameters described above. We can visualise this below. Note that because the values for F are so infinitesimally small with these parameters and at this F-value, I've zoomed in the plot to visualise the highlighted area:

```{r}
tibble(
  x = seq(0, 15, by = .5),
  y = df(x, df1 = 1, df2 = 809)
) %>%
  ggplot(
    aes(x = x, y = y)
  ) +
  geom_line(linewidth = 1) +
  theme_pubr() +
  geom_vline(xintercept = 11.455, linewidth = 1, colour = "royalblue") +
  annotate("text", x = 12, y = 0.0024, label = "F = 11.455") +
  stat_function(fun = df, args = list(df1 = 1, df2 = 809), 
                geom = "area", xlim = c(11.455, 15),
                fill = "royalblue", alpha = 0.5) + 
  scale_x_continuous(expand = c(0, 0), limits = c(10, NA)) + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, 0.003)) +
  labs(x = "F-value", y = "Density")

```

R can manually calculate a p-value with the `pf()` function. `pf()` will calculate the probability of a value on the F distribution, given the two degrees of freedom parameters to characterise the distribution. `lower.tail = FALSE` is used to indicate that we want to calculate the probability of getting something *above* our critical F-value; `lower.tail = TRUE` would calculuate the probability *below* it.

```{r}
pf(11.45514, df1 = 1, df2 = 808, lower.tail = FALSE)
```


Note that our p-value isn't exactly the same as the value in the table - this is because we've used rounded values. The code below extracts the unrounded values and uses them in the calculations. As you can see we get the exact p-value in the table.

```{r}
x <- anova(flow_block1, flow_block2)
x

# Using the output from anova() to manually calculate p-value
SSb <- x$RSS[1]
SSa <- x$RSS[2]
pa <- length(coef(flow_block2))
pb <- length(coef(flow_block1))
dfa <- nrow(w10_flow) - pa
dfb <- nrow(w10_flow) - pb

f_val <- ((SSb-SSa)/(pa-pb))/(SSa/dfa)

pf(f_val, df1 = dfb-dfa, df2 = dfa, lower.tail = FALSE)

```

